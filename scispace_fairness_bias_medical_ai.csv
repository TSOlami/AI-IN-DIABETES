"Paper Title","Paper Link","Publication Year","Publication Type","Publication Title","Author Names","DOI","PDF Link","Abstract","TL;DR"
"Mitigating machine learning bias between high income and low–middle income countries for enhanced model fairness and generalizability","https://scispace.com/paper/mitigating-machine-learning-bias-between-high-income-and-low-3vhwysnpsi","2024","Journal Article","Dental science reports","Jenny Yang
Lei Clifton
Nguyen Thanh Dung
Nguyễn Thanh Phong
Lam Minh Yen
Doan Bui Xuan Thy
Andrew A. S. Soltan
Louise Thwaites
David A. Clifton","10.1038/s41598-024-64210-5","https://scispace.compdf/mitigating-machine-learning-bias-between-high-income-and-low-3vhwysnpsi.pdf","Abstract Collaborative efforts in artificial intelligence (AI) are increasingly common between high-income countries (HICs) and low- to middle-income countries (LMICs). Given the resource limitations often encountered by LMICs, collaboration becomes crucial for pooling resources, expertise, and knowledge. Despite the apparent advantages, ensuring the fairness and equity of these collaborative models is essential, especially considering the distinct differences between LMIC and HIC hospitals. In this study, we show that collaborative AI approaches can lead to divergent performance outcomes across HIC and LMIC settings, particularly in the presence of data imbalances. Through a real-world COVID-19 screening case study, we demonstrate that implementing algorithmic-level bias mitigation methods significantly improves outcome fairness between HIC and LMIC sites while maintaining high diagnostic sensitivity. We compare our results against previous benchmarks, utilizing datasets from four independent United Kingdom Hospitals and one Vietnamese hospital, representing HIC and LMIC settings, respectively. ","Collaborative AI approaches can lead to biased outcomes between HIC and LMIC settings due to data imbalances. Algorithmic-level bias mitigation methods improve fairness and generalizability."
"Focusing on Decisions, Outcomes, and Value Judgments to Confront Algorithmic Bias.","https://scispace.com/paper/focusing-on-decisions-outcomes-and-value-judgments-to-3o65e2ae","2023","Journal Article","JAMA network open","Ankur Pandya
Jinyi Zhu","10.1001/jamanetworkopen.2023.18501","https://scispace.com/pdf/focusing-on-decisions-outcomes-and-value-judgments-to-3o65e2ae.pdf","Sara Khor, MASc; Eric C. Haupt, ScM; Erin E. Hahn, PhD, MPH; Lindsay Joe L. Lyons, LVN; Veena Shankaran, MD, MS; Aasthaa Bansal, PhD ","Sara Khor, MASc; Eric C. Haupt, ScM; Erin E. Hahn, PhD, MPH; Lindsay Joe L. Lyons, LVN; Veena Shankaran, MD, MS; Aasthaa Bansal, PhD as discussed by the authors"
"Causal fairness assessment of treatment allocation with electronic health records","https://scispace.com/paper/causal-fairness-assessment-of-treatment-allocation-with-4sifyjvjrk","2024","Journal Article","Journal of Biomedical Informatics","Linying Zhang
Lauren R. Richter
Yixin Wang
Anna Ostropolets
Noémie Elhadad
David M. Blei
George Hripcsak","10.1016/j.jbi.2024.104656","","Healthcare continues to grapple with the persistent issue of treatment disparities, sparking concerns regarding the equitable allocation of treatments in clinical practice. While various fairness metrics have emerged to assess fairness in decision-making processes, a growing focus has been on causality-based fairness concepts due to their capacity to mitigate confounding effects and reason about bias. However, the application of causal fairness notions in evaluating the fairness of clinical decision-making with electronic health record (EHR) data remains an understudied domain. This study aims to address the methodological gap in assessing causal fairness of treatment allocation with electronic health records data. In addition, we investigate the impact of social determinants of health on the assessment of causal fairness of treatment allocation. We propose a causal fairness algorithm to assess fairness in clinical decision-making. Our algorithm accounts for the heterogeneity of patient populations and identifies potential unfairness in treatment allocation by conditioning on patients who have the same likelihood to benefit from the treatment. We apply this framework to a patient cohort with coronary artery disease derived from an EHR database to evaluate the fairness of treatment decisions. Our analysis reveals notable disparities in coronary artery bypass grafting (CABG) allocation among different patient groups. Women were found to be 4.4%–7.7% less likely to receive CABG than men in two out of four treatment response strata. Similarly, Black or African American patients were 5.4%–8.7% less likely to receive CABG than others in three out of four response strata. These results were similar when social determinants of health (insurance and area deprivation index) were dropped from the algorithm. These findings highlight the presence of disparities in treatment allocation among similar patients, suggesting potential unfairness in the clinical decision-making process. This study introduces a novel approach for assessing the fairness of treatment allocation in healthcare. By incorporating responses to treatment into fairness framework, our method explores the potential of quantifying fairness from a causal perspective using EHR data. Our research advances the methodological development of fairness assessment in healthcare and highlight the importance of causality in determining treatment fairness. ","Causal fairness assessment of treatment allocation with electronic health records data reveals significant disparities in treatment allocation among different patient groups."
"Evaluating gender bias in ML-based clinical risk prediction models: A study on multiple use cases at different hospitals","https://scispace.com/paper/evaluating-gender-bias-in-ml-based-clinical-risk-prediction-244j34sq01","2024","Journal Article","Journal of Biomedical Informatics","Patricia Cabanillas Silva
Hong Sun
Pablo Esteban Rodríguez
Mohamed Rezk
Xianchao Zhang
Janis Fliegenschmidt
Nikolai Hulde
Vera von Dossow
Laurent Meesseman
Kristof Depraetere
Ralph Szymanowsky
Jörg Stieg
Fried-Michael Dahlweid","10.1016/j.jbi.2024.104692","","An inherent difference exists between male and female bodies, the historical under-representation of females in clinical trials widened this gap in existing healthcare data. The fairness of clinical decision-support tools is at risk when developed based on biased data. This paper aims to quantitatively assess the gender bias in risk prediction models. We aim to generalize our findings by performing this investigation on multiple use cases at different hospitals. ","The presence of gender bias within risk prediction models varies across different clinical use cases and healthcare institutions, and the decision curve analysis demonstrates there is no statistically significant difference between the model's clinical utility across gender groups within the interested range of thresholds."
"Fairness and inclusion in biomedical artificial intelligence research and clinical use: Technical and social perspectives","https://scispace.com/paper/fairness-and-inclusion-in-biomedical-artificial-intelligence-2435dvpzgi","2024","Journal Article","Journal of Biomedical Informatics","Ashley C Griffin
Karen Wang
Tiffany I. Leung
Julio C. Facelli","10.1016/j.jbi.2024.104693","","Understanding and quantifying biases when designing and implementing actionable approaches to increase fairness and inclusion is critical for artificial intelligence (AI) in biomedical applications. ","Recommendations to address biases when developing and using AI in clinical applications can be applied to informatics research and practice to foster more equitable and inclusive health care systems and research discoveries."
"Fair AI-powered orthopedic image segmentation: addressing bias and promoting equitable healthcare","https://scispace.com/paper/fair-ai-powered-orthopedic-image-segmentation-addressing-2pwzrvp94u","2024","Journal Article","Dental science reports","Iram Siddiqui
Nickolas Littlefield
Luke Carlson
Matthew Gong
Avani Chhabra
Zoe Menezes
George Mastorakos
Sakshi Mehul Thakar
Mehrnaz Abedian
Ines Lohse
Kurt R. Weiss
Johannes F. Plate
Hamidreza Moradi
Soheyla Amirian
Ahmad P. Tafti","10.1038/s41598-024-66873-6","","Abstract AI-powered segmentation of hip and knee bony anatomy has revolutionized orthopedics, transforming pre-operative planning and post-operative assessment. Despite the remarkable advancements in AI algorithms for medical imaging, the potential for biases inherent within these models remains largely unexplored. This study tackles these concerns by thoroughly re-examining AI-driven segmentation for hip and knee bony anatomy. While advanced imaging modalities like CT and MRI offer comprehensive views, plain radiographs (X-rays) predominate the standard initial clinical assessment due to their widespread availability, low cost, and rapid acquisition. Hence, we focused on plain radiographs to ensure the utilization of our contribution in diverse healthcare settings, including those with limited access to advanced imaging technologies. This work provides insights into the underlying causes of biases in AI-based knee and hip image segmentation through an extensive evaluation, presenting targeted mitigation strategies to alleviate biases related to sex, race, and age, using an automatic segmentation that is fair, impartial, and safe in the context of AI. Our contribution can enhance inclusivity, ethical practices, equity, and an unbiased healthcare environment with advanced clinical outcomes, aiding decision-making and osteoarthritis research. Furthermore, we have made all the codes and datasets publicly and freely accessible to promote open scientific research. ","This work provides insights into the underlying causes of biases in AI-based knee and hip image segmentation through an extensive evaluation, presenting targeted mitigation strategies to alleviate biases related to sex, race, and age using an automatic segmentation that is fair, impartial, and safe in the context of AI."
"Resilient Artificial Intelligence in Health: Synthesis and Research Agenda Toward Next-Generation Trustworthy Clinical Decision Support","https://scispace.com/paper/resilient-artificial-intelligence-in-health-synthesis-and-2xp2f6e6yu","2024","Journal Article","Journal of Medical Internet Research","Carlos Sáez
Carlos Sáez
Carlos Sáez","10.2196/50295","","Artificial intelligence (AI)-based clinical decision support systems are gaining momentum by relying on a greater volume and variety of secondary use data. However, the uncertainty, variability, and biases in real-world data environments still pose significant challenges to the development of health AI, its routine clinical use, and its regulatory frameworks. Health AI should be resilient against real-world environments throughout its lifecycle, including the training and prediction phases and maintenance during production, and health AI regulations should evolve accordingly. Data quality issues, variability over time or across sites, information uncertainty, human-computer interaction, and fundamental rights assurance are among the most relevant challenges. If health AI is not designed resiliently with regard to these real-world data effects, potentially biased data-driven medical decisions can risk the safety and fundamental rights of millions of people. In this viewpoint, we review the challenges, requirements, and methods for resilient AI in health and provide a research framework to improve the trustworthiness of next-generation AI-based clinical decision support. ","Resilient AI in health requires addressing data quality issues, variability over time or across sites, information uncertainty, human-computer interaction, and fundamental rights assurance to ensure trustworthy clinical decision support."
"Advancing Fairness in Cardiac Care: Strategies for Mitigating Bias in Artificial Intelligence Models within Cardiology.","https://scispace.com/paper/advancing-fairness-in-cardiac-care-strategies-for-mitigating-4tiaqwudcz","2024","","Canadian Journal of Cardiology","Alexis Nolin Lapalme
Denis Corbin
Olivier Tastet
Robert Avram
Julie G. Hussin","10.1016/j.cjca.2024.04.026","","In the dynamic field of medical artificial intelligence (AI), cardiology stands out as a key area for its technological advancements and clinical application. This review explores the complex issue of data bias, specifically addressing those encountered during the development and implementation of AI tools in cardiology. We dissect the origins and impacts of these biases, which challenge their reliability and widespread applicability in healthcare. Using a case study, we highlight the complexities involved in addressing these biases from a clinical viewpoint. The goal of this review is to equip researchers and clinicians with the practical knowledge needed to identify, understand, and mitigate these biases, advocating for the creation of AI solutions that are not just technologically sound, but also fair and effective for all patient demographics. In the dynamic field of medical artificial intelligence (AI), cardiology stands out as a key area for its technological advancements and clinical application. This review explores the complex issue of data bias, specifically addressing those encountered during the development and implementation of AI tools in cardiology. We dissect the origins and impacts of these biases, which challenge their reliability and widespread applicability in healthcare. Using a case study, we highlight the complexities involved in addressing these biases from a clinical viewpoint. The goal of this review is to equip researchers and clinicians with the practical knowledge needed to identify, understand, and mitigate these biases, advocating for the creation of AI solutions that are not just technologically sound, but also fair and effective for all patient demographics. Dans le domaine dynamique de l'intelligence artificielle (IA) médicale, la cardiologie se distingue comme un secteur clé pour son progrès et son application clinique. Cette revue de littérature examine la question complexe des biais, en se concentrant spécifiquement sur ceux rencontrés lors du développement et du déploiement d'outils d'IA en cardiologie. Nous analysons les origines et les impacts de ces biais, qui remettent en question leur fiabilité et leur applicabilité dans les soins de santé. À travers une étude de cas, nous soulignons les subtilités à prendre en compte pour détecter et éviter ces biais du point de vue clinique. L'objectif de cette revue est de fournir aux chercheurs et aux cliniciens des connaissances pratiques pour identifier, comprendre et atténuer les biais, en plaidant pour la création de solutions d'IA qui ne sont pas seulement performantes, mais aussi équitables pour tous les groupes de patients. Dans le domaine dynamique de l'intelligence artificielle (IA) médicale, la cardiologie se distingue comme un secteur clé pour son progrès et son application clinique. Cette revue de littérature examine la question complexe des biais, en se concentrant spécifiquement sur ceux rencontrés lors du développement et du déploiement d'outils d'IA en cardiologie. Nous analysons les origines et les impacts de ces biais, qui remettent en question leur fiabilité et leur applicabilité dans les soins de santé. À travers une étude de cas, nous soulignons les subtilités à prendre en compte pour détecter et éviter ces biais du point de vue clinique. L'objectif de cette revue est de fournir aux chercheurs et aux cliniciens des connaissances pratiques pour identifier, comprendre et atténuer les biais, en plaidant pour la création de solutions d'IA qui ne sont pas seulement performantes, mais aussi équitables pour tous les groupes de patients. ","This review explores data bias in AI models in cardiology, dissecting origins and impacts, and advocating for fair and effective AI solutions that address biases and promote equitable care for all patient demographics."
"Addressing AI Algorithmic Bias in Health Care","https://scispace.com/paper/addressing-ai-algorithmic-bias-in-health-care-172rv8u6le2t","2024","Journal Article","JAMA","Raj M. Ratwani
Karey M. Sutton
Jessica E. Galarraga","10.1001/jama.2024.13486","","This Viewpoint discusses the bias that exists in artificial intelligence (AI) algorithms used in health care despite recent federal rules to prohibit discriminatory outcomes from AI and recommends ways in which health care facilities, AI developers, and regulators could share responsibilities and actions to address bias. ","This Viewpoint highlights AI algorithmic bias in healthcare, despite federal regulations, and proposes shared responsibilities among healthcare facilities, AI developers, and regulators to address bias and ensure equitable healthcare outcomes through AI."
"Mitigating the risk of artificial intelligence bias in cardiovascular care","https://scispace.com/paper/mitigating-the-risk-of-artificial-intelligence-bias-in-5cz5ptoerfze","2024","Journal Article","The Lancet Digital Health","Ariana Mihan
Ambarish Pandey
Harriette G.C. Van Spall","10.1016/s2589-7500(24)00155-9","","Digital health technologies can generate data that can be used to train artificial intelligence (AI) algorithms, which have been particularly transformative in cardiovascular health-care delivery. However, digital and health-care data repositories that are used to train AI algorithms can introduce bias when data are homogeneous and health-care processes are inequitable. AI bias can also be introduced during algorithm development, testing, implementation, and post-implementation processes. The consequences of AI algorithmic bias can be considerable, including missed diagnoses, misclassification of disease, incorrect risk prediction, and inappropriate treatment recommendations. This bias can disproportionately affect marginalised demographic groups. In this Series paper, we provide a brief overview of AI applications in cardiovascular health care, discuss stages of algorithm development and associated sources of bias, and provide examples of harm from biased algorithms. We propose strategies that can be applied during the training, testing, and implementation of AI algorithms to mitigate bias so that all those at risk for or living with cardiovascular disease might benefit equally from AI. ","This paper discusses AI bias in cardiovascular care, highlighting its introduction through data repositories, algorithm development, and implementation, and proposes strategies to mitigate bias and ensure equitable benefits for all patients, particularly marginalized groups."
"Machine Learning and Bias in Medical Imaging: Opportunities and Challenges.","https://scispace.com/paper/machine-learning-and-bias-in-medical-imaging-opportunities-557a6g5msr","2024","Journal Article","Circulation-cardiovascular Imaging","Amey Vrudhula
A.C.Y. Kwan
David Ouyang
Susan Cheng","10.1161/circimaging.123.015495","","Bias in health care has been well documented and results in disparate and worsened outcomes for at-risk groups. Medical imaging plays a critical role in facilitating patient diagnoses but involves multiple sources of bias including factors related to access to imaging modalities, acquisition of images, and assessment (ie, interpretation) of imaging data. Machine learning (ML) applied to diagnostic imaging has demonstrated the potential to improve the quality of imaging-based diagnosis and the precision of measuring imaging-based traits. Algorithms can leverage subtle information not visible to the human eye to detect underdiagnosed conditions or derive new disease phenotypes by linking imaging features with clinical outcomes, all while mitigating cognitive bias in interpretation. Importantly, however, the application of ML to diagnostic imaging has the potential to either reduce or propagate bias. Understanding the potential gain as well as the potential risks requires an understanding of how and what ML models learn. Common risks of propagating bias can arise from unbalanced training, suboptimal architecture design or selection, and uneven application of models. Notwithstanding these risks, ML may yet be applied to improve gain from imaging across all 3A's (access, acquisition, and assessment) for all patients. In this review, we present a framework for understanding the balance of opportunities and challenges for minimizing bias in medical imaging, how ML may improve current approaches to imaging, and what specific design considerations should be made as part of efforts to maximize the quality of health care for all.","A framework for understanding the balance of opportunities and challenges for minimizing bias in medical imaging is presented, how ML may improve current approaches to imaging, and what specific design considerations should be made as part of efforts to maximize the quality of health care for all are presented."
"Critical Appraisal for Racial and Ethnic Equity in Clinical Prediction Models Extension: Development of a Critical Appraisal Tool Extension to Assess Racial and Ethnic Equity-Related Risk of Bias for Clinical Prediction Models","https://scispace.com/paper/critical-appraisal-for-racial-and-ethnic-equity-in-clinical-48m1goouv6","2023","Journal Article","Health equity","Shazia M. Siddique
Corinne V. Evans
Michael Harhay
Eric S. Johnson
Jaya Aysola
Gary E. Weissman
Nikhil K. Mull
Emilia Flores
Harald Schmidt
K. Tipton
B. Leas
Jennifer S. Lin","10.1089/heq.2023.0035","","Introduction: Despite mounting evidence that the inclusion of race and ethnicity in clinical prediction models may contribute to health disparities, existing critical appraisal tools do not directly address such equity considerations. Objective: This study developed a critical appraisal tool extension to assess algorithmic bias in clinical prediction models. Methods: A modified e-Delphi approach was utilized to develop and obtain expert consensus on a set of racial and ethnic equity-based signaling questions for appraisal of risk of bias in clinical prediction models. Through a series of virtual meetings, initial pilot application, and an online survey, individuals with expertise in clinical prediction model development, systematic review methodology, and health equity developed and refined this tool. Results: Consensus was reached for ten equity-based signaling questions, which led to the development of the Critical Appraisal for Racial and Ethnic Equity in Clinical Prediction Models (CARE-CPM) extension. This extension is intended for use along with existing critical appraisal tools for clinical prediction models. Conclusion: CARE-CPM provides a valuable risk-of-bias assessment tool extension for clinical prediction models to identify potential algorithmic bias and health equity concerns. Further research is needed to test usability, interrater reliability, and application to decision-makers.","A critical appraisal tool extension to assess algorithmic bias in clinical prediction models and the Critical Appraisal for Racial and Ethnic Equity in Clinical Prediction Models (CARE-CPM) extension is developed, intended for use along with existing critical appraisal tools forclinical prediction models."
"Assessing Bias in Skin Lesion Classifiers with Contemporary Deep Learning and Post-Hoc Explainability Techniques","https://scispace.com/paper/assessing-bias-in-skin-lesion-classifiers-with-contemporary-1pwkudme","2023","Journal Article","IEEE Access","Adam Corbin
Oge Marques","10.1109/access.2023.3289320","https://scispace.com/pdf/assessing-bias-in-skin-lesion-classifiers-with-contemporary-1pwkudme.pdf","As Artificial Intelligence is increasingly utilized in dermatology, ensuring fairness in the development of Machine Learning models is crucial, particularly in skin lesion classification, where decisions can significantly impact people’s lives. This study investigates the presence of biases between different Fitzpatrick Skin Types in baseline pretrained models and evaluates various training techniques to mitigate these disparities. An unsupervised skin transformer is developed to adjust an image’s FST, and joint regularization and synthetic image blending methods are employed to address bias concerns. Additionally, eXplainable AI techniques, such as Grad-CAM, are utilized to identify any underlying reasons for bias in the models. The results indicate that joint regularization and synthetic blending methods enhance the area under the curve performance and fairness. Meanwhile, eXplainable AI was found to be a valuable tool for fine-tuning Deep Learning models and uncovering problems. These findings can aid in developing accurate and unbiased skin lesion classification models, promoting equitable healthcare, and improving patient outcomes. ","In this article , an unsupervised skin transformer is developed to adjust an image's FST, and joint regularization and synthetic image blending methods are employed to address bias concerns."
"Eliminating Algorithmic Racial Bias in Clinical Decision Support Algorithms: Use Cases from the Veterans Health Administration","https://scispace.com/paper/eliminating-algorithmic-racial-bias-in-clinical-decision-3krqyb5qog","2023","Journal Article","Health equity","Justin M. List
Paul Palevsky
Suzanne Tamang
Susan Crowley
David Au
William C. Yarbrough
Amol S. Navathe
Craig Kreisler
Ravi B. Parikh
Jessica Wang-rodriguez
J. S. Klutts
Paul Conlin
Leonard Pogach
Esther Meerwijk
Ernest Moy","10.1089/heq.2023.0037","","The Veterans Health Administration uses equity- and evidence-based principles to examine, correct, and eliminate use of potentially biased clinical equations and predictive models. We discuss the processes, successes, challenges, and next steps in four examples. We detail elimination of the race modifier for estimated kidney function and discuss steps to achieve more equitable pulmonary function testing measurement. We detail the use of equity lenses in two predictive clinical modeling tools: Stratification Tool for Opioid Risk Mitigation (STORM) and Care Assessment Need (CAN) predictive models. We conclude with consideration of ways to advance racial health equity in clinical decision support algorithms.","The Veterans Health Administration uses equity- and evidence-based principles to examine, correct, and eliminate use of potentially biased clinical equations and predictive models and consideration of ways to advance racial health equity in clinical decision support algorithms is considered."
"An intersectional framework for counterfactual fairness in risk prediction","https://scispace.com/paper/an-intersectional-framework-for-counterfactual-fairness-in-4j684oo7gv","2023","Journal Article","Biostatistics","Solvejg Wastvedt
Jared D. Huling
Julian Wolfson","10.1093/biostatistics/kxad021","","Along with the increasing availability of health data has come the rise of data-driven models to inform decision making and policy. These models have the potential to benefit both patients and health care providers but can also exacerbate health inequities. Existing ""algorithmic fairness"" methods for measuring and correcting model bias fall short of what is needed for health policy in two key ways. First, methods typically focus on a single grouping along which discrimination may occur rather than considering multiple, intersecting groups. Second, in clinical applications, risk prediction is typically used to guide treatment, creating distinct statistical issues that invalidate most existing techniques. We present novel unfairness metrics that address both challenges. We also develop a complete framework of estimation and inference tools for our metrics, including the unfairness value (""u-value""), used to determine the relative extremity of unfairness, and standard errors and confidence intervals employing an alternative to the standard bootstrap. We demonstrate application of our framework to a COVID-19 risk prediction model deployed in a major Midwestern health system. ","An intersectional framework for counterfactual fairness in risk prediction provides novel metrics and tools to address bias in health data models."
"A toolbox for surfacing health equity harms and biases in large language models","https://scispace.com/paper/a-toolbox-for-surfacing-health-equity-harms-and-biases-in-1npxi9bix4cx","2024","Journal Article","Nature Medicine","Stephen Pfohl
Heather Cole-Lewis
Rory Sayres
Darlene Neal
Mercy Asiedu
Awa Dieng
Nenad Tomašev
Qazi Mamunur Rashid
Shekoofeh Azizi
Negar Rostamzadeh
Liam G. McCoy
Leo Anthony Celi
Yun Liu
Mike Schaekermann
Alanna Walton
Alicia Parrish
Chirag Nagpal
Rajesh Singh
Akeiylah Dewitt
P. Mansfield
Sushant Prakash
Katherine Heller
Alan Karthikesalingam
Christopher Semturs
Joëlle Barral
Greg S. Corrado
Yossi Matias
Jamila Smith-Loud
Ivor B. Horn
K. K. Singhal","10.1038/s41591-024-03258-2","","Large language models (LLMs) hold promise to serve complex health information needs but also have the potential to introduce harm and exacerbate health disparities. Reliably evaluating equity-related model failures is a critical step toward developing systems that promote health equity. We present resources and methodologies for surfacing biases with potential to precipitate equity-related harms in long-form, LLM-generated answers to medical questions and conduct a large-scale empirical case study with the Med-PaLM 2 LLM. Our contributions include a multifactorial framework for human assessment of LLM-generated answers for biases and EquityMedQA, a collection of seven datasets enriched for adversarial queries. Both our human assessment framework and our dataset design process are grounded in an iterative participatory approach and review of Med-PaLM 2 answers. Through our empirical study, we find that our approach surfaces biases that may be missed by narrower evaluation approaches. Our experience underscores the importance of using diverse assessment methodologies and involving raters of varying backgrounds and expertise. While our approach is not sufficient to holistically assess whether the deployment of an artificial intelligence (AI) system promotes equitable health outcomes, we hope that it can be leveraged and built upon toward a shared goal of LLMs that promote accessible and equitable healthcare. ","Researchers develop a toolbox to identify health equity harms and biases in large language models, presenting a multifactorial framework and a dataset collection to surface potential biases in LLM-generated medical answers."
"Enabling Fairness in Healthcare Through Machine Learning","https://scispace.com/paper/enabling-fairness-in-healthcare-through-machine-learning-14vibtuf","2022","Journal Article","Ethics and Information Technology","Thomas Grote
Geoff Keeling","10.1007/s10676-022-09658-7","https://scispace.com/pdf/enabling-fairness-in-healthcare-through-machine-learning-14vibtuf.pdf","The use of machine learning systems for decision-support in healthcare may exacerbate health inequalities. However, recent work suggests that algorithms trained on sufficiently diverse datasets could in principle combat health inequalities. One concern about these algorithms is that their performance for patients in traditionally disadvantaged groups exceeds their performance for patients in traditionally advantaged groups. This renders the algorithmic decisions unfair relative to the standard fairness metrics in machine learning. In this paper, we defend the permissible use of affirmative algorithms; that is, algorithms trained on diverse datasets that perform better for traditionally disadvantaged groups. Whilst such algorithmic decisions may be unfair, the fairness of algorithmic decisions is not the appropriate locus of moral evaluation. What matters is the fairness of final decisions, such as diagnoses, resulting from collaboration between clinicians and algorithms. We argue that affirmative algorithms can permissibly be deployed provided the resultant final decisions are fair. ","In this paper , the authors defend the permissible use of affirmative algorithms, that is, algorithms trained on diverse datasets that perform better for traditionally disadvantaged groups than for traditionally advantaged groups, arguing that the fairness of algorithmic decisions is not the appropriate locus of moral evaluation."
"A survey of recent methods for addressing AI fairness and bias in biomedicine.","https://scispace.com/paper/a-survey-of-recent-methods-for-addressing-ai-fairness-and-3zgr6f40x9","2024","","Journal of Biomedical Informatics","Yifan Yang
Ming Chi Lin
Yifan Peng
Furong Huang
Zhiyong Lu","10.1016/j.jbi.2024.104646","","Artificial intelligence (AI) systems have the potential to revolutionize clinical practices, including improving diagnostic accuracy and surgical decision-making, while also reducing costs and manpower. However, it is important to recognize that these systems may perpetuate social inequities or demonstrate biases, such as those based on race or gender. Such biases can occur before, during, or after the development of AI models, making it critical to understand and address potential biases to enable the accurate and reliable application of AI models in clinical settings. To mitigate bias concerns during model development, we surveyed recent publications on different debiasing methods in the fields of biomedical natural language processing (NLP) or computer vision (CV). Then we discussed the methods that have been applied in the biomedical domain to address bias. We performed our literature search on PubMed, ACM digital library, and IEEE Xplore of relevant articles published between January 2018 and December 2023 using multiple combinations of keywords. We then filtered the result of 10,041 articles automatically with loose constraints, and manually inspected the abstracts of the remaining 890 articles to identify the 55 articles included in this review. Additional articles in the references are also included in this review. We discuss each method and compare its strengths and weaknesses. Finally, we review other potential methods from the general domain that could be applied to biomedicine to address bias and improve fairness.The bias of AIs in biomedicine can originate from multiple sources. Existing debiasing methods that focus on algorithms can be categorized into distributional or algorithmic. ","To mitigate bias concerns during model development, a survey of recent publications on different debiasing methods in the fields of biomedical natural language processing (NLP) or computer vision (CV) and other potential methods from the general domain that could be applied to biomedicine to address bias and improve fairness are surveyed."
"Minimizing bias when using artificial intelligence in critical care medicine.","https://scispace.com/paper/minimizing-bias-when-using-artificial-intelligence-in-1aru44k04g","2024","Journal Article","Journal of Critical Care","Benjamin L. Ranard
Soojin Park
Yugang Jia
Yiye Zhang
Fatima Alwan
L. A. Celi
Elizabeth R Lusczek","10.1016/j.jcrc.2024.154796","","• As artificial intelligence (AI) use in critical care increases, identifying and minimizing sources of bias is essential • Bias can be introduced (and be minimized) at all stages of the AI lifecycle • Reliance on AI in clinical practice without addressing bias may result in unfair and inequitable treatment of patients • Diverse, multidisciplinary teams that are attuned to sources of bias may have the best chance of minimizing bias in AI ","As AI use in critical care increases, identifying and minimizing bias is crucial to prevent unfair treatment of patients; diverse teams can effectively minimize bias by addressing it at all stages of the AI lifecycle."
"Promoting Equity In Clinical Decision Making: Dismantling Race-Based Medicine.","https://scispace.com/paper/promoting-equity-in-clinical-decision-making-dismantling-3xjr688orv","2023","Journal Article","Health affairs Web exclusive","Tina Hernandez-Boussard
Shazia M. Siddique
Arlene S. Bierman
Maia Hightower
Helen Burstin","10.1377/hlthaff.2023.00545","","As the use of artificial intelligence has spread rapidly throughout the US health care system, concerns have been raised about racial and ethnic biases built into the algorithms that often guide clinical decision making. Race-based medicine, which relies on algorithms that use race as a proxy for biological differences, has led to treatment patterns that are inappropriate, unjust, and harmful to minoritized racial and ethnic groups. These patterns have contributed to persistent disparities in health and health care. To reduce these disparities, we recommend a race-aware approach to clinical decision support that considers social and environmental factors such as structural racism and social determinants of health. Recent policy changes in medical specialty societies and innovations in algorithm development represent progress on the path to dismantling race-based medicine. Success will require continued commitment and sustained efforts among stakeholders in the health care, research, and technology sectors. Increasing the diversity of clinical trial populations, broadening the focus of precision medicine, improving education about the complex factors shaping health outcomes, and developing new guidelines and policies to enable culturally responsive care are important next steps. ","A race-aware approach to clinical decision support that considers social and environmental factors such as structural racism and social determinants of health is recommended to reduce disparities in health and health care."
"Integrating equity, diversity and inclusion throughout the lifecycle of AI within healthcare: a scoping review protocol","https://scispace.com/paper/integrating-equity-diversity-and-inclusion-throughout-the-40mz5i1s90","2023","Journal Article","BMJ Open","Milka Perez Nyariro
Elham Emami
Pascale Caidor
Samira Abbasgholizadeh Rahimi","10.1136/bmjopen-2023-072069","","Introduction Artificial intelligence (AI) has the potential to improve efficiency and quality of care in healthcare settings. The lack of consideration for equity, diversity and inclusion (EDI) in the lifecycle of AI within healthcare settings may intensify social and health inequities, potentially causing harm to under-represented populations. This article describes the protocol for a scoping review of the literature relating to integration of EDI in the AI interventions within healthcare setting. The objective of the review is to evaluate what has been done on integrating EDI concepts, principles and practices in the lifecycles of AI interventions within healthcare settings. It also aims to explore which EDI concepts, principles and practices have been integrated into the design, development and implementation of AI in healthcare settings. Method and analysis The scoping review will be guided by the six-step methodological framework developed by Arksey and O’Malley supplemented by Levac et al, and Joanna Briggs Institute methodological framework for scoping reviews. Relevant literature will be identified by searching seven electronic databases in engineering/computer science and healthcare, and searching the reference lists and citations of studies that meet the inclusion criteria. Studies on AI in any healthcare and geographical settings, that have considered aspects of EDI, published in English and French between 2005 and present will be considered. Two reviewers will independently screen titles, abstracts and full-text articles according to inclusion criteria. We will conduct a thematic analysis and use a narrative description to describe the work. Any disagreements will be resolved through discussion with the third reviewer. Extracted data will be summarised and analysed to address aims of the scoping review. Reporting will follow the Preferred Reporting Items for Systematic Reviews and Meta-analyses Extension for Scoping Reviews. The study began in April 2022 and is expected to end in September 2023. The database initial searches resulted in 5,745 records when piloted in April 2022. Ethics and dissemination Ethical approval is not required. The study will map the available literature on EDI concepts, principles and practices in AI interventions within healthcare settings, highlight the significance of this context, and offer insights into the best practices for incorporating EDI into AI-based solutions in healthcare settings. The results will be disseminated through open-access peer-reviewed publications, conference presentations, social media and 2-day workshops with relevant stakeholders. ","The study will map the available literature on EDI concepts, principles and practices in AI interventions within healthcare settings, highlight the significance of this context, and offer insights into the best practices for incorporating EDI into AI-based solutions in healthcare settings."
"Multidisciplinary considerations of fairness in medical AI: A scoping review.","https://scispace.com/paper/multidisciplinary-considerations-of-fairness-in-medical-ai-a-2wufm59kmc","2023","","International Journal of Medical Informatics","Yue Wang
Yaxin Song
Zhuo Ma
Xiaoxue Han","10.1016/j.ijmedinf.2023.105175","","Artificial Intelligence (AI) technology has been developed significantly in recent years. The fairness of medical AI is of great concern due to its direct relation to human life and health. This review aims to analyze the existing research literature on fairness in medical AI from the perspectives of computer science, medical science, and social science (including law and ethics). The objective of the review is to examine the similarities and differences in the understanding of fairness, explore influencing factors, and investigate potential measures to implement fairness in medical AI across English and Chinese literature. This study employed a scoping review methodology and selected the following databases: Web of Science, MEDLINE, Pubmed, OVID, CNKI, WANFANG Data, etc., for the fairness issues in medical AI through February 2023. The search was conducted using various keywords such as “artificial intelligence,” “machine learning,” “medical,” “algorithm,” “fairness,” “decision-making,” and “bias.” The collected data were charted, synthesized, and subjected to descriptive and thematic analysis. After reviewing 468 English papers and 356 Chinese papers, 53 and 42 were included in the final analysis. Our results show the three different disciplines all show significant differences in the research on the core issues. Data is the foundation that affects medical AI fairness in addition to algorithmic bias and human bias. Legal, ethical, and technological measures all promote the implementation of medical AI fairness. Our review indicates a consensus regarding the importance of data fairness as the foundation for achieving fairness in medical AI across multidisciplinary perspectives. However, there are substantial discrepancies in core aspects such as the concept, influencing factors, and implementation measures of fairness in medical AI. Consequently, future research should facilitate interdisciplinary discussions to bridge the cognitive gaps between different fields and enhance the practical implementation of fairness in medical AI. ","A consensus is indicated regarding the importance of data fairness as the foundation for achieving fairness in medical AI across multidisciplinary perspectives, however, there are substantial discrepancies in core aspects such as the concept, influencing factors, and implementation measures of fairness inmedical AI."
"Promoting Ethical Deployment of Artificial Intelligence and Machine Learning in Healthcare","https://scispace.com/paper/promoting-ethical-deployment-of-artificial-intelligence-and-2x9yvl92","2022","Journal Article","American Journal of Bioethics","Kayte Spector-Bagdady
Vasiliki Rahimzadeh
Kaitlyn Jaffe
Jonathan D. Moreno","10.1080/15265161.2022.2059206","","The ethics of artificial intelligence (AI) and machine learning (ML) exemplify the conceptual struggle between applying familiar pathways of ethical analysis versus generating novel strategies. Melissa McCradden et al.’s “A Research Ethics Framework for the Clinical Translation of Healthcare Machine Learning” puts pressure on this tension while still attempting not to break it—trying to impute structure and epistemic consistency where it is currently lacking (McCradden et al. 2022). They highlight an “AI chasm” “generated by a clash between the... cultures of computer science and clinical science,” but argue that the “ethical norms of human subjects research” are still the right pathway to bridge this divide. The Open Peer Commentaries included in this issue agree with this central premise while critiquing the insufficiency of current ethics and regulatory solutions to adequately protect communities at higher risk for ML bias. The current U.S. human subjects research regulations (HSRR) were developed from traditional conceptions of research ethics (Schupmann and Moreno 2020). Research ethicists have subsequently been asked to apply existing concepts to new areas over which the regulatory structure does not reach. AI/ML is an excellent example of the strengths and limitations of this current default.","McCradden et al. as discussed by the authors argue that the ethical norms of human subjects research are still the right pathway to bridge the divide between the cultures of computer science and clinical science, and the Open Peer Commentaries included in this issue agree with this central premise while critiquing the insufficiency of current ethics and regulatory solutions."
"Harvard Glaucoma Fairness: A Retinal Nerve Disease Dataset for Fairness Learning and Fair Identity Normalization","https://scispace.com/paper/harvard-glaucoma-fairness-a-retinal-nerve-disease-dataset-1crsyncwdw","2024","Journal Article","IEEE Transactions on Medical Imaging","Yan Luo
Yu Tian
Min Shi
Louis R. Pasquale
Lucy Q. Shen
Nazlee Zebardast
Tobias Elze
Mengyu Wang","10.1109/tmi.2024.3377552","","Fairness (also known as equity interchangeably) in machine learning is important for societal wellbeing, but limited public datasets hinder its progress. Currently, no dedicated public medical datasets with imaging data for fairness learning are available, though minority groups suffer from more health issues. To address this gap, we introduce Harvard Glaucoma Fairness (Harvard-GF), a retinal nerve disease dataset including 3,300 subjects with both 2D and 3D imaging data and balanced racial groups for glaucoma detection. Glaucoma is the leading cause of irreversible blindness globally with Blacks having doubled glaucoma prevalence than other races. We also propose a fair identity normalization (FIN) approach to equalize the feature importance between different identity groups. Our FIN approach is compared with various state-of-the-art fairness learning methods with superior performance in the racial, gender, and ethnicity fairness tasks with 2D and 3D imaging data, demonstrating the utilities of our dataset Harvard-GF for fairness learning. To facilitate fairness comparisons between different models, we propose an equity-scaled performance measure, which can be flexibly used to compare all kinds of performance metrics in the context of fairness. The dataset and code are publicly accessible via https://ophai.hms.harvard.edu/harvard-gf3300/. ","Harvard Glaucoma Fairness (Harvard-GF) is a novel retinal nerve disease dataset designed for fairness learning and fair identity normalization. It includes 3,300 subjects with balanced racial groups and offers 2D and 3D imaging data. The dataset facilitates fairness comparisons between different models and includes a novel fair identity normalization approach."
"Artificial intelligence bias in medical system designs: a systematic review","https://scispace.com/paper/artificial-intelligence-bias-in-medical-system-designs-a-492a5ulcww","2023","Journal Article","Multimedia Tools and Applications","Ashish Kumar
Vivekanand Aelgani
Rubeena Vohra
Suneet K. Gupta
Mrinalini Bhagawati
Sudip Paul
Luca Saba
Neha A. Suri
Narendra N. Khanna
John R. Laird
Amer M. Johri
Manudeep S. Kalra
Mostafa M. Fouda
Mostafa Fatemi
Subbaram Naidu
Jasjit S. Suri","10.1007/s11042-023-16029-x","","Inherent bias in the artificial intelligence (AI)-model brings inaccuracies and variabilities during clinical deployment of the model. It is challenging to recognize the source of bias in AI-model due to variations in datasets and black box nature of system design. Additionally, there is no distinct process to identify the potential source of bias in the AI-model. To the best of our knowledge, this is the first review of its kind that addresses the bias in AI-model by categorizing 48 studies into three classes, namely, point-based, image-based, and hybrid-based AI-models. Selection strategy using PRISMA is adopted to select the 72 crucial AI studies for identifying bias in AI models. Using the three classes, bias is identified in these studies based on 44 critical AI attributes. Bias in the AI-models is computed by analytical, butterfly, and ranking-based bias models. These bias models were evaluated using two experts and compared using variability analysis. AI-studies that lacked sufficient AI-attributes are more prone to risk-of-bias (RoB) in all three classes. Studies with high RoB loses fins in the butterfly model. It has been analyzed that the majority of the studies in healthcare suffer from data bias and algorithmic bias due to incomplete specifications mentioned in the design protocol and weak AI design exploited for prediction. ","This is the first review of its kind that addresses the bias in AI-model by categorizing 48 studies into three classes, namely, point-based, image-based, and hybrid-based AI-models, and identifying bias in these studies based on 44 critical AI attributes."
"The Impact of Health Care Algorithms on Racial and Ethnic Disparities : A Systematic Review.","https://scispace.com/paper/the-impact-of-health-care-algorithms-on-racial-and-ethnic-2uh7nki3py","2024","","Annals of Internal Medicine","Shazia M. Siddique
K. Tipton
B. Leas
Christopher Jepson
Jaya Aysola
Jordana B Cohen
Emilia Flores
Michael O Harhay
Harald Schmidt
Gary E Weissman
Julie Fricke
Jonathan R. Treadwell
Nikhil Mull","10.7326/m23-2960","","BACKGROUND
There is increasing concern for the potential impact of health care algorithms on racial and ethnic disparities.


PURPOSE
To examine the evidence on how health care algorithms and associated mitigation strategies affect racial and ethnic disparities.


DATA SOURCES
Several databases were searched for relevant studies published from 1 January 2011 to 30 September 2023.


STUDY SELECTION
Using predefined criteria and dual review, studies were screened and selected to determine: 1) the effect of algorithms on racial and ethnic disparities in health and health care outcomes and 2) the effect of strategies or approaches to mitigate racial and ethnic bias in the development, validation, dissemination, and implementation of algorithms.


DATA EXTRACTION
Outcomes of interest (that is, access to health care, quality of care, and health outcomes) were extracted with risk-of-bias assessment using the ROBINS-I (Risk Of Bias In Non-randomised Studies - of Interventions) tool and adapted CARE-CPM (Critical Appraisal for Racial and Ethnic Equity in Clinical Prediction Models) equity extension.


DATA SYNTHESIS
Sixty-three studies (51 modeling, 4 retrospective, 2 prospective, 5 prepost studies, and 1 randomized controlled trial) were included. Heterogenous evidence on algorithms was found to: a) reduce disparities (for example, the revised kidney allocation system), b) perpetuate or exacerbate disparities (for example, severity-of-illness scores applied to critical care resource allocation), and/or c) have no statistically significant effect on select outcomes (for example, the HEART Pathway [history, electrocardiogram, age, risk factors, and troponin]). To mitigate disparities, 7 strategies were identified: removing an input variable, replacing a variable, adding race, adding a non-race-based variable, changing the racial and ethnic composition of the population used in model development, creating separate thresholds for subpopulations, and modifying algorithmic analytic techniques.


LIMITATION
Results are mostly based on modeling studies and may be highly context-specific.


CONCLUSION
Algorithms can mitigate, perpetuate, and exacerbate racial and ethnic disparities, regardless of the explicit use of race and ethnicity, but evidence is heterogeneous. Intentionality and implementation of the algorithm can impact the effect on disparities, and there may be tradeoffs in outcomes.


PRIMARY FUNDING SOURCE
Agency for Healthcare Quality and Research.","The impact of health care algorithms on racial and ethnic disparities is heterogeneous and depends on the specific algorithm and its implementation. Strategies to mitigate disparities include removing or modifying variables, changing the racial composition of the population used in model development, and modifying algorithmic analytic techniques."
"Unmasking bias in artificial intelligence: a systematic review of bias detection and mitigation strategies in electronic health record-based models.","https://scispace.com/paper/unmasking-bias-in-artificial-intelligence-a-systematic-4i6snyv5xz","2024","Journal Article","Journal of the American Medical Informatics Association","Liqin Wang","10.1093/jamia/ocae060","","OBJECTIVES
Leveraging artificial intelligence (AI) in conjunction with electronic health records (EHRs) holds transformative potential to improve healthcare. However, addressing bias in AI, which risks worsening healthcare disparities, cannot be overlooked. This study reviews methods to handle various biases in AI models developed using EHR data.


MATERIALS AND METHODS
We conducted a systematic review following the Preferred Reporting Items for Systematic Reviews and Meta-analyses guidelines, analyzing articles from PubMed, Web of Science, and IEEE published between January 01, 2010 and December 17, 2023. The review identified key biases, outlined strategies for detecting and mitigating bias throughout the AI model development, and analyzed metrics for bias assessment.


RESULTS
Of the 450 articles retrieved, 20 met our criteria, revealing 6 major bias types: algorithmic, confounding, implicit, measurement, selection, and temporal. The AI models were primarily developed for predictive tasks, yet none have been deployed in real-world healthcare settings. Five studies concentrated on the detection of implicit and algorithmic biases employing fairness metrics like statistical parity, equal opportunity, and predictive equity. Fifteen studies proposed strategies for mitigating biases, especially targeting implicit and selection biases. These strategies, evaluated through both performance and fairness metrics, predominantly involved data collection and preprocessing techniques like resampling and reweighting.


DISCUSSION
This review highlights evolving strategies to mitigate bias in EHR-based AI models, emphasizing the urgent need for both standardized and detailed reporting of the methodologies and systematic real-world testing and evaluation. Such measures are essential for gauging models' practical impact and fostering ethical AI that ensures fairness and equity in healthcare.","A systematic review of bias detection and mitigation strategies in EHR-based AI models identifies key bias types and explores methods for addressing them. The review emphasizes the need for standardized reporting and rigorous testing to ensure fair and equitable AI in healthcare."
"Fairness and bias correction in machine learning for depression prediction across four study populations","https://scispace.com/paper/fairness-and-bias-correction-in-machine-learning-for-1z14n29hh8","2024","Journal Article","Dental science reports","Vien Ngoc Dang
Anna Cascarano
Rosa H. Mulder
Charlotte Cecil
Maria A. Zuluaga
Jerónimo Hernández-González
Karim Lekadir","10.1038/s41598-024-58427-7","","A significant level of stigma and inequality exists in mental healthcare, especially in under-served populations. Inequalities are reflected in the data collected for scientific purposes. When not properly accounted for, machine learning (ML) models learned from data can reinforce these structural inequalities or biases. Here, we present a systematic study of bias in ML models designed to predict depression in four different case studies covering different countries and populations. We find that standard ML approaches regularly present biased behaviors. We also show that mitigation techniques, both standard and our own post-hoc method, can be effective in reducing the level of unfair bias. There is no one best ML model for depression prediction that provides equality of outcomes. This emphasizes the importance of analyzing fairness during model selection and transparent reporting about the impact of debiasing interventions. Finally, we also identify positive habits and open challenges that practitioners could follow to enhance fairness in their models. ","A systematic study of bias in ML models designed to predict depression in four different case studies covering different countries and populations finds that standard ML approaches regularly present biased behaviors."
"Call for algorithmic fairness to mitigate amplification of racial biases in artificial intelligence models used in orthodontics and craniofacial health.","https://scispace.com/paper/call-for-algorithmic-fairness-to-mitigate-amplification-of-4v2jb02iol","2023","","Orthodontics & Craniofacial Research","Veerasathpurush Allareddy
Maysaa Oubaidin
Sankeerth Rampa
Shankar Rengasamy Venugopalan
Mohammed H. Elnagar
Sumit Yadav
Min Kyeong Lee","10.1111/ocr.12721","","Machine Learning (ML), a subfield of Artificial Intelligence (AI), is being increasingly used in Orthodontics and craniofacial health for predicting clinical outcomes. Current ML/AI models are prone to accentuate racial disparities. The objective of this narrative review is to provide an overview of how AI/ML models perpetuate racial biases and how we can mitigate this situation. A narrative review of articles published in the medical literature on racial biases and the use of AI/ML models was undertaken. Current AI/ML models are built on homogenous clinical datasets that have a gross underrepresentation of historically disadvantages demographic groups, especially the ethno-racial minorities. The consequence of such AI/ML models is that they perform poorly when deployed on ethno-racial minorities thus further amplifying racial biases. Healthcare providers, policymakers, AI developers and all stakeholders should pay close attention to various steps in the pipeline of building AI/ML models and every effort must be made to establish algorithmic fairness to redress inequities.","A narrative review of articles published in the medical literature on racial biases and the use of AI/ML models was undertaken to provide an overview of how AI/ ML models perpetuateracial biases and how to mitigate this situation."
"A scoping review of fair machine learning techniques when using real-world data","https://scispace.com/paper/a-scoping-review-of-fair-machine-learning-techniques-when-tqlajp5kxj","2024","Journal Article","Journal of Biomedical Informatics","Yu Huang
Jingchuan Guo
Wei‐Han Chen
Hsuan-Yin Lin
Huilin Tang
Fei Wang
Hua Xu
Jiang Bian","10.1016/j.jbi.2024.104622","","The integration of artificial intelligence (AI) and machine learning (ML) in health care to aid clinical decisions is widespread. However, as AI and ML take important roles in health care, there are concerns about AI and ML associated fairness and bias. That is, an AI tool may have a disparate impact, with its benefits and drawbacks unevenly distributed across societal strata and subpopulations, potentially exacerbating existing health inequities. Thus, the objectives of this scoping review were to summarize existing literature and identify gaps in the topic of tackling algorithmic bias and optimizing fairness in AI/ML models using real-world data (RWD) in health care domains.We conducted a thorough review of techniques for assessing and optimizing AI/ML model fairness in health care when using RWD in health care domains. The focus lies on appraising different quantification metrics for accessing fairness, publicly accessible datasets for ML fairness research, and bias mitigation approaches.We identified 11 papers that are focused on optimizing model fairness in health care applications. The current research on mitigating bias issues in RWD is limited, both in terms of disease variety and health care applications, as well as the accessibility of public datasets for ML fairness research. Existing studies often indicate positive outcomes when using pre-processing techniques to address algorithmic bias. There remain unresolved questions within the field that require further research, which includes pinpointing the root causes of bias in ML models, broadening fairness research in AI/ML with the use of RWD and exploring its implications in healthcare settings, and evaluating and addressing bias in multi-modal data.This paper provides useful reference material and insights to researchers regarding AI/ML fairness in real-world health care data and reveals the gaps in the field. Fair AI/ML in health care is a burgeoning field that requires a heightened research focus to cover diverse applications and different types of RWD. ","A scoping review of fair machine learning techniques when using real-world data in health care identifies gaps in the current research and highlights the need for further investigation into fairness issues in AI/ML models using real-world data in health care domains."
"How Biased is Artificial Intelligence in Healthcare? – An Online Survey (Preprint)","https://scispace.com/paper/how-biased-is-artificial-intelligence-in-healthcare-an-31flesbo","2022","Journal Article","Journal of Medical Internet Research","Carina Vorisek
Caroline Stellmach
Paula Josephine Mayer
Sophie Anne Ines Klopfenstein
Anke Diehl
Maike Henningsen
Sylvia Thun","10.2196/41089","","Background Resources are increasingly spent on artificial intelligence (AI) solutions for medical applications aiming to improve diagnosis, treatment, and prevention of diseases. While the need for transparency and reduction of bias in data and algorithm development has been addressed in past studies, little is known about the knowledge and perception of bias among AI developers. Objective This study’s objective was to survey AI specialists in health care to investigate developers’ perceptions of bias in AI algorithms for health care applications and their awareness and use of preventative measures. Methods A web-based survey was provided in both German and English language, comprising a maximum of 41 questions using branching logic within the REDCap web application. Only the results of participants with experience in the field of medical AI applications and complete questionnaires were included for analysis. Demographic data, technical expertise, and perceptions of fairness, as well as knowledge of biases in AI, were analyzed, and variations among gender, age, and work environment were assessed. Results A total of 151 AI specialists completed the web-based survey. The median age was 30 (IQR 26-39) years, and 67% (101/151) of respondents were male. One-third rated their AI development projects as fair (47/151, 31%) or moderately fair (51/151, 34%), 12% (18/151) reported their AI to be barely fair, and 1% (2/151) not fair at all. One participant identifying as diverse rated AI developments as barely fair, and among the 2 undefined gender participants, AI developments were rated as barely fair or moderately fair, respectively. Reasons for biases selected by respondents were lack of fair data (90/132, 68%), guidelines or recommendations (65/132, 49%), or knowledge (60/132, 45%). Half of the respondents worked with image data (83/151, 55%) from 1 center only (76/151, 50%), and 35% (53/151) worked with national data exclusively. Conclusions This study shows that the perception of biases in AI overall is moderately fair. Gender minorities did not once rate their AI development as fair or very fair. Therefore, further studies need to focus on minorities and women and their perceptions of AI. The results highlight the need to strengthen knowledge about bias in AI and provide guidelines on preventing biases in AI health care applications. ","In this paper , a survey of AI specialists in health care to investigate developers' perceptions of bias in AI algorithms for health care applications and their awareness and use of preventative measures was conducted."
"The limits of fair medical imaging AI in real-world generalization","https://scispace.com/paper/the-limits-of-fair-medical-imaging-ai-in-real-world-20xne0x2wj","2024","Journal Article","Nature Medicine","Yuzhe Yang
Haoran Zhang
Judy Wawira Gichoya
Dina Katabi
Marzyeh Ghassemi","10.1038/s41591-024-03113-4","","Abstract As artificial intelligence (AI) rapidly approaches human-level performance in medical imaging, it is crucial that it does not exacerbate or propagate healthcare disparities. Previous research established AI’s capacity to infer demographic data from chest X-rays, leading to a key concern: do models using demographic shortcuts have unfair predictions across subpopulations? In this study, we conducted a thorough investigation into the extent to which medical AI uses demographic encodings, focusing on potential fairness discrepancies within both in-distribution training sets and external test sets. Our analysis covers three key medical imaging disciplines—radiology, dermatology and ophthalmology—and incorporates data from six global chest X-ray datasets. We confirm that medical imaging AI leverages demographic shortcuts in disease classification. Although correcting shortcuts algorithmically effectively addresses fairness gaps to create ‘locally optimal’ models within the original data distribution, this optimality is not true in new test settings. Surprisingly, we found that models with less encoding of demographic attributes are often most ‘globally optimal’, exhibiting better fairness during model evaluation in new test environments. Our work establishes best practices for medical imaging models that maintain their performance and fairness in deployments beyond their initial training contexts, underscoring critical considerations for AI clinical deployments across populations and sites. ","Medical imaging AI uses demographic shortcuts, leading to fairness discrepancies across subpopulations. Models with less encoding of demographic attributes are often most globally optimal."
"Generative models improve fairness of medical classifiers under distribution shifts","https://scispace.com/paper/generative-models-improve-fairness-of-medical-classifiers-31038qa4of","2024","Journal Article","Nature Medicine","Ira Ktena
Olivia Wiles
Isabela Maria Carneiro Albuquerque
Sylvestre-Alvise Rebuffi
Ryutaro Tanno
Abhijit Guha Roy
Shekoofeh Azizi
Danielle Belgrave
Pushmeet Kohli
Ali Taylan Cemgil
Alan Karthikesalingam
Sven Gowal","10.1038/s41591-024-02838-6","https://scispace.compdf/generative-models-improve-fairness-of-medical-classifiers-31038qa4of.pdf","Abstract Domain generalization is a ubiquitous challenge for machine learning in healthcare. Model performance in real-world conditions might be lower than expected because of discrepancies between the data encountered during deployment and development. Underrepresentation of some groups or conditions during model development is a common cause of this phenomenon. This challenge is often not readily addressed by targeted data acquisition and ‘labeling’ by expert clinicians, which can be prohibitively expensive or practically impossible because of the rarity of conditions or the available clinical expertise. We hypothesize that advances in generative artificial intelligence can help mitigate this unmet need in a steerable fashion, enriching our training dataset with synthetic examples that address shortfalls of underrepresented conditions or subgroups. We show that diffusion models can automatically learn realistic augmentations from data in a label-efficient manner. We demonstrate that learned augmentations make models more robust and statistically fair in-distribution and out of distribution. To evaluate the generality of our approach, we studied three distinct medical imaging contexts of varying difficulty: (1) histopathology, (2) chest X-ray and (3) dermatology images. Complementing real samples with synthetic ones improved the robustness of models in all three medical tasks and increased fairness by improving the accuracy of clinical diagnosis within underrepresented groups, especially out of distribution. ","Generative models improve fairness of medical classifiers under distribution shifts by generating synthetic examples that address underrepresentation of groups or conditions in training data."
"When Your Only Tool Is A Hammer: Ethical Limitations of Algorithmic Fairness Solutions in Healthcare Machine Learning","https://scispace.com/paper/when-your-only-tool-is-a-hammer-ethical-limitations-of-1ys758v571","2020","Proceedings Article","National Conference on Artificial Intelligence","Melissa D McCradden
Mjaye Mazwi
Shalmali Joshi
James A Anderson","10.1145/3375627.3375824","","It is no longer a hypothetical worry that artificial intelligence - more specifically, machine learning (ML) - can propagate the effects of pernicious bias in healthcare. To address these problems, some have proposed the development of 'algorithmic fairness' solutions. The primary goal of these solutions is to constrain the effect of pernicious bias with respect to a given outcome of interest as a function of one's protected identity (i.e., characteristics generally protected by civil or human rights legislation. The technical limitations of these solutions have been well-characterized. Ethically, the problematic implication - of developers, potentially, and end users - is that by virtue of algorithmic fairness solutions a model can be rendered 'objective' (i.e., free from the influence of pernicious bias). The ostensible neutrality of these solutions may unintentionally prompt new consequences for vulnerable groups by obscuring downstream problems due to the persistence of real-world bias. The main epistemic limitation of algorithmic fairness is that it assumes the relationship between the extent of bias's impact on a given health outcome and one's protected identity is mathematically quantifiable. The reality is that social and structural factors confluence in complex and unknown ways to produce health inequalities. Some of these are biologic in nature, and differences like these are directly relevant to predicting a health event and should be incorporated into the model's design. Others are reflective of prejudice, lack of access to healthcare, or implicit bias. Sometimes, there may be a combination. With respect to any specific task, it is difficult to untangle the complex relationships between potentially influential factors and which ones are 'fair' and which are not to inform their inclusion or mitigation in the model's design.","It is no longer a hypothetical worry that artificial intelligence - more specifically, machine learning (ML) - can propagate the effects of pernicious bias in healthcare, and the development of 'algorithmic fairness' solutions to address these problems are proposed."
"Artificial Intelligence and Machine Learning Technologies in Cancer Care: Addressing Disparities, Bias, and Data Diversity","https://scispace.com/paper/artificial-intelligence-and-machine-learning-technologies-in-vx964uav","2022","Journal Article","Cancer Discovery","Irene Dankwa-Mullan
Dilhan Weeraratne","10.1158/2159-8290.CD-22-0373","https://aacrjournals.org/cancerdiscovery/article-pdf/12/6/1423/3153044/1423.pdf","Summary: Artificial intelligence (AI) and machine learning (ML) technologies have not only tremendous potential to augment clinical decision-making and enhance quality care and precision medicine efforts, but also the potential to worsen existing health disparities without a thoughtful, transparent, and inclusive approach that includes addressing bias in their design and implementation along the cancer discovery and care continuum. We discuss applications of AI/ML tools in cancer and provide recommendations for addressing and mitigating potential bias with AI and ML technologies while promoting cancer health equity.","Approaches of AI/ML tools in cancer and recommendations for addressing and mitigating potential bias with AI and ML technologies while promoting cancer health equity are discussed."
"Fairness issues, current approaches, and challenges in machine learning models","https://scispace.com/paper/fairness-issues-current-approaches-and-challenges-in-machine-1hg1tq845q","2024","Journal Article","International Journal of Machine Learning and Cybernetics","Tonni Das Jui
Pablo Rivas","10.1007/s13042-023-02083-2","","Abstract With the increasing influence of machine learning algorithms in decision-making processes, concerns about fairness have gained significant attention. This area now offers significant literature that is complex and hard to penetrate for newcomers to the domain. Thus, a mapping study of articles exploring fairness issues is a valuable tool to provide a general introduction to this field. Our paper presents a systematic approach for exploring existing literature by aligning their discoveries with predetermined inquiries and a comprehensive overview of diverse bias dimensions, encompassing training data bias, model bias, conflicting fairness concepts, and the absence of prediction transparency, as observed across several influential articles. To establish connections between fairness issues and various issue mitigation approaches, we propose a taxonomy of machine learning fairness issues and map the diverse range of approaches scholars developed to address issues. We briefly explain the responsible critical factors behind these issues in a graphical view with a discussion and also highlight the limitations of each approach analyzed in the reviewed articles. Our study leads to a discussion regarding the potential future direction in ML and AI fairness. ","A systematic approach for exploring existing literature by aligning their discoveries with predetermined inquiries and a comprehensive overview of diverse bias dimensions, encompassing training data bias, model bias, conflicting fairness concepts, and the absence of prediction transparency are presented."
"Ethical Redress of Racial Inequities in AI: Lessons from Decoupling Machine Learning from Optimization in Medical Appointment Scheduling","https://scispace.com/paper/ethical-redress-of-racial-inequities-in-ai-lessons-from-3j647vtu","2022","Journal Article","Philosophy & Technology","R. E. Shanklin
Michele Samorani
Shannon L. Harris
Michael D. Santoro","10.1007/s13347-022-00590-8","https://link.springer.com/content/pdf/10.1007/s13347-022-00590-8.pdf","An Artificial Intelligence algorithm trained on data that reflect racial biases may yield racially biased outputs, even if the algorithm on its own is unbiased. For example, algorithms used to schedule medical appointments in the USA predict that Black patients are at a higher risk of no-show than non-Black patients, though technically accurate given existing data that prediction results in Black patients being overwhelmingly scheduled in appointment slots that cause longer wait times than non-Black patients. This perpetuates racial inequity, in this case lesser access to medical care. This gives rise to one type of Accuracy-Fairness trade-off: preserve the efficiency offered by using AI to schedule appointments or discard that efficiency in order to avoid perpetuating ethno-racial disparities. Similar trade-offs arise in a range of AI applications including others in medicine, as well as in education, judicial systems, and public security, among others. This article presents a framework for addressing such trade-offs where Machine Learning and Optimization components of the algorithm are decoupled. Applied to medical appointment scheduling, our framework articulates four approaches intervening in different ways on different components of the algorithm. Each yields specific results, in one case preserving accuracy comparable to the current state-of-the-art while eliminating the disparity. ","In this paper , the authors present a framework for addressing Accuracy-Fairness trade-offs where Machine Learning and Optimization components of the algorithm are decoupled, in order to avoid perpetuating ethno-racial disparities."
"Artificial Intelligence, Heuristic Biases, and the Optimization of Health Outcomes: Cautionary Optimism.","https://scispace.com/paper/artificial-intelligence-heuristic-biases-and-the-iif8vqq6vc","2021","Journal Article","Journal of Clinical Medicine","Michael Feehan
Michael Feehan
Leah A. Owen
Ian M McKinnon
Margaret M. DeAngelis","10.3390/JCM10225284","","The use of artificial intelligence (AI) and machine learning (ML) in clinical care offers great promise to improve patient health outcomes and reduce health inequity across patient populations. However, inherent biases in these applications, and the subsequent potential risk of harm can limit current use. Multi-modal workflows designed to minimize these limitations in the development, implementation, and evaluation of ML systems in real-world settings are needed to improve efficacy while reducing bias and the risk of potential harms. Comprehensive consideration of rapidly evolving AI technologies and the inherent risks of bias, the expanding volume and nature of data sources, and the evolving regulatory landscapes, can contribute meaningfully to the development of AI-enhanced clinical decision making and the reduction in health inequity.","In this paper, a comprehensive consideration of rapidly evolving AI technologies and the inherent risks of bias, the expanding volume and nature of data sources, and the evolving regulatory landscapes, can contribute meaningfully to the development of AI-enhanced clinical decision making and the reduction in health inequity."
"Assessing Biases in Medical Decisions via Clinician and AI Chatbot Responses to Patient Vignettes.","https://scispace.com/paper/assessing-biases-in-medical-decisions-via-clinician-and-ai-3h9r239dqi","2023","Journal Article","JAMA network open","Jiyeong Kim
Z. Cai
Michael L Chen
Julia Simard
Eleni Linos","10.1001/jamanetworkopen.2023.38050","https://jamanetwork.com/journals/jamanetworkopen/articlepdf/2810775/kim_2023_ld_230191_1696951660.74663.pdf","
 This cross-sectional study compares clinician and artificial intelligence (AI) chatbot responses to patient vignettes used to identify bias in medical decisions.
","This study assesses biases in medical decisions by comparing clinician and AI chatbot responses to patient vignettes."
"Generalization—a key challenge for responsible AI in patient-facing clinical applications","https://scispace.com/paper/generalization-a-key-challenge-for-responsible-ai-in-patient-3dp36snwtm","2024","Journal Article","npj digital medicine","Lea Goetz
Nabeel Seedat
Robert Vandersluis
Mihaela van der Schaar","10.1038/s41746-024-01127-3","","Generalization – the ability of AI systems to apply and/or extrapolate their knowledge to new data which might differ from the original training data – is a major challenge for the effective and responsible implementation of human-centric AI applications. Current debate in bioethics proposes selective prediction as a solution. Here we explore data-based reasons for generalization challenges and look at how selective predictions might be implemented technically, focusing on clinical AI applications in real-world healthcare settings. ","Generalization is a major challenge for responsible AI in patient-facing clinical applications, hindering effective implementation, and selective prediction is proposed as a solution, requiring technical implementation in real-world healthcare settings to address data-based generalization challenges."
"Enabling Fairness in Healthcare Through Machine Learning","https://scispace.com/paper/enabling-fairness-in-healthcare-through-machine-learning-2b8s25kq","2022","Journal Article","Ethics and Information Technology","Thomas Grote
Geoff Keeling","10.1007/s10676-022-09658-7","https://scispace.com/pdf/enabling-fairness-in-healthcare-through-machine-learning-2b8s25kq.pdf","The use of machine learning systems for decision-support in healthcare may exacerbate health inequalities. However, recent work suggests that algorithms trained on sufficiently diverse datasets could in principle combat health inequalities. One concern about these algorithms is that their performance for patients in traditionally disadvantaged groups exceeds their performance for patients in traditionally advantaged groups. This renders the algorithmic decisions unfair relative to the standard fairness metrics in machine learning. In this paper, we defend the permissible use of affirmative algorithms; that is, algorithms trained on diverse datasets that perform better for traditionally disadvantaged groups. Whilst such algorithmic decisions may be unfair, the fairness of algorithmic decisions is not the appropriate locus of moral evaluation. What matters is the fairness of final decisions, such as diagnoses, resulting from collaboration between clinicians and algorithms. We argue that affirmative algorithms can permissibly be deployed provided the resultant final decisions are fair. ","In this paper , the authors defend the permissible use of affirmative algorithms, that is, algorithms trained on diverse datasets that perform better for traditionally disadvantaged groups than for traditionally advantaged groups, arguing that the fairness of algorithmic decisions is not the appropriate locus of moral evaluation."
"Exclusion cycles: Reinforcing disparities in medicine","https://scispace.com/paper/exclusion-cycles-reinforcing-disparities-in-medicine-1pnvzq7d","2022","Journal Article","Science","Ana Bracic
Shawneequa L. Callier
W. Nicholson Price","10.1126/science.abo2788","https://scispace.com/pdf/exclusion-cycles-reinforcing-disparities-in-medicine-1pnvzq7d.pdf","Description Clinical practice, data collection, and medical AI constitute self-reinforcing and interacting cycles of exclusion Social exclusion of minoritized populations is a pernicious and intractable problem across different domains, from politics to medical practice. In medicine, substantial disparities exist in both experiences and health outcomes for minoritized populations, with origins in systemic racism, implicit bias, historical practice, and social determinants of health (1, 2). We draw on a theory of “exclusion cycles” developed in the context of nonmedical social interactions (3) to link known dominant-group and minoritized-group behaviors and demonstrate their self-reinforcing interactions. Interlinked cycles help reveal why exclusion and racial disparities are so intractable in medicine, despite efforts to reduce them on the part of physicians and health systems through strategies focused on individual parts of the cycle such as diverse workforce recruitment or implicit bias training (1). This framework highlights particular dangers that may arise through expanding use of big data and artificial intelligence (AI)–based systems in medicine, making bias especially intractable unless tackled directly and early.","Clinical practice, data collection, and medical AI constitute self-reinforcing and interacting cycles of exclusion that contribute to social exclusion."
"A translational perspective towards clinical AI fairness","https://scispace.com/paper/a-translational-perspective-towards-clinical-ai-fairness-4jwh3rl72t","2023","","npj digital medicine","Mingxuan Liu
Yilin Ning
Salinelat Teixayavong
Mayli Mertens
Jie Xu
Daniel Shu Wei Ting
Lionel Tim-Ee Cheng
Jasmine Xue Ling. Ong
Zhen Ling Teo
Ting Fang Tan
Narrendar RaviChandran
Fei Wang
Leo Anthony Celi
Marcus Ong
Nan Liu","10.1038/s41746-023-00918-4","","Abstract Artificial intelligence (AI) has demonstrated the ability to extract insights from data, but the fairness of such data-driven insights remains a concern in high-stakes fields. Despite extensive developments, issues of AI fairness in clinical contexts have not been adequately addressed. A fair model is normally expected to perform equally across subgroups defined by sensitive variables (e.g., age, gender/sex, race/ethnicity, socio-economic status, etc.). Various fairness measurements have been developed to detect differences between subgroups as evidence of bias, and bias mitigation methods are designed to reduce the differences detected. This perspective of fairness, however, is misaligned with some key considerations in clinical contexts. The set of sensitive variables used in healthcare applications must be carefully examined for relevance and justified by clear clinical motivations. In addition, clinical AI fairness should closely investigate the ethical implications of fairness measurements (e.g., potential conflicts between group- and individual-level fairness) to select suitable and objective metrics. Generally defining AI fairness as “equality” is not necessarily reasonable in clinical settings, as differences may have clinical justifications and do not indicate biases. Instead, “equity” would be an appropriate objective of clinical AI fairness. Moreover, clinical feedback is essential to developing fair and well-performing AI models, and efforts should be made to actively involve clinicians in the process. The adaptation of AI fairness towards healthcare is not self-evident due to misalignments between technical developments and clinical considerations. Multidisciplinary collaboration between AI researchers, clinicians, and ethicists is necessary to bridge the gap and translate AI fairness into real-life benefits. ","The adaptation of AI fairness towards healthcare is not self-evident due to misalignments between technical developments and clinical considerations, and efforts should be made to actively involve clinicians in the process."
"Mitigating bias in AI at the point of care","https://scispace.com/paper/mitigating-bias-in-ai-at-the-point-of-care-260yc08g","2023","Journal Article","Science","Matthew DeCamp
Charlotta Lindvall","10.1126/science.adh2713","","Description Promoting equity in AI in health care requires addressing biases at cli nical implementation Artificial intelligence (AI) shows promise for improving basic and translational science, medicine, and public health, but its success is not guaranteed. Numerous examples have arisen of racial, ethnic, gender, disability, and other biases in AI applications to health care. In ethics, bias generally refers to any systematic, unfair favoring of people in terms of how they are treated or the outcomes they experience. Consensus has emerged among scientists, ethicists, and policy-makers that minimizing bias is a shared responsibility among all involved in AI development. For example, ensuring equity by eliminating bias in AI is a core principle of the World Health Organization for governing AI (1). But ensuring equity will require more than unbiased data and algorithms. It will also require reducing biases in how clinicians and patients use AI-based algorithms—a potentially more challenging task than reducing biases in algorithms themselves.","In this paper , the authors argue that ensuring equity will require more than unbiased data and algorithms, it will also require reducing biases in how clinicians and patients use AI-based algorithms."
"Mitigating Bias in Algorithmic Systems—A Fish-eye View","https://scispace.com/paper/mitigating-bias-in-algorithmic-systems-a-fish-eye-view-16bp8zez","2022","Journal Article","ACM Computing Surveys","","10.1145/3527152","https://scispace.com/pdf/mitigating-bias-in-algorithmic-systems-a-fish-eye-view-16bp8zez.pdf","Mitigating bias in algorithmic systems is a critical issue drawing attention across communities within the information and computer sciences. Given the complexity of the problem and the involvement of multiple stakeholders—including developers, end users, and third-parties—there is a need to understand the landscape of the sources of bias, and the solutions being proposed to address them, from a broad, cross-domain perspective. This survey provides a “fish-eye view,” examining approaches across four areas of research. The literature describes three steps toward a comprehensive treatment—bias detection, fairness management, and explainability management—and underscores the need to work from within the system as well as from the perspective of stakeholders in the broader context. ","The literature describes three steps toward a comprehensive treatment of bias detection, fairness management, and explainability management as discussed by the authors , and emphasizes the need to work from within the system as well as from the perspective of stakeholders in the broader context."
"Mitigating Racial And Ethnic Bias And Advancing Health Equity In Clinical Algorithms: A Scoping Review.","https://scispace.com/paper/mitigating-racial-and-ethnic-bias-and-advancing-health-3r7to2dhqe","2023","Journal Article","Health affairs Web exclusive","Michael P Cary
Anna Zink
Sijia Wei
Andrew Olson
Mengying Yan
Rashaud Senior
Sophia Bessias
Kais Gadhoumi
Genevieve Jean-Pierre
Demy Wang
Leila S Ledbetter
Nicoleta Economou-Zavlanos
Ziad Obermeyer
M. Pencina","10.1377/hlthaff.2023.00553","","In August 2022 the Department of Health and Human Services (HHS) issued a notice of proposed rulemaking prohibiting covered entities, which include health care providers and health plans, from discriminating against individuals when using clinical algorithms in decision making. However, HHS did not provide specific guidelines on how covered entities should prevent discrimination. We conducted a scoping review of literature published during the period 2011-22 to identify health care applications, frameworks, reviews and perspectives, and assessment tools that identify and mitigate bias in clinical algorithms, with a specific focus on racial and ethnic bias. Our scoping review encompassed 109 articles comprising 45 empirical health care applications that included tools tested in health care settings, 16 frameworks, and 48 reviews and perspectives. We identified a wide range of technical, operational, and systemwide bias mitigation strategies for clinical algorithms, but there was no consensus in the literature on a single best practice that covered entities could employ to meet the HHS requirements. Future research should identify optimal bias mitigation methods for various scenarios, depending on factors such as patient population, clinical setting, algorithm design, and types of bias to be addressed. ","A scoping review of literature published during the period 2011-22 identified health care applications, frameworks, reviews and perspectives, and assessment tools that identify and mitigate bias in clinical algorithms, with a specific focus on racial and ethnic bias."
"The impact of artificial intelligence on health equity in oncology: A scoping review (Preprint)","https://scispace.com/paper/the-impact-of-artificial-intelligence-on-health-equity-in-1uwcrkz9","2022","Journal Article","Journal of Medical Internet Research","Paul Istasy
Wen-Shen Lee
Alla Iansavichene
Ross E.G. Upshur
Bishal Gyawali
Jacquelyn Burkell
Bekim Sadikovic
Alejandro Lazo-Langner
Benjamin Chin-Yee","10.2196/39748","https://scispace.com/pdf/the-impact-of-artificial-intelligence-on-health-equity-in-1uwcrkz9.pdf","The field of oncology is at the forefront of advances in artificial intelligence (AI) in health care, providing an opportunity to examine the early integration of these technologies in clinical research and patient care. Hope that AI will revolutionize health care delivery and improve clinical outcomes has been accompanied by concerns about the impact of these technologies on health equity.We aimed to conduct a scoping review of the literature to address the question, ""What are the current and potential impacts of AI technologies on health equity in oncology?""Following PRISMA-ScR (Preferred Reporting Items for Systematic Reviews and Meta-Analyses extension for Scoping Reviews) guidelines for scoping reviews, we systematically searched MEDLINE and Embase electronic databases from January 2000 to August 2021 for records engaging with key concepts of AI, health equity, and oncology. We included all English-language articles that engaged with the 3 key concepts. Articles were analyzed qualitatively for themes pertaining to the influence of AI on health equity in oncology.Of the 14,011 records, 133 (0.95%) identified from our review were included. We identified 3 general themes in the literature: the use of AI to reduce health care disparities (58/133, 43.6%), concerns surrounding AI technologies and bias (16/133, 12.1%), and the use of AI to examine biological and social determinants of health (55/133, 41.4%). A total of 3% (4/133) of articles focused on many of these themes.Our scoping review revealed 3 main themes on the impact of AI on health equity in oncology, which relate to AI's ability to help address health disparities, its potential to mitigate or exacerbate bias, and its capability to help elucidate determinants of health. Gaps in the literature included a lack of discussion of ethical challenges with the application of AI technologies in low- and middle-income countries, lack of discussion of problems of bias in AI algorithms, and a lack of justification for the use of AI technologies over traditional statistical methods to address specific research questions in oncology. Our review highlights a need to address these gaps to ensure a more equitable integration of AI in cancer research and clinical practice. The limitations of our study include its exploratory nature, its focus on oncology as opposed to all health care sectors, and its analysis of solely English-language articles. ","This paper conducted a scoping review of the literature to address the question, ""What are the current and potential impacts of AI technologies on health equity in oncology?""Following the PRISMA-ScR (Preferred Reporting Items for Systematic Reviews and Meta-Analyses extension for Scoping Reviews) guidelines for scoping reviews, systematically searched MEDLINE and Embase electronic databases from January 2000 to August 2021 for records engaging with key concepts of AI, health equity, and cancer."
"MEDFAIR: Benchmarking Fairness for Medical Imaging","https://scispace.com/paper/medfair-benchmarking-fairness-for-medical-imaging-1562ntvz","2022","Proceedings Article","International Conference on Learning Representations","Yongshuo Zong
Yongxin Yang
Timothy M. Hospedales","10.48550/arXiv.2210.01725","","A multitude of work has shown that machine learning-based medical diagnosis systems can be biased against certain subgroups of people. This has motivated a growing number of bias mitigation algorithms that aim to address fairness issues in machine learning. However, it is difficult to compare their effectiveness in medical imaging for two reasons. First, there is little consensus on the criteria to assess fairness. Second, existing bias mitigation algorithms are developed under different settings, e.g., datasets, model selection strategies, backbones, and fairness metrics, making a direct comparison and evaluation based on existing results impossible. In this work, we introduce MEDFAIR, a framework to benchmark the fairness of machine learning models for medical imaging. MEDFAIR covers eleven algorithms from various categories, nine datasets from different imaging modalities, and three model selection criteria. Through extensive experiments, we find that the under-studied issue of model selection criterion can have a significant impact on fairness outcomes; while in contrast, state-of-the-art bias mitigation algorithms do not significantly improve fairness outcomes over empirical risk minimization (ERM) in both in-distribution and out-of-distribution settings. We evaluate fairness from various perspectives and make recommendations for different medical application scenarios that require different ethical principles. Our framework provides a reproducible and easy-to-use entry point for the development and evaluation of future bias mitigation algorithms in deep learning. Code is available at https://github.com/ys-zong/MEDFAIR.","MEDFAIR, a framework to benchmark the fairness of machine learning models for medical imaging, provides a reproducible and easy-to-use entry point for the development and evaluation of future bias mitigation algorithms in deep learning."
"Those designing healthcare algorithms must become actively anti-racist","https://scispace.com/paper/those-designing-healthcare-algorithms-must-become-actively-25sdo74rin","2020","Journal Article","Nature Medicine","Kellie Owens
Alexis Walker","10.1038/S41591-020-1020-3","","Many widely used health algorithms have been shown to encode and reinforce racial health inequities, prioritizing the needs of white patients over those of patients of color. Because automated systems are becoming so crucial to access to health, researchers in the field of artificial intelligence must become actively anti-racist. Here we list some concrete steps to enable anti-racist practices in medical research and practice.","Many widely used health algorithms have been shown to encode and reinforce racial health inequities, prioritizing the needs of white patients over those of patients of color."
"Peeking into a black box, the fairness and generalizability of a MIMIC-III benchmarking model","https://scispace.com/paper/peeking-into-a-black-box-the-fairness-and-generalizability-1ce4k8y5","2022","Journal Article","Scientific Data","Eliane Röösli
Selen Bozkurt
Tina Hernandez-Boussard","10.1038/s41597-021-01110-7","https://scispace.com/pdf/peeking-into-a-black-box-the-fairness-and-generalizability-1ce4k8y5.pdf","As artificial intelligence (AI) makes continuous progress to improve quality of care for some patients by leveraging ever increasing amounts of digital health data, others are left behind. Empirical evaluation studies are required to keep biased AI models from reinforcing systemic health disparities faced by minority populations through dangerous feedback loops. The aim of this study is to raise broad awareness of the pervasive challenges around bias and fairness in risk prediction models. We performed a case study on a MIMIC-trained benchmarking model using a broadly applicable fairness and generalizability assessment framework. While open-science benchmarks are crucial to overcome many study limitations today, this case study revealed a strong class imbalance problem as well as fairness concerns for Black and publicly insured ICU patients. Therefore, we advocate for the widespread use of comprehensive fairness and performance assessment frameworks to effectively monitor and validate benchmark pipelines built on open data resources. ","In this article , the authors performed a case study on a MIMIC-trained benchmarking model using a broadly applicable fairness and generalizability assessment framework and revealed a strong class imbalance problem as well as fairness concerns for Black and publicly insured ICU patients."
"Peeking into a black box, the fairness and generalizability of a MIMIC-III benchmarking model","https://scispace.com/paper/peeking-into-a-black-box-the-fairness-and-generalizability-e7nm6at8","2022","Journal Article","Scientific Data","Eliane Röösli
Selen Bozkurt
Tina Hernandez-Boussard","10.1038/s41597-021-01110-7","https://scispace.com/pdf/peeking-into-a-black-box-the-fairness-and-generalizability-e7nm6at8.pdf","As artificial intelligence (AI) makes continuous progress to improve quality of care for some patients by leveraging ever increasing amounts of digital health data, others are left behind. Empirical evaluation studies are required to keep biased AI models from reinforcing systemic health disparities faced by minority populations through dangerous feedback loops. The aim of this study is to raise broad awareness of the pervasive challenges around bias and fairness in risk prediction models. We performed a case study on a MIMIC-trained benchmarking model using a broadly applicable fairness and generalizability assessment framework. While open-science benchmarks are crucial to overcome many study limitations today, this case study revealed a strong class imbalance problem as well as fairness concerns for Black and publicly insured ICU patients. Therefore, we advocate for the widespread use of comprehensive fairness and performance assessment frameworks to effectively monitor and validate benchmark pipelines built on open data resources. ","In this article , the authors performed a case study on a MIMIC-trained benchmarking model using a broadly applicable fairness and generalizability assessment framework and revealed a strong class imbalance problem as well as fairness concerns for Black and publicly insured ICU patients."
"Algorithmic fairness and bias mitigation for clinical machine learning with deep reinforcement learning","https://scispace.com/paper/algorithmic-fairness-and-bias-mitigation-for-clinical-21u98t761w","2023","Journal Article","Nature Machine Intelligence","Jenny Yang
Andrew Soltan
David W Eyre
David A. Clifton","10.1038/s42256-023-00697-3","","Abstract As models based on machine learning continue to be developed for healthcare applications, greater effort is needed to ensure that these technologies do not reflect or exacerbate any unwanted or discriminatory biases that may be present in the data. Here we introduce a reinforcement learning framework capable of mitigating biases that may have been acquired during data collection. In particular, we evaluated our model for the task of rapidly predicting COVID-19 for patients presenting to hospital emergency departments and aimed to mitigate any site (hospital)-specific and ethnicity-based biases present in the data. Using a specialized reward function and training procedure, we show that our method achieves clinically effective screening performances, while significantly improving outcome fairness compared with current benchmarks and state-of-the-art machine learning methods. We performed external validation across three independent hospitals, and additionally tested our method on a patient intensive care unit discharge status task, demonstrating model generalizability. ","A reinforcement-learning-based method for algorithmic bias mitigation is proposed and demonstrated on COVID-19 screening and patient discharge prediction tasks and achieves clinically effective screening performances while significantly improving outcome fairness."
"Considering Biased Data as Informative Artifacts in AI-Assisted Health Care.","https://scispace.com/paper/considering-biased-data-as-informative-artifacts-in-ai-50eu32wcc8","2023","","The New England Journal of Medicine","Kadija Ferryman
Maxine Mackintosh
Marzyeh Ghassemi","10.1056/nejmra2214964","","Medical AI tools that are trained on skewed data may exhibit bias. The authors discuss how thinking of clinical data as artifacts can identify values, practices, and patterns of inequity in health care. ","Researchers propose rethinking biased clinical data as informative artifacts in AI-assisted healthcare, highlighting values, practices, and patterns of inequity to identify and address biases in medical AI tools trained on skewed data."
"Addressing Algorithmic Bias and the Perpetuation of Health Inequities: An AI Bias Aware Framework","https://scispace.com/paper/addressing-algorithmic-bias-and-the-perpetuation-of-health-3ss8thkw","2022","Journal Article","Health policy and technology","Ria Agarwal
Maria Bjarnadottir
Lauren Rhue
Michelle Dugas
Kenyon Crowley
John Clark
Guanju Gao","10.1016/j.hlpt.2022.100702","","The emergence and increasing use of artificial intelligence and machine learning (AI/ML) in healthcare practice and delivery is being greeted with both optimism and caution. We focus on the nexus of AI/ML and racial disparities in healthcare: an issue that must be addressed if the promise of AI to improve patient care and health outcomes is to be realized in an equitable manner for all populations. We unpack the challenge of algorithmic bias that may perpetuate health disparities. Synthesizing research from multiple disciplines, we describe a four-step analytical process used to build and deploy AI/ML algorithms and solutions, highlighting both the sources of bias as well as methods for bias mitigation. Finally, we offer recommendations for moving the pursuit of fairness further. ","The authors unpack the challenge of algorithmic bias that may perpetuate health disparities and propose a four-step analytical process used to build and deploy AI/ML algorithms and solutions, highlighting both the sources of bias as well as methods for bias mitigation."
"Medical artificial intelligence ethics: A systematic review of empirical studies","https://scispace.com/paper/medical-artificial-intelligence-ethics-a-systematic-review-1w6rftvi","2023","Journal Article","Digital health","Lu Tang
Jinxu Li
Sophia Fantus","10.1177/20552076231186064","https://journals.sagepub.com/doi/pdf/10.1177/20552076231186064","Background Artificial intelligence (AI) technologies are transforming medicine and healthcare. Scholars and practitioners have debated the philosophical, ethical, legal, and regulatory implications of medical AI, and empirical research on stakeholders’ knowledge, attitude, and practices has started to emerge. This study is a systematic review of published empirical studies of medical AI ethics with the goal of mapping the main approaches, findings, and limitations of scholarship to inform future practice considerations. Methods We searched seven databases for published peer-reviewed empirical studies on medical AI ethics and evaluated them in terms of types of technologies studied, geographic locations, stakeholders involved, research methods used, ethical principles studied, and major findings. Findings Thirty-six studies were included (published 2013-2022). They typically belonged to one of the three topics: exploratory studies of stakeholder knowledge and attitude toward medical AI, theory-building studies testing hypotheses regarding factors contributing to stakeholders’ acceptance of medical AI, and studies identifying and correcting bias in medical AI. Interpretation There is a disconnect between high-level ethical principles and guidelines developed by ethicists and empirical research on the topic and a need to embed ethicists in tandem with AI developers, clinicians, patients, and scholars of innovation and technology adoption in studying medical AI ethics.","A systematic review of published empirical studies of medical AI ethics with the goal of mapping the main approaches, findings, and limitations of scholarship to inform future practice considerations is presented in this article ."
"FairLens: Auditing black-box clinical decision support systems","https://scispace.com/paper/fairlens-auditing-black-box-clinical-decision-support-2pm5vowtot","2021","Journal Article","Information Processing and Management","Cecilia Panigutti
Alan Perotti
André Panisson
Paolo Bajardi
Dino Pedreschi","10.1016/J.IPM.2021.102657","https://arxiv.org/pdf/2011.04049","The pervasive application of algorithmic decision-making is raising concerns on the risk of unintended bias in AI systems deployed in critical settings such as healthcare. The detection and mitigation of model bias is a very delicate task that should be tackled with care and involving domain experts in the loop. In this paper we introduce FairLens, a methodology for discovering and explaining biases. We show how this tool can audit a fictional commercial black-box model acting as a clinical decision support system (DSS). In this scenario, the healthcare facility experts can use FairLens on their historical data to discover the biases of the model before incorporating it into the clinical decision flow. FairLens first stratifies the available patient data according to demographic attributes such as age, ethnicity, gender and healthcare insurance; it then assesses the model performance on such groups highlighting the most common misclassifications. Finally, FairLens allows the expert to examine one misclassification of interest by explaining which elements of the affected patients’ clinical history drive the model error in the problematic group. We validate FairLens’ ability to highlight bias in multilabel clinical DSSs introducing a multilabel-appropriate metric of disparity and proving its efficacy against other standard metrics.","In this paper, the authors introduce FairLens, a methodology for discovering and explaining biases in a clinical decision support system (DSS), which can audit a fictional commercial black-box model acting as a clinical DSS."
"An adversarial training framework for mitigating algorithmic biases in clinical machine learning","https://scispace.com/paper/an-adversarial-training-framework-for-mitigating-algorithmic-2ez6fltm","2023","Journal Article","npj digital medicine","Jenny Yang
Andrew Soltan
David W Eyre
Yang Yang
David A. Clifton","10.1038/s41746-023-00805-y","https://scispace.com/pdf/an-adversarial-training-framework-for-mitigating-algorithmic-2ez6fltm.pdf","Machine learning is becoming increasingly prominent in healthcare. Although its benefits are clear, growing attention is being given to how these tools may exacerbate existing biases and disparities. In this study, we introduce an adversarial training framework that is capable of mitigating biases that may have been acquired through data collection. We demonstrate this proposed framework on the real-world task of rapidly predicting COVID-19, and focus on mitigating site-specific (hospital) and demographic (ethnicity) biases. Using the statistical definition of equalized odds, we show that adversarial training improves outcome fairness, while still achieving clinically-effective screening performances (negative predictive values >0.98). We compare our method to previous benchmarks, and perform prospective and external validation across four independent hospital cohorts. Our method can be generalized to any outcomes, models, and definitions of fairness. ","In this article , the authors proposed an adversarial training framework that is capable of mitigating biases that may have been acquired through data collection, and demonstrated this proposed framework on the real-world task of rapidly predicting COVID-19, and focus on mitigating site-specific (hospital) and demographic (ethnicity) biases."
"Guiding Principles to Address the Impact of Algorithm Bias on Racial and Ethnic Disparities in Health and Health Care.","https://scispace.com/paper/guiding-principles-to-address-the-impact-of-algorithm-bias-2q9ts0toap","2023","Journal Article","JAMA network open","Marshall H Chin
Nasim Afsar-Manesh
Arlene S. Bierman
Christine Chang
Caleb J Colón-Rodríguez
Prashila Dullabh
Deborah Guadalupe Duran
Malika Fair
Tina Hernandez-Boussard
Maia Hightower
Anjali Jain
William B Jordan
Stephen Konya
Roslyn Holliday Moore
Tamra Tyree Moore
Richard Rodriguez
Gauher Shaheen
Lynne Page Snyder
Mithuna Srinivasan
Craig A Umscheid
Lucila Ohno-Machado","10.1001/jamanetworkopen.2023.45050","","Importance
Health care algorithms are used for diagnosis, treatment, prognosis, risk stratification, and allocation of resources. Bias in the development and use of algorithms can lead to worse outcomes for racial and ethnic minoritized groups and other historically marginalized populations such as individuals with lower income.


Objective
To provide a conceptual framework and guiding principles for mitigating and preventing bias in health care algorithms to promote health and health care equity.


Evidence Review
The Agency for Healthcare Research and Quality and the National Institute for Minority Health and Health Disparities convened a diverse panel of experts to review evidence, hear from stakeholders, and receive community feedback.


Findings
The panel developed a conceptual framework to apply guiding principles across an algorithm's life cycle, centering health and health care equity for patients and communities as the goal, within the wider context of structural racism and discrimination. Multiple stakeholders can mitigate and prevent bias at each phase of the algorithm life cycle, including problem formulation (phase 1); data selection, assessment, and management (phase 2); algorithm development, training, and validation (phase 3); deployment and integration of algorithms in intended settings (phase 4); and algorithm monitoring, maintenance, updating, or deimplementation (phase 5). Five principles should guide these efforts: (1) promote health and health care equity during all phases of the health care algorithm life cycle; (2) ensure health care algorithms and their use are transparent and explainable; (3) authentically engage patients and communities during all phases of the health care algorithm life cycle and earn trustworthiness; (4) explicitly identify health care algorithmic fairness issues and trade-offs; and (5) establish accountability for equity and fairness in outcomes from health care algorithms.


Conclusions and Relevance
Multiple stakeholders must partner to create systems, processes, regulations, incentives, standards, and policies to mitigate and prevent algorithmic bias. Reforms should implement guiding principles that support promotion of health and health care equity in all phases of the algorithm life cycle as well as transparency and explainability, authentic community engagement and ethical partnerships, explicit identification of fairness issues and trade-offs, and accountability for equity and fairness.","A conceptual framework to apply guiding principles across an algorithm's life cycle is developed, centering health and health care equity for patients and communities as the goal, within the wider context of structural racism and discrimination."
"Advancing health equity with artificial intelligence.","https://scispace.com/paper/advancing-health-equity-with-artificial-intelligence-4c235wu4t7","2021","Journal Article","Journal of Public Health Policy","Nicole M. Thomasian
Carsten Eickhoff
Eli Y. Adashi","10.1057/S41271-021-00319-5","","Population and public health are in the midst of an artificial intelligence revolution capable of radically altering existing models of care delivery and practice. Just as AI seeks to mirror human cognition through its data-driven analytics, it can also reflect the biases present in our collective conscience. In this Viewpoint, we use past and counterfactual examples to illustrate the sequelae of unmitigated bias in healthcare artificial intelligence. Past examples indicate that if the benefits of emerging AI technologies are to be realized, consensus around the regulation of algorithmic bias at the policy level is needed to ensure their ethical integration into the health system. This paper puts forth regulatory strategies for uprooting bias in healthcare AI that can inform ongoing efforts to establish a framework for federal oversight. We highlight three overarching oversight principles in bias mitigation that maps to each phase of the algorithm life cycle.","In this article, the authors use past and counterfactual examples to illustrate the sequelae of unmitigated bias in healthcare artificial intelligence and highlight three overarching oversight principles in bias mitigation that map to each phase of the algorithm life cycle."
"Human-Centered Design to Address Biases in Artificial Intelligence","https://scispace.com/paper/human-centered-design-to-address-biases-in-artificial-3ljwre8o","2023","Journal Article","Journal of Medical Internet Research","You Chen
Ellen Wright Clayton
Laurie L. Novak
Shilo Anders
Bradley A. Malin","10.2196/43251","","The potential of artificial intelligence (AI) to reduce health care disparities and inequities is recognized, but it can also exacerbate these issues if not implemented in an equitable manner. This perspective identifies potential biases in each stage of the AI life cycle, including data collection, annotation, machine learning model development, evaluation, deployment, operationalization, monitoring, and feedback integration. To mitigate these biases, we suggest involving a diverse group of stakeholders, using human-centered AI principles. Human-centered AI can help ensure that AI systems are designed and used in a way that benefits patients and society, which can reduce health disparities and inequities. By recognizing and addressing biases at each stage of the AI life cycle, AI can achieve its potential in health care.","In this paper , the authors identify potential biases in each stage of the AI life cycle, including data collection, annotation, machine learning model development, evaluation, deployment, operationalization, monitoring, and feedback integration, and suggest involving a diverse group of stakeholders using human-centered AI principles."
"Fairness in Machine Learning for Healthcare","https://scispace.com/paper/fairness-in-machine-learning-for-healthcare-32nvu7vdz4","2020","Proceedings Article","Knowledge Discovery and Data Mining","Muhammad Aurangzeb Ahmad
Arpit Patel
Carly Eckert
Vikas Kumar
Ankur Teredesai","10.1145/3394486.3406461","","The issue of bias and fairness in healthcare has been around for centuries. With the integration of AI in healthcare the potential to discriminate and perpetuate unfair and biased practices in healthcare increases many folds The tutorial focuses on the challenges, requirements and opportunities in the area of fairness in healthcare AI and the various nuances associated with it. The problem healthcare as a multi-faceted systems level problem that necessitates careful of different notions of fairness in healthcare to corresponding concepts in machine learning is elucidated via different real world examples.","The problem healthcare as a multi-faceted systems level problem that necessitates careful of different notions of fairness in healthcare to corresponding concepts in machine learning is elucidated via different real world examples."
"Beyond bias and discrimination: redefining the AI ethics principle of fairness in healthcare machine-learning algorithms","https://scispace.com/paper/beyond-bias-and-discrimination-redefining-the-ai-ethics-2pldqiaj","2022","Journal Article","Ai & Society","Benedetta Giovanola
Simona Tiribelli","10.1007/s00146-022-01455-6","https://scispace.com/pdf/beyond-bias-and-discrimination-redefining-the-ai-ethics-2pldqiaj.pdf","The increasing implementation of and reliance on machine-learning (ML) algorithms to perform tasks, deliver services and make decisions in health and healthcare have made the need for fairness in ML, and more specifically in healthcare ML algorithms (HMLA), a very important and urgent task. However, while the debate on fairness in the ethics of artificial intelligence (AI) and in HMLA has grown significantly over the last decade, the very concept of fairness as an ethical value has not yet been sufficiently explored. Our paper aims to fill this gap and address the AI ethics principle of fairness from a conceptual standpoint, drawing insights from accounts of fairness elaborated in moral philosophy and using them to conceptualise fairness as an ethical value and to redefine fairness in HMLA accordingly. To achieve our goal, following a first section aimed at clarifying the background, methodology and structure of the paper, in the second section, we provide an overview of the discussion of the AI ethics principle of fairness in HMLA and show that the concept of fairness underlying this debate is framed in purely distributive terms and overlaps with non-discrimination, which is defined in turn as the absence of biases. After showing that this framing is inadequate, in the third section, we pursue an ethical inquiry into the concept of fairness and argue that fairness ought to be conceived of as an ethical value. Following a clarification of the relationship between fairness and non-discrimination, we show that the two do not overlap and that fairness requires much more than just non-discrimination. Moreover, we highlight that fairness not only has a distributive but also a socio-relational dimension. Finally, we pinpoint the constitutive components of fairness. In doing so, we base our arguments on a renewed reflection on the concept of respect, which goes beyond the idea of equal respect to include respect for individual persons. In the fourth section, we analyse the implications of our conceptual redefinition of fairness as an ethical value in the discussion of fairness in HMLA. Here, we claim that fairness requires more than non-discrimination and the absence of biases as well as more than just distribution; it needs to ensure that HMLA respects persons both as persons and as particular individuals. Finally, in the fifth section, we sketch some broader implications and show how our inquiry can contribute to making HMLA and, more generally, AI promote the social good and a fairer society. ","In this article , the authors define the concept of fairness as an ethical value in the context of healthcare ML algorithms, and argue that fairness should be defined as a socio-relational value rather than a distributive value."
"Considerations for addressing bias in artificial intelligence for health equity.","https://scispace.com/paper/considerations-for-addressing-bias-in-artificial-v8lpjq2jja","2023","","npj digital medicine","Michael D Abràmoff
Michelle E. Tarver
Nilsa Loyo-Berrios
Sylvia Trujillo
Danton Char
Ziad Obermeyer
Malvina B. Eydelman
William H Maisel","10.1038/s41746-023-00913-9","","Abstract Health equity is a primary goal of healthcare stakeholders: patients and their advocacy groups, clinicians, other providers and their professional societies, bioethicists, payors and value based care organizations, regulatory agencies, legislators, and creators of artificial intelligence/machine learning (AI/ML)-enabled medical devices. Lack of equitable access to diagnosis and treatment may be improved through new digital health technologies, especially AI/ML, but these may also exacerbate disparities, depending on how bias is addressed. We propose an expanded Total Product Lifecycle (TPLC) framework for healthcare AI/ML, describing the sources and impacts of undesirable bias in AI/ML systems in each phase, how these can be analyzed using appropriate metrics, and how they can be potentially mitigated. The goal of these “Considerations” is to educate stakeholders on how potential AI/ML bias may impact healthcare outcomes and how to identify and mitigate inequities; to initiate a discussion between stakeholders on these issues, in order to ensure health equity along the expanded AI/ML TPLC framework, and ultimately, better health outcomes for all. ","The goal of these “Considerations” is to educate stakeholders on how potential AI/ML bias may impact healthcare outcomes and how to identify and mitigate inequities; to initiate a discussion between stakeholders on these issues, in order to ensure health equity along the expandedAI/ML TPLC framework, and ultimately, better health outcomes for all."
"Addressing fairness in artificial intelligence for medical imaging","https://scispace.com/paper/addressing-fairness-in-artificial-intelligence-for-medical-10drnbjj","2022","Journal Article","Nature Communications","María Agustina Ricci Lara
Rodrigo Echeveste
Enzo Ferrante","10.1038/s41467-022-32186-3","https://www.nature.com/articles/s41467-022-32186-3.pdf","A plethora of work has shown that AI systems can systematically and unfairly be biased against certain populations in multiple scenarios. The field of medical imaging, where AI systems are beginning to be increasingly adopted, is no exception. Here we discuss the meaning of fairness in this area and comment on the potential sources of biases, as well as the strategies available to mitigate them. Finally, we analyze the current state of the field, identifying strengths and highlighting areas of vacancy, challenges and opportunities that lie ahead. ","In this article , the authors discuss the meaning of fairness in medical imaging and comment on the potential sources of biases, as well as the strategies available to mitigate them, and analyze the current state of the field, identifying strengths and highlighting areas of vacancy, challenges and opportunities."
"Large language models propagate race-based medicine.","https://scispace.com/paper/large-language-models-propagate-race-based-medicine-2g927vb9z6","2023","Journal Article","npj digital medicine","Jesutofunmi A. Omiye
Jenna C Lester
Simon Spichak
Veronica Rotemberg
R. Daneshjou","10.1038/s41746-023-00939-z","","Abstract Large language models (LLMs) are being integrated into healthcare systems; but these models may recapitulate harmful, race-based medicine. The objective of this study is to assess whether four commercially available large language models (LLMs) propagate harmful, inaccurate, race-based content when responding to eight different scenarios that check for race-based medicine or widespread misconceptions around race. Questions were derived from discussions among four physician experts and prior work on race-based medical misconceptions believed by medical trainees. We assessed four large language models with nine different questions that were interrogated five times each with a total of 45 responses per model. All models had examples of perpetuating race-based medicine in their responses. Models were not always consistent in their responses when asked the same question repeatedly. LLMs are being proposed for use in the healthcare setting, with some models already connecting to electronic health record systems. However, this study shows that based on our findings, these LLMs could potentially cause harm by perpetuating debunked, racist ideas. ","Large language models are being integrated into healthcare systems; but these models may recapitulate harmful, race-based medicine, and could potentially cause harm by perpetuating debunked, racist ideas."
"Assessing and Mitigating Bias in Medical Artificial Intelligence: The Effects of Race and Ethnicity on a Deep Learning Model for ECG Analysis","https://scispace.com/paper/assessing-and-mitigating-bias-in-medical-artificial-4ca7673t7v","2020","Journal Article","Circulation-arrhythmia and Electrophysiology","Peter A. Noseworthy
Zachi I. Attia
La Princess C. Brewer
Sharonne N. Hayes
Xiaoxi Yao
Suraj Kapa
Paul A. Friedman
Francisco Lopez-Jimenez","10.1161/CIRCEP.119.007988","https://www.ahajournals.org/doi/pdf/10.1161/CIRCEP.119.007988?download=true","Background: Deep learning algorithms derived in homogeneous populations may be poorly generalizable and have the potential to reflect, perpetuate, and even exacerbate racial/ethnic disparities in h...","It is demonstrated that while ECG characteristics vary by race, this did not impact the ability of a convolutional neural network to predict low left ventricular ejection fraction from the ECG."
"Machine learning and algorithmic fairness in public and population health","https://scispace.com/paper/machine-learning-and-algorithmic-fairness-in-public-and-1t4taoh9qw","2021","Journal Article","Nature Machine Intelligence","Vishwali Mhasawade
Yuan Zhao
Rumi Chunara","10.1038/S42256-021-00373-4","https://scispace.com/pdf/machine-learning-and-algorithmic-fairness-in-public-and-1t4taoh9qw.pdf","Until now, much of the work on machine learning and health has focused on processes inside the hospital or clinic. However, this represents only a narrow set of tasks and challenges related to health; there is greater potential for impact by leveraging machine learning in health tasks more broadly. In this Perspective we aim to highlight potential opportunities and challenges for machine learning within a holistic view of health and its influences. To do so, we build on research in population and public health that focuses on the mechanisms between different cultural, social and environmental factors and their effect on the health of individuals and communities. We present a brief introduction to research in these fields, data sources and types of tasks, and use these to identify settings where machine learning is relevant and can contribute to new knowledge. Given the key foci of health equity and disparities within public and population health, we juxtapose these topics with the machine learning subfield of algorithmic fairness to highlight specific opportunities where machine learning, public and population health may synergize to achieve health equity. Algorithmic solutions to improve treatment are starting to transform health care. Mhasawade and colleagues discuss in this Perspective how machine learning applications in population and public health can extend beyond clinical practice. While working with general health data comes with its own challenges, most notably ensuring algorithmic fairness in the face of existing health disparities, the area provides new kinds of data and questions for the machine learning community.","This Perspective aims to highlight potential opportunities and challenges for machine learning within a holistic view of health, and builds on research in population and public health that focuses on the mechanisms between different cultural, social and environmental factors and their effect on the health of individuals and communities."
"A Review on Fairness in Machine Learning","https://scispace.com/paper/a-review-on-fairness-in-machine-learning-2udedm7f","2022","Journal Article","ACM Computing Surveys","Dana Pessach
Erez Shmueli","10.1145/3494672","","An increasing number of decisions regarding the daily lives of human beings are being controlled by artificial intelligence and machine learning (ML) algorithms in spheres ranging from healthcare, transportation, and education to college admissions, recruitment, provision of loans, and many more realms. Since they now touch on many aspects of our lives, it is crucial to develop ML algorithms that are not only accurate but also objective and fair. Recent studies have shown that algorithmic decision making may be inherently prone to unfairness, even when there is no intention for it. This article presents an overview of the main concepts of identifying, measuring, and improving algorithmic fairness when using ML algorithms, focusing primarily on classification tasks. The article begins by discussing the causes of algorithmic bias and unfairness and the common definitions and measures for fairness. Fairness-enhancing mechanisms are then reviewed and divided into pre-process, in-process, and post-process mechanisms. A comprehensive comparison of the mechanisms is then conducted, toward a better understanding of which mechanisms should be used in different scenarios. The article ends by reviewing several emerging research sub-fields of algorithmic fairness, beyond classification.","An overview of the main concepts of identifying, measuring, and improving algorithmic fairness when using ML algorithms, focusing primarily on classification tasks is presented."
"Ensuring Fairness in Machine Learning to Advance Health Equity.","https://scispace.com/paper/ensuring-fairness-in-machine-learning-to-advance-health-2yz9acqaph","2018","Journal Article","Annals of Internal Medicine","Alvin Rajkomar
Michaela Hardt
Michael D. Howell
Greg S. Corrado
Marshall H. Chin","10.7326/M18-1990","https://scispace.com/pdf/ensuring-fairness-in-machine-learning-to-advance-health-2yz9acqaph.pdf","Machine learning is used increasingly in clinical care to improve diagnosis, treatment selection, and health system efficiency. Because machine-learning models learn from historically collected data, populations that have experienced human and structural biases in the past-called protected groups-are vulnerable to harm by incorrect predictions or withholding of resources. This article describes how model design, biases in data, and the interactions of model predictions with clinicians and patients may exacerbate health care disparities. Rather than simply guarding against these harms passively, machine-learning systems should be used proactively to advance health equity. For that goal to be achieved, principles of distributive justice must be incorporated into model design, deployment, and evaluation. The article describes several technical implementations of distributive justice-specifically those that ensure equality in patient outcomes, performance, and resource allocation-and guides clinicians as to when they should prioritize each principle. Machine learning is providing increasingly sophisticated decision support and population-level monitoring, and it should encode principles of justice to ensure that models benefit all patients.","The mechanisms by which a model's design, data, and deployment may lead to disparities are described; how different approaches to distributive justice in machine learning can advance health equity are explained; and what contexts are more appropriate for different equity approaches inMachine learning."
"Potential Biases in Machine Learning Algorithms Using Electronic Health Record Data.","https://scispace.com/paper/potential-biases-in-machine-learning-algorithms-using-4n43rc97ch","2018","Journal Article","JAMA Internal Medicine","Milena A. Gianfrancesco
Suzanne Tamang
Jinoos Yazdany
Gabriela Schmajuk
Gabriela Schmajuk","10.1001/JAMAINTERNMED.2018.3763","https://scispace.com/pdf/potential-biases-in-machine-learning-algorithms-using-4n43rc97ch.pdf","A promise of machine learning in health care is the avoidance of biases in diagnosis and treatment; a computer algorithm could objectively synthesize and interpret the data in the medical record. Integration of machine learning with clinical decision support tools, such as computerized alerts or diagnostic support, may offer physicians and others who provide health care targeted and timely information that can improve clinical decisions. Machine learning algorithms, however, may also be subject to biases. The biases include those related to missing data and patients not identified by algorithms, sample size and underestimation, and misclassification and measurement error. There is concern that biases and deficiencies in the data used by machine learning algorithms may contribute to socioeconomic disparities in health care. This Special Communication outlines the potential biases that may be introduced into machine learning–based clinical decision support tools that use electronic health record data and proposes potential solutions to the problems of overreliance on automation, algorithms based on biased data, and algorithms that do not provide information that is clinically meaningful. Existing health care disparities should not be amplified by thoughtless or excessive reliance on machines.","The potential biases that may be introduced into machine learning–based clinical decision support tools that use electronic health record data are outlined and potential solutions to the problems of overreliance on automation, algorithms based on biased data, and algorithms that do not provide information that is clinically meaningful are proposed."
"A Survey on Bias and Fairness in Machine Learning","https://scispace.com/paper/a-survey-on-bias-and-fairness-in-machine-learning-52igbyxaaa","2021","Journal Article","ACM Computing Surveys","Ninareh Mehrabi
Fred Morstatter
Nripsuta Saxena
Kristina Lerman
Aram Galstyan","10.1145/3457607","","With the widespread use of artificial intelligence (AI) systems and applications in our everyday lives, accounting for fairness has gained significant importance in designing and engineering of such systems. AI systems can be used in many sensitive environments to make important and life-changing decisions; thus, it is crucial to ensure that these decisions do not reflect discriminatory behavior toward certain groups or populations. More recently some work has been developed in traditional machine learning and deep learning that address such challenges in different subdomains. With the commercialization of these systems, researchers are becoming more aware of the biases that these applications can contain and are attempting to address them. In this survey, we investigated different real-world applications that have shown biases in various ways, and we listed different sources of biases that can affect AI applications. We then created a taxonomy for fairness definitions that machine learning researchers have defined to avoid the existing bias in AI systems. In addition to that, we examined different domains and subdomains in AI showing what researchers have observed with regard to unfair outcomes in the state-of-the-art methods and ways they have tried to address them. There are still many future directions and solutions that can be taken to mitigate the problem of bias in AI systems. We are hoping that this survey will motivate researchers to tackle these issues in the near future by observing existing work in their respective fields.","In this article, the authors present a taxonomy for fairness definitions that machine learning researchers have defined to avoid the existing bias in AI systems and examine different domains and subdomains in AI showing what researchers have observed with regard to unfair outcomes in the state-of-the-art methods and ways they have tried to address them."
"Improving Equity in Deep Learning Medical Applications with the Gerchberg-Saxton Algorithm","https://scispace.com/paper/improving-equity-in-deep-learning-medical-applications-with-4winfmkz0i","2024","Journal Article","Journal of healthcare informatics research","Seha Ay
Michael Cardei
Anne‐Marie Meyer
Wei Zhang
Ümit Topaloĝlu","10.1007/s41666-024-00163-8","https://scispace.compdf/improving-equity-in-deep-learning-medical-applications-with-4winfmkz0i.pdf","Abstract Deep learning (DL) has gained prominence in healthcare for its ability to facilitate early diagnosis, treatment identification with associated prognosis, and varying patient outcome predictions. However, because of highly variable medical practices and unsystematic data collection approaches, DL can unfortunately exacerbate biases and distort estimates. For example, the presence of sampling bias poses a significant challenge to the efficacy and generalizability of any statistical model. Even with DL approaches, selection bias can lead to inconsistent, suboptimal, or inaccurate model results, especially for underrepresented populations. Therefore, without addressing bias, wider implementation of DL approaches can potentially cause unintended harm. In this paper, we studied a novel method for bias reduction that leverages the frequency domain transformation via the Gerchberg-Saxton and corresponding impact on the outcome from a racio-ethnic bias perspective. ","The Gerchberg-Saxton algorithm is effective in reducing bias in deep learning medical applications."
"Exploring Artificial Intelligence Biases in Predictive Models for Cancer Diagnosis","https://scispace.com/paper/exploring-artificial-intelligence-biases-in-predictive-kc1494kfhteg","2025","Journal Article","Cancers","Aref Smiley
C Mahony Reátegui-Rivera
David Villarreal‐Zegarra
Stefan Escobar-Agreda
Joseph Finkelstein","10.3390/cancers17030407","","The American Society of Clinical Oncology (ASCO) has released the principles for the responsible use of artificial intelligence (AI) in oncology emphasizing fairness, accountability, oversight, equity, and transparency. However, the extent to which these principles are followed is unknown. The goal of this study was to assess the presence of biases and the quality of studies on AI models according to the ASCO principles and examine their potential impact through citation analysis and subsequent research applications. A review of original research articles centered on the evaluation of predictive models for cancer diagnosis published in the ASCO journal dedicated to informatics and data science in clinical oncology was conducted. Seventeen potential bias criteria were used to evaluate the sources of bias in the studies, aligned with the ASCO’s principles for responsible AI use in oncology. The CREMLS checklist was applied to assess the study quality, focusing on the reporting standards, and the performance metrics along with citation counts of the included studies were analyzed. Nine studies were included. The most common biases were environmental and life-course bias, contextual bias, provider expertise bias, and implicit bias. Among the ASCO principles, the least adhered to were transparency, oversight and privacy, and human-centered AI application. Only 22% of the studies provided access to their data. The CREMLS checklist revealed the deficiencies in methodology and evaluation reporting. Most studies reported performance metrics within moderate to high ranges. Additionally, two studies were replicated in the subsequent research. In conclusion, most studies exhibited various types of bias, reporting deficiencies, and failure to adhere to the principles for responsible AI use in oncology, limiting their applicability and reproducibility. Greater transparency, data accessibility, and compliance with international guidelines are recommended to improve the reliability of AI-based research in oncology. ","This study assesses AI biases in cancer diagnosis predictive models, evaluating 17 bias criteria and study quality using the CREMLS checklist, revealing widespread biases, reporting deficiencies, and non-compliance with ASCO's responsible AI use principles in 9 included studies."
"Measuring AI Fairness in a Continuum Maintaining Nuances: A Robustness Case Study","https://scispace.com/paper/measuring-ai-fairness-in-a-continuum-maintaining-nuances-a-1omenlftgxdn","2024","Journal Article","IEEE Internet Computing","Kuniko Paxton
Koorosh Aslansefat
Dhavalkumar Thakker
Yiannis Papadopoulos","10.1109/mic.2024.3450815","","As machine learning is increasingly making decisions about hiring or health care, we want AI to treat ethnic and socioeconomic groups fairly. Fairness is currently measured by comparing the average accuracy of reasoning across groups. We argue that improved measurement is possible on a continuum and without averaging, with the advantage that nuances could be observed within groups. Through the example of skin cancer diagnosis, we illustrate a new statistical method that works on multidimensional data and treats fairness in a continuum. We outline this new approach and focus on its robustness against three types of adversarial attacks. Indeed, such attacks can influence data in ways that may cause different levels of misdiagnosis for different skin tones, thereby distorting fairness. Our results reveal nuances that would not be evident in a strictly categorical approach.","This study proposes a new method to measure AI fairness on a continuum, rather than categorically, using multidimensional data and statistical analysis, and demonstrates its robustness against adversarial attacks in a skin cancer diagnosis example."
"Mitigating the risk of health inequity exacerbated by large language models","https://scispace.com/paper/mitigating-the-risk-of-health-inequity-exacerbated-by-large-ldq49xufen3v","2025","Journal Article","npj digital medicine","Yuelyu Ji
Weigang Ma
Sonish Sivarajkumar
Hang Zhang
Eugene M. Sadhu
Zhuochun Li
Xizhi Wu
Shyam Visweswaran
Yanshan Wang","10.1038/s41746-025-01576-4","","Recent advancements in large language models (LLMs) have demonstrated their potential in numerous medical applications, particularly in automating clinical trial matching for translational research and enhancing medical question-answering for clinical decision support. However, our study shows that incorporating non-decisive socio-demographic factors, such as race, sex, income level, LGBT+ status, homelessness, illiteracy, disability, and unemployment, into the input of LLMs can lead to incorrect and harmful outputs. These discrepancies could worsen existing health disparities if LLMs are broadly implemented in healthcare. To address this issue, we introduce EquityGuard, a novel framework designed to detect and mitigate the risk of health inequities in LLM-based medical applications. Our evaluation demonstrates its effectiveness in promoting equitable outcomes across diverse populations. ","This study reveals that incorporating socio-demographic factors into large language models can exacerbate health inequities, and proposes EquityGuard, a framework to detect and mitigate these risks, promoting equitable outcomes in medical applications."
"Racing Against the Algorithm: Leveraging Inclusive <scp>AI</scp> as an Antiracist Tool for Brain Health","https://scispace.com/paper/racing-against-the-algorithm-leveraging-inclusive-scp-ai-scp-si00we03qgmh","2025","Journal Article","Clinical and Translational Science","Victor Ekuta","10.1111/cts.70364","","ABSTRACT Artificial intelligence (AI) is transforming medicine, including neurology and mental health. Yet without equity‐centered design, AI risks reinforcing systemic racism. This article explores how algorithmic bias and phenotypic exclusion disproportionately affect marginalized communities in brain health. Drawing on lived experience and scientific evidence, the essay outlines five design principles—centered on inclusion, transparency, and accountability—to ensure AI promotes equity. By reimagining AI as a tool for justice, we can reshape translational science to serve all populations. ","This article explores how AI in brain health disproportionately affects marginalized communities due to algorithmic bias and phenotypic exclusion, and proposes five design principles to ensure AI promotes equity and serves all populations."
"Towards objective and systematic evaluation of bias in artificial intelligence for medical imaging","https://scispace.com/paper/towards-objective-and-systematic-evaluation-of-bias-in-45panpx3ea","2024","Journal Article","Journal of the American Medical Informatics Association","Emma A. M. Stanley
Raissa Souza
Anthony Winder
Vedant Gulve
Kimberly Amador
Matthias Wilms
Nils D. Forkert","10.1093/jamia/ocae165","","Abstract Objective Artificial intelligence (AI) models trained using medical images for clinical tasks often exhibit bias in the form of subgroup performance disparities. However, since not all sources of bias in real-world medical imaging data are easily identifiable, it is challenging to comprehensively assess their impacts. In this article, we introduce an analysis framework for systematically and objectively investigating the impact of biases in medical images on AI models. Materials and Methods Our framework utilizes synthetic neuroimages with known disease effects and sources of bias. We evaluated the impact of bias effects and the efficacy of 3 bias mitigation strategies in counterfactual data scenarios on a convolutional neural network (CNN) classifier. Results The analysis revealed that training a CNN model on the datasets containing bias effects resulted in expected subgroup performance disparities. Moreover, reweighing was the most successful bias mitigation strategy for this setup. Finally, we demonstrated that explainable AI methods can aid in investigating the manifestation of bias in the model using this framework. Discussion The value of this framework is showcased in our findings on the impact of bias scenarios and efficacy of bias mitigation in a deep learning model pipeline. This systematic analysis can be easily expanded to conduct further controlled in silico trials in other investigations of bias in medical imaging AI. Conclusion Our novel methodology for objectively studying bias in medical imaging AI can help support the development of clinical decision-support tools that are robust and responsible. ","Researchers introduce a framework to systematically evaluate bias in medical imaging AI, using synthetic neuroimages to assess subgroup performance disparities and efficacy of bias mitigation strategies, revealing reweighing as the most effective approach."
"Inherent Bias in Electronic Health Records: A Scoping Review of Sources of Bias","https://scispace.com/paper/inherent-bias-in-electronic-health-records-a-scoping-review-pqn1cm5nc7pm","2025","Journal Article","ACM Transactions on Intelligent Systems and Technology","Oriel Perets
Emanuela Stagno
Eyal Ben Yehuda
Megan McNichol
Leo Anthony Celi
Nadav Rappoport
Matilda Dorotić","10.1145/3757924","","1 Abstract 1.1 Objectives Biases inherent in electronic health records (EHRs), which are often used as a data source to train medical AI models, may significantly exacerbate health inequities and challenge the adoption of ethical and responsible AI in healthcare. Biases arise from multiple sources, some of which are not as documented in the literature (e.g., bias in medical devices measurement). Biases are encoded in how the data has been collected and labeled, by implicit and unconscious biases of clinicians, or by the tools used for data processing. These biases and their encoding in healthcare records can potentially undermine the reliability of such data and bias clinical judgments and medical outcomes. Moreover, when healthcare records are used to build data-driven solutions, the biases can be further exacerbated, resulting in systems that can perpetuate biases and induce healthcare disparities. This literature scoping review aims to categorize the main sources of biases inherent in EHRs. 1.2 Methods We queried PubMed and Web of Science on January 19th, 2023, for peer-reviewed sources in English, published between 2016 and 2023, using the PRISMA approach to stepwise scoping of the literature. To select the papers that empirically analyze bias in EHR, from the initial yield of 430 papers, 27 duplicates were removed, and 403 studies were screened for eligibility. 196 articles were removed after the title and abstract screening, and 96 articles were excluded after the full-text review resulting in a final selection of 116 articles. 1.3 Results Existing studies often focus on individual biases in EHR data, but a comprehensive review categorizing these biases is largely absent. To address this gap, we propose a systematic taxonomy to classify and better understand the multiplicity of biases in EHR data. Our framework identifies six primary sources: a) bias from past clinical trials ; b) data-related biases , such as missing or incomplete information; human-related biases , including c) implicit clinician bias, d) referral and admission bias, and e) diagnosis or risk disparities bias; and f) biases in devices and algorithms. This taxonomy, illustrated in Table 1, provides a valuable tool for systematically evaluating and addressing these issues. 1.4 Conclusions Machine learning and data-driven solutions can potentially transform healthcare delivery, but not without limitations. The core inputs in the systems (data and human factors) currently contain several sources of bias that are poorly documented and analyzed for remedies. The current evidence heavily focuses on data-related biases, while other sources are less often analyzed or anecdotal. However, these different sources of bias can compound each other, leading to a cumulative effect. Therefore, to understand the issues holistically we need to explore these diverse sources of bias. While racial biases in EHR have been often documented, other sources of biases have been less frequently investigated and documented (e.g. gender-related biases, sexual orientation discrimination, socially induced biases, and implicit, often unconscious, human-related cognitive biases). Moreover, some existing studies lack concrete evidence of the effects of the bias, but rather illustrate the different prevalence of disease across groups, which does not per se prove the effect of the bias. Our review shows that data-, human- and machine biases are prevalent in healthcare and can significantly affect treatment decisions and outcomes and amplify healthcare disparities. Understanding how diverse biases affect AI systems and recommendations is critical. We recommend that researchers and medical personnel develop safeguards and adopt data-driven solutions with a “bias-in-mind” approach. More empirical evidence is needed to tease out the effects of different sources of bias on health outcomes. ","This scoping review identifies six primary sources of inherent bias in electronic health records, including data-related, human-related, and device-related biases, which can exacerbate health inequities and challenge AI adoption in healthcare."
"XAI Unveiled: Revealing the Potential of Explainable AI in Medicine - A Systematic Review","https://scispace.com/paper/xai-unveiled-revealing-the-potential-of-explainable-ai-in-2hb3yggbcga7","2024","Journal Article","IEEE Access","Noemi Scarpato
Patrizia Ferroni
Fiorella Guadagni","10.1109/access.2024.3514197","","Nowadays, artificial intelligence in medicine plays a leading role. This necessitates the need to ensure that artificial intelligence systems are not only high-performing but also comprehensible to all stakeholders involved, including doctors, patients, healthcare providers, etc. As a result, the explainability of artificial intelligence systems has become a widely discussed subject in recent times, leading to the publication of numerous approaches and solutions. In this paper, we aimed to provide a systematic review of these approaches in order to analyze their role in making artificial intelligence interpretable for everyone. The conducted review was carried out in accordance with the PRISMA statement. We conducted a BIAS analysis, identifying 87 scientific papers from those retrieved as having a low risk of BIAS. Subsequently, we defined a classification framework based on the classification taxonomy and applied it to analyze these papers. The results show that, although most AI approaches in medicine currently incorporate explainability methods, the evaluation of these systems is not always performed. When evaluation does occur, it is most often focused on improving the system itself rather than assessing users’ perception of the system’s effectiveness. To address these limitations, we propose a framework for evaluating explainability approaches in medicine, aimed at guiding developers in designing effective human-centered methods.","This systematic review examines the role of explainable AI (XAI) in medicine, analyzing 87 papers and proposing a framework for evaluating XAI approaches to improve human-centered methods and ensure AI systems are comprehensible to stakeholders."
"Sociodemographic bias in clinical machine learning models: A scoping review of algorithmic bias instances and mechanisms","https://scispace.com/paper/sociodemographic-bias-in-clinical-machine-learning-models-a-ws4moakladt4","2024","Journal Article","Journal of Clinical Epidemiology","Michael Colacci
Yu Qing Huang
Gemma Postill
Pavel Zhelnov
Orna Fennelly
Amol A. Verma
Sharon E. Straus
Andrea C. Tricco","10.1016/j.jclinepi.2024.111606","","Clinical machine learning (ML) technologies can sometimes be biased and their use could exacerbate health disparities. The extent to which bias is present, the groups who most frequently experience bias, and the mechanism through which bias is introduced in clinical ML applications is not well described. The objective of this study was to examine instances of bias in clinical ML models. We identified the sociodemographic subgroups (using the PROGRESS-Plus framework) that experienced bias and the reported mechanisms of bias introduction METHODS: We searched MEDLINE, EMBASE, PsycINFO and Web of Science for all studies that evaluated bias on sociodemographic factors within ML algorithms created for the purpose of facilitating clinical care. The scoping review was conducted according to the JBI guide and reported using the PRISMA extension for scoping reviews. ","This scoping review examines sociodemographic bias in clinical machine learning models, identifying instances and mechanisms of bias introduction, and sociodemographic subgroups most frequently experiencing bias in clinical ML applications, highlighting potential health disparities exacerbation."
"Fair Machine Learning in Healthcare: A Survey","https://scispace.com/paper/fair-machine-learning-in-healthcare-a-survey-3ibcygwbhkms","2024","Journal Article","IEEE transactions on artificial intelligence","Qizhang Feng
Mengnan Du
Na Zou
Xia Hu","10.1109/tai.2024.3361836","","The digitization of healthcare data coupled with advances in computational capabilities has propelled the adoption of machine learning (ML) in healthcare. However, these methods can perpetuate or even exacerbate existing disparities, leading to fairness concerns such as the unequal distribution of resources and diagnostic inaccuracies among different demographic groups. Addressing these fairness problem is paramount to prevent further entrenchment of social injustices. In this survey, we analyze the intersection of fairness in machine learning and healthcare disparities. We adopt a framework based on the principles of distributive justice to categorize fairness concerns into two distinct classes: equal allocation and equal performance. We provide a critical review of the associated fairness metrics from a machine learning standpoint and examine biases and mitigation strategies across the stages of the ML lifecycle, discussing the relationship between biases and their countermeasures. The paper concludes with a discussion on the pressing challenges that remain unaddressed in ensuring fairness in healthcare ML, and proposes several new research directions that hold promise for developing ethical and equitable ML applications in healthcare.","This survey analyzes the intersection of fairness in machine learning and healthcare disparities, categorizing concerns into equal allocation and equal performance, and reviews fairness metrics, biases, and mitigation strategies across the ML lifecycle in healthcare."
"What Is Fair? Defining Fairness in Machine Learning for Health","https://scispace.com/paper/what-is-fair-defining-fairness-in-machine-learning-for-ywbze78nlto1","2025","Journal Article","Statistics in Medicine","Jianhui Gao
Benson Chou
Zachary R. McCaw
Hilary Thurston
Paul Varghese
Chuan Hong
Jessica Gronsbell","10.1002/sim.70234","","Ensuring that machine-learning (ML) models are safe, effective, and equitable across all patients is critical for clinical decision-making and for preventing the amplification of existing health disparities. In this work, we examine how fairness is conceptualized in ML for health, including why ML models may lead to unfair decisions and how fairness has been measured in diverse real-world applications. We review commonly used fairness notions within group, individual, and causal-based frameworks. We also discuss the outlook for future research and highlight opportunities and challenges in operationalizing fairness in health-focused applications. ","This study examines fairness in machine learning for health, conceptualizing fairness within group, individual, and causal-based frameworks, and discusses the challenges and opportunities in operationalizing fairness in health-focused applications to prevent health disparities."
"Enhancing fairness in AI-enabled medical systems with the attribute neutral framework","https://scispace.com/paper/enhancing-fairness-in-ai-enabled-medical-systems-with-the-236nr0nr39l1","2024","Journal Article","Nature Communications","Lianting Hu
Dantong Li
Huazhang Liu
Xuanhui Chen
Yunfei Gao
Shuai Huang
Xiaoting Peng
Xueli Zhang
Xiaohe Bai
Huan Yang
Lingcong Kong
Jiajie Tang
Peixin Lu
Chao Xiong
Huiying Liang","10.1038/s41467-024-52930-1","","Questions of unfairness and inequity pose critical challenges to the successful deployment of artificial intelligence (AI) in healthcare settings. In AI models, unequal performance across protected groups may be partially attributable to the learning of spurious or otherwise undesirable correlations between sensitive attributes and disease-related information. Here, we introduce the Attribute Neutral Framework, designed to disentangle biased attributes from disease-relevant information and subsequently neutralize them to improve representation across diverse subgroups. Within the framework, we develop the Attribute Neutralizer (AttrNzr) to generate neutralized data, for which protected attributes can no longer be easily predicted by humans or by machine learning classifiers. We then utilize these data to train the disease diagnosis model (DDM). Comparative analysis with other unfairness mitigation algorithms demonstrates that AttrNzr outperforms in reducing the unfairness of the DDM while maintaining DDM's overall disease diagnosis performance. Furthermore, AttrNzr supports the simultaneous neutralization of multiple attributes and demonstrates utility even when applied solely during the training phase, without being used in the test phase. Moreover, instead of introducing additional constraints to the DDM, the AttrNzr directly addresses a root cause of unfairness, providing a model-independent solution. Our results with AttrNzr highlight the potential of data-centered and model-independent solutions for fairness challenges in AI-enabled medical systems. ","Researchers introduce the Attribute Neutral Framework to enhance fairness in AI-enabled medical systems by disentangling biased attributes from disease-relevant information, improving representation across diverse subgroups and reducing unfairness in disease diagnosis models."
"Guidance for unbiased predictive information for healthcare decision-making and equity (GUIDE): considerations when race may be a prognostic factor","https://scispace.com/paper/guidance-for-unbiased-predictive-information-for-healthcare-3tadujy5lf9j","2024","Journal Article","npj digital medicine","Keren Ladin
John K. Cuddeback
O. Kenrik Duru
Sharad Goel
William Harvey
Jinny G. Park
Jessica K. Paulus
Joyce Sackey
Richard R. Sharp
Ewout W. Steyerberg
Berk Ustun
David van Klaveren
Saul N. Weingart
David M. Kent","10.1038/s41746-024-01245-y","","Clinical prediction models (CPMs) are tools that compute the risk of an outcome given a set of patient characteristics and are routinely used to inform patients, guide treatment decision-making, and resource allocation. Although much hope has been placed on CPMs to mitigate human biases, CPMs may potentially contribute to racial disparities in decision-making and resource allocation. While some policymakers, professional organizations, and scholars have called for eliminating race as a variable from CPMs, others raise concerns that excluding race may exacerbate healthcare disparities and this controversy remains unresolved. The Guidance for Unbiased predictive Information for healthcare Decision-making and Equity (GUIDE) provides expert guidelines for model developers and health system administrators on the transparent use of race in CPMs and mitigation of algorithmic bias across contexts developed through a 5-round, modified Delphi process from a diverse 14-person technical expert panel (TEP). Deliberations affirmed that race is a social construct and that the goals of prediction are distinct from those of causal inference, and emphasized: the importance of decisional context (e.g., shared decision-making versus healthcare rationing); the conflicting nature of different anti-discrimination principles (e.g., anticlassification versus antisubordination principles); and the importance of identifying and balancing trade-offs in achieving equity-related goals with race-aware versus race-unaware CPMs for conditions where racial identity is prognostically informative. The GUIDE, comprising 31 key items in the development and use of CPMs in healthcare, outlines foundational principles, distinguishes between bias and fairness, and offers guidance for examining subgroup invalidity and using race as a variable in CPMs. This GUIDE presents a living document that supports appraisal and reporting of bias in CPMs to support best practice in CPM development and use. ","This study presents the Guidance for Unbiased predictive Information for healthcare Decision-making and Equity (GUIDE), expert guidelines for using race in clinical prediction models to mitigate algorithmic bias and promote equity in healthcare decision-making and resource allocation."
"Role and Use of Race in AI/ML Models Related to Health (Preprint)","https://scispace.com/paper/role-and-use-of-race-in-ai-ml-models-related-to-health-k22fb2ozcyfr","2025","Journal Article","Journal of Medical Internet Research","Martin C. Were
Ang Li
Bradley Malin
Zhijun Yin
Joseph Coco
Benjamin Collins
Ellen Wright Clayton
Laurie L. Novak
Rachele Hendricks‐Sturrup
Abiodun Oluyomi
Shilo Anders
Chao Yan","10.2196/73996","","The role and use of race within health-related artificial intelligence (AI) and machine learning (ML) models have sparked increasing attention and controversy. Despite the complexity and breadth of related issues, a robust and holistic framework to guide stakeholders in their examination and resolution remains lacking. This perspective provides a broad-based, systematic, and crosscutting landscape analysis of race-related challenges, structured around the AI and ML life cycle and framed through ""points to consider"" to support inquiry and decision-making. ","This preprint provides a systematic analysis of race-related challenges in health-related AI/ML models, structured around the life cycle and framed with ""points to consider"" to guide stakeholders in examining and resolving these complex issues."
"Demographic bias of expert-level vision-language foundation models in medical imaging","https://scispace.com/paper/demographic-bias-of-expert-level-vision-language-foundation-zivyp49wws3y","2025","Journal Article","Science Advances","Yuzhe Yang
Yujia Liu
Xin Liu
Avanti Gulhane
Domenico Mastrodicasa
Wei Wu
Edward Jay Wang
Dushyant V. Sahani
Shwetak Patel","10.1126/sciadv.adq0305","","Advances in artificial intelligence (AI) have achieved expert-level performance in medical imaging applications. Notably, self-supervised vision-language foundation models can detect a broad spectrum of pathologies without relying on explicit training annotations. However, it is crucial to ensure that these AI models do not mirror or amplify human biases, disadvantaging historically marginalized groups such as females or Black patients. In this study, we investigate the algorithmic fairness of state-of-the-art vision-language foundation models in chest x-ray diagnosis across five globally sourced datasets. Our findings reveal that compared to board-certified radiologists, these foundation models consistently underdiagnose marginalized groups, with even higher rates seen in intersectional subgroups such as Black female patients. Such biases present over a wide range of pathologies and demographic attributes. Further analysis of the model embedding uncovers its substantial encoding of demographic information. Deploying medical AI systems with biases can intensify preexisting care disparities, posing potential challenges to equitable healthcare access and raising ethical questions about their clinical applications. ","This study investigates the algorithmic fairness of expert-level vision-language foundation models in chest x-ray diagnosis, revealing consistent underdiagnosis of marginalized groups, including Black female patients, and substantial encoding of demographic information in model embeddings."
"Healthcare Resource Allocation, Machine Learning, and Distributive Justice","https://scispace.com/paper/healthcare-resource-allocation-machine-learning-and-3rfdltoci85v","2025","Journal Article","American Philosophical Quarterly","Jamie Webb","10.5406/21521123.62.1.03","","Abstract The literature on the ethics of machine learning in healthcare contains a great deal of work on algorithmic fairness. But a focus on fairness has not been matched with sufficient attention to the relationship between machine learning and distributive justice in healthcare. A significant number of clinical prediction models have been developed which could be used to inform the allocation of scarce healthcare resources. As such, philosophical theories of distributive justice are relevant when considering the ethics of their design and implementation. This paper considers the relationship between machine learning in healthcare and distributive justice with a focus on four aspects of algorithmic design and deployment: the choice of target variable, the model's socio-technical context, the choice of input variables, and the membership of the datasets that models are trained and validated on. Procedural recommendations for how these considerations should be accounted for in the design and implementation of such models follow. ","This paper explores the intersection of machine learning in healthcare and distributive justice, examining how clinical prediction models can inform resource allocation and proposing procedural recommendations for designing and implementing fair and just models."
"Addressing fairness issues in deep learning-based medical image analysis: a systematic review","https://scispace.com/paper/addressing-fairness-issues-in-deep-learning-based-medical-5476y74ae111","2024","Journal Article","npj digital medicine","Zikang Xu
Jun Li
Qingsong Yao
Han Li
Mingyue Zhao
S. Kevin Zhou","10.1038/s41746-024-01276-5","","Deep learning algorithms have demonstrated remarkable efficacy in various medical image analysis (MedIA) applications. However, recent research highlights a performance disparity in these algorithms when applied to specific subgroups, such as exhibiting poorer predictive performance in elderly females. Addressing this fairness issue has become a collaborative effort involving AI scientists and clinicians seeking to understand its origins and develop solutions for mitigation within MedIA. In this survey, we thoroughly examine the current advancements in addressing fairness issues in MedIA, focusing on methodological approaches. We introduce the basics of group fairness and subsequently categorize studies on fair MedIA into fairness evaluation and unfairness mitigation. Detailed methods employed in these studies are presented too. Our survey concludes with a discussion of existing challenges and opportunities in establishing a fair MedIA and healthcare system. By offering this comprehensive review, we aim to foster a shared understanding of fairness among AI researchers and clinicians, enhance the development of unfairness mitigation methods, and contribute to the creation of an equitable MedIA society. ","This systematic review examines fairness issues in deep learning-based medical image analysis, highlighting performance disparities in subgroups, and categorizes methodological approaches to address these issues, including fairness evaluation and mitigation methods."
"The bias algorithm: how AI in healthcare exacerbates ethnic and racial disparities – a scoping review","https://scispace.com/paper/the-bias-algorithm-how-ai-in-healthcare-exacerbates-ethnic-1k03lxhj4tof","2024","Journal Article","Ethnicity & Health","Syed Ali Hussain
Mary Bresnahan
Jie Zhang","10.1080/13557858.2024.2422848","","This scoping review examined racial and ethnic bias in artificial intelligence health algorithms (AIHA), the role of stakeholders in oversight, and the consequences of AIHA for health equity. Using the PRISMA-ScR guidelines, databases were searched between 2020 and 2024 using the terms racial and ethnic bias in health algorithms resulting in a final sample of 23 sources. Suggestions for how to mitigate algorithmic bias were compiled and evaluated, roles played by stakeholders were identified, and governance and stewardship plans for AIHA were examined. While AIHA represent a significant breakthrough in predictive analytics and treatment optimization, regularly outperforming humans in diagnostic precision and accuracy, they also present serious challenges to patient privacy, data security, institutional transparency, and health equity. Evidence from extant sources including those in this review showed that AIHA carry the potential to perpetuate health inequities. While the current study considered AIHA in the US, the use of AIHA carries implications for global health equity. ","This scoping review examines racial and ethnic bias in AI health algorithms, their consequences for health equity, and potential mitigation strategies, highlighting the need for governance and stewardship to address AI's exacerbation of disparities in healthcare."
"Issues and Limitations on the Road to Fair and Inclusive AI Solutions for Biomedical Challenges","https://scispace.com/paper/issues-and-limitations-on-the-road-to-fair-and-inclusive-ai-xlkybsn58awd","2025","Journal Article","Sensors","Oliver Faust
Massimo Salvi
Prabal Datta Barua
Subrata Chakraborty
Filippo Molinari
U. Rajendra Acharya","10.3390/s25010205","","Objective: In this paper, we explore the correlation between performance reporting and the development of inclusive AI solutions for biomedical problems. Our study examines the critical aspects of bias and noise in the context of medical decision support, aiming to provide actionable solutions. Contributions: A key contribution of our work is the recognition that measurement processes introduce noise and bias arising from human data interpretation and selection. We introduce the concept of “noise-bias cascade” to explain their interconnected nature. While current AI models handle noise well, bias remains a significant obstacle in achieving practical performance in these models. Our analysis spans the entire AI development lifecycle, from data collection to model deployment. Recommendations: To effectively mitigate bias, we assert the need to implement additional measures such as rigorous study design; appropriate statistical analysis; transparent reporting; and diverse research representation. Furthermore, we strongly recommend the integration of uncertainty measures during model deployment to ensure the utmost fairness and inclusivity. These comprehensive recommendations aim to minimize both bias and noise, thereby improving the performance of future medical decision support systems. ","This study examines the limitations of AI solutions for biomedical challenges, identifying bias and noise as critical obstacles. It proposes a ""noise-bias cascade"" concept and recommends rigorous study design, transparent reporting, and uncertainty measures to ensure fairness and inclusivity."
"Bias Mitigation in Primary Healthcare Artificial Intelligence Models: A Scoping Review (Preprint)","https://scispace.com/paper/bias-mitigation-in-primary-healthcare-artificial-6m9d1bytauqt","2024","Journal Article","Journal of Medical Internet Research","Maxime Sasseville
Steven Ouellet
Caroline Rhéaume
Malek Sahlia
Vincent Couture
Philippe Després
Jean‐Sébastien Paquette
David Darmon
Frédéric Bergeron
Marie‐Pierre Gagnon","10.2196/60269","","BACKGROUND
Artificial intelligence (AI) predictive models in primary health care have the potential to enhance population health by rapidly and accurately identifying individuals who should receive care and health services. However, these models also carry the risk of perpetuating or amplifying existing biases toward diverse groups. We identified a gap in the current understanding of strategies used to assess and mitigate bias in primary health care algorithms related to individuals' personal or protected attributes.


OBJECTIVE
This study aimed to describe the attempts, strategies, and methods used to mitigate bias in AI models within primary health care, to identify the diverse groups or protected attributes considered, and to evaluate the results of these approaches on both bias reduction and AI model performance.


METHODS
We conducted a scoping review following Joanna Briggs Institute (JBI) guidelines, searching Medline (Ovid), CINAHL (EBSCO), PsycINFO (Ovid), and Web of Science databases for studies published between January 1, 2017, and November 15, 2022. Pairs of reviewers independently screened titles and abstracts, applied selection criteria, and performed full-text screening. Discrepancies regarding study inclusion were resolved by consensus. Following reporting standards for AI in health care, we extracted data on study objectives, model features, targeted diverse groups, mitigation strategies used, and results. Using the mixed methods appraisal tool, we appraised the quality of the studies.


RESULTS
After removing 585 duplicates, we screened 1018 titles and abstracts. From the remaining 189 full-text articles, we included 17 studies. The most frequently investigated protected attributes were race (or ethnicity), examined in 12 of the 17 studies, and sex (often identified as gender), typically classified as ""male versus female"" in 10 of the studies. We categorized bias mitigation approaches into four clusters: (1) modifying existing AI models or datasets, (2) sourcing data from electronic health records, (3) developing tools with a ""human-in-the-loop"" approach, and (4) identifying ethical principles for informed decision-making. Algorithmic preprocessing methods, such as relabeling and reweighing data, along with natural language processing techniques that extract data from unstructured notes, showed the greatest potential for bias mitigation. Other methods aimed at enhancing model fairness included group recalibration and the application of the equalized odds metric. However, these approaches sometimes exacerbated prediction errors across groups or led to overall model miscalibrations.


CONCLUSIONS
The results suggest that biases toward diverse groups are more easily mitigated when data are open-sourced, multiple stakeholders are engaged, and during the algorithm's preprocessing stage. Further empirical studies that include a broader range of groups, such as Indigenous peoples in Canada, are needed to validate and expand upon these findings.


TRIAL REGISTRATION
OSF Registry osf.io/9ngz5/; https://osf.io/9ngz5/.


INTERNATIONAL REGISTERED REPORT IDENTIFIER (IRRID)
RR2-10.2196/46684.","This scoping review identifies strategies to mitigate bias in primary healthcare AI models, finding that open-sourced data, multi-stakeholder engagement, and preprocessing methods like relabeling and reweighing data are effective, but further studies are needed to validate these findings and expand to diverse groups."
"AI for all: bridging data gaps in machine learning and health","https://scispace.com/paper/ai-for-all-bridging-data-gaps-in-machine-learning-and-health-3dld2wd7iklg","2025","Journal Article","Translational behavioral medicine","Monica L. Wang
Kimberly A. Bertrand","10.1093/tbm/ibae075","","Abstract Artificial intelligence (AI) and its subset, machine learning, have tremendous potential to transform health care, medicine, and population health through improved diagnoses, treatments, and patient care. However, the effectiveness of these technologies hinges on the quality and diversity of the data used to train them. Many datasets currently used in machine learning are inherently biased and lack diversity, leading to inaccurate predictions that may perpetuate existing health disparities. This commentary highlights the challenges of biased datasets, the impact on marginalized communities, and the critical need for strategies to address these disparities throughout the research continuum. To overcome these challenges, it is essential to adopt more inclusive data collection practices, engage collaboratively with community stakeholders, and leverage innovative approaches like federated learning. These steps can help mitigate bias and enhance the accuracy and fairness of AI-assisted or informed health care solutions. By addressing systemic biases embedded across research phases, we can build a better foundation for AI to enhance diagnostic and treatment capabilities and move society closer to the goal where improved health and health care can be a fundamental right for all, and not just for some. ","This commentary highlights the need for inclusive data collection practices and collaborative approaches to address biased datasets in machine learning, ensuring AI-assisted health care solutions are accurate, fair, and equitable for all, particularly marginalized communities."
"FanFAIR: sensitive data sets semi-automatic fairness assessment","https://scispace.com/paper/fanfair-sensitive-data-sets-semi-automatic-fairness-u61rir2qfdms","2025","Journal Article","BMC Medical Informatics and Decision Making","Chiara Gallese
Teresa Scantamburlo
Luca Manzoni
Simone Giannerini
Marco S. Nobile","10.1186/s12911-025-03184-4","","Research has shown how data sets convey social bias in Artificial Intelligence systems, especially those based on machine learning. A biased data set is not representative of reality and might contribute to perpetuate societal biases within the model. To tackle this problem, it is important to understand how to avoid biases, errors, and unethical practices while creating the data sets. In order to provide guidance for the use of data sets in contexts of critical decision-making, such as health decisions, we identified six fundamental data set features (balance, numerosity, unevenness, compliance, quality, incompleteness) that could affect model fairness. These features were the foundation for the FanFAIR framework. We extended the FanFAIR framework for the semi-automated evaluation of fairness in data sets, by combining statistical information on data with qualitative features. In particular, we present an improved version of FanFAIR which introduces novel outlier detection capabilities working in multivariate fashion, using two state-of-the-art methods: the Empirical Cumulative-distribution Outlier Detection (ECOD) and Isolation Forest. We also introduce a novel metric for data set balance, based on an entropy measure. We addressed the issue of how much (un)fairness can be included in a data set used for machine learning research, focusing on classification issues. We developed a rule-based approach based on fuzzy logic that combines these characteristics into a single score and enables a semi-automatic evaluation of a data set in algorithmic fairness research. Our tool produces a detailed visual report about the fairness of the data set. We show the effectiveness of FanFAIR by applying the method on two open data sets. ","FanFAIR framework semi-automates fairness assessment in data sets by combining statistical and qualitative features, introducing novel outlier detection and balance metrics, and providing a rule-based approach for evaluating data set fairness in machine learning research."
"De-biasing the bias: methods for improving disparity assessments with noisy group measurements","https://scispace.com/paper/de-biasing-the-bias-methods-for-improving-disparity-192hnibdk4ek","2024","Journal Article","Biometrics","Solvejg Wastvedt
Joshua Snoke
Denis Agniel
Julie Lai
Marc N. Elliott
Steven C. Martino","10.1093/biomtc/ujae155","","Health care decisions are increasingly informed by clinical decision support algorithms, but these algorithms may perpetuate or increase racial and ethnic disparities in access to and quality of health care. Further complicating the problem, clinical data often have missing or poor quality racial and ethnic information, which can lead to misleading assessments of algorithmic bias. We present novel statistical methods that allow for the use of probabilities of racial/ethnic group membership in assessments of algorithm performance and quantify the statistical bias that results from error in these imputed group probabilities. We propose a sensitivity analysis approach to estimating the statistical bias that allows practitioners to assess disparities in algorithm performance under a range of assumed levels of group probability error. We also prove theoretical bounds on the statistical bias for a set of commonly used fairness metrics and describe real-world scenarios where our theoretical results are likely to apply. We present a case study using imputed race and ethnicity from the modified Bayesian Improved First and Surname Geocoding algorithm for estimation of disparities in a clinical decision support algorithm used to inform osteoporosis treatment. Our novel methods allow policymakers to understand the range of potential disparities under a given algorithm even when race and ethnicity information is missing and to make informed decisions regarding the implementation of machine learning for clinical decision support. ","Researchers develop novel statistical methods to assess algorithmic bias in healthcare decisions, accounting for noisy racial/ethnic data, and propose a sensitivity analysis approach to quantify potential disparities under various levels of group probability error."
"Racial Bias in Clinical and Population Health Algorithms: A Critical Review of Current Debates","https://scispace.com/paper/racial-bias-in-clinical-and-population-health-algorithms-a-64epzn1dauo9","2024","Journal Article","Annual Review of Public Health","Madison Coots
Kristin A. Linn
Sharad Goel
Amol S. Navathe
Ravi B. Parikh","10.1146/annurev-publhealth-071823-112058","","Among health care researchers, there is increasing debate over how best to assess and ensure the fairness of algorithms used for clinical decision support and population health, particularly concerning potential racial bias. Here we first distill concerns over the fairness of health care algorithms into four broad categories: ( a ) the explicit inclusion (or, conversely, the exclusion) of race and ethnicity in algorithms, ( b ) unequal algorithm decision rates across groups, ( c ) unequal error rates across groups, and ( d ) potential bias in the target variable used in prediction. With this taxonomy, we critically examine seven prominent and controversial health care algorithms. We show that popular approaches that aim to improve the fairness of health care algorithms can in fact worsen outcomes for individuals across all racial and ethnic groups. We conclude by offering an alternative, consequentialist framework for algorithm design that mitigates these harms by instead foregrounding outcomes and clarifying trade-offs in the pursuit of equitable decision-making. ","This critical review examines racial bias in healthcare algorithms, identifying four categories of bias and analyzing seven prominent algorithms, concluding that popular fairness approaches can worsen outcomes and proposing an alternative framework for equitable decision-making."
"Sex-Based Performance Disparities in Machine Learning Algorithms for Cardiac Disease Prediction: An Exploratory Study (Preprint)","https://scispace.com/paper/sex-based-performance-disparities-in-machine-learning-2n9lhlrqhe","2023","Journal Article","Journal of Medical Internet Research","Isabel Straw
Geraint Rees
Parashkev Nachev","10.2196/46936","","BACKGROUNDThe presence of bias in AI systems has garnered increased attention over the past decade, with inequities in algorithmic performance being exposed across the fields of criminal justice, education, and welfare services.In healthcare, the inequitable performance of medical algorithms across demographic groups may widen health inequalities.Here we identify and characterise bias in cardiology algorithms, looking specifically at algorithms used in the management of heart failure. DISCUSSIONOur research exposes a significant gap in cardiac ML research, highlighting that the underperformance of algorithms for female patients has been overlooked in the published literature.Our study quantifies sex disparities in the algorithmic performance and explores several sources of bias.We found an underrepresentation of females in the datasets used to train algorithms, identified sex biases in model error rates and demonstrated that a series of remediation techniques were unable to address the inequities present. ","This exploratory study reveals significant sex-based performance disparities in machine learning algorithms for cardiac disease prediction, highlighting underrepresentation of females in training datasets and inequitable model error rates across sexes."
"Algorithm Development and Validation: Detecting, Characterizing and Mitigating Implicit and Explicit Racial Biases in Healthcare Datasets with Subgroup Learnability (Preprint)","https://scispace.com/paper/algorithm-development-and-validation-detecting-miu0whqq7f3o","2025","Journal Article","Journal of Medical Internet Research","F. Gulamali
A. Sawant
L. Liharska
Carol Horowitz
Lili Chan
Ira Hofer
Karandeep Singh
Lynne Richardson
Emmanuel Mensah
Alexander Charney
David Reich
Jianying Hu
G. Nadkarni","10.2196/71757","","Abstract Background The growing adoption of diagnostic and prognostic algorithms in health care has led to concerns about the perpetuation of algorithmic bias against disadvantaged groups of individuals. Deep learning methods to detect and mitigate bias have revolved around modifying models, optimization strategies, and threshold calibration with varying levels of success and tradeoffs. However, there have been limited substantive efforts to address bias at the level of the data used to generate algorithms in health care datasets. Objective The aim of this study is to create a simple metric (AEquity) that uses a learning curve approximation to distinguish and mitigate bias via guided dataset collection or relabeling. Methods We demonstrate this metric in 2 well-known examples, chest X-rays and health care cost utilization, and detect novel biases in the National Health and Nutrition Examination Survey. Results We demonstrated that using AEquity to guide data-centric collection for each diagnostic finding in the chest radiograph dataset decreased bias by between 29% and 96.5% when measured by differences in area under the curve. Next, we wanted to examine (1) whether AEquity worked on intersectional populations and (2) if AEquity is invariant to different types of fairness metrics, not just area under the curve. Subsequently, we examined the effect of AEquity on mitigating bias when measured by false negative rate, precision, and false discovery rate for Black patients on Medicaid. When we examined Black patients on Medicaid, at the intersection of race and socioeconomic status, we found that AEquity-based interventions reduced bias across a number of different fairness metrics including overall false negative rate by 33.3% (bias reduction absolute=1.88×10−1, 95% CI 1.4×10−1 to 2.5×10−1; bias reduction of 33.3%, 95% CI 26.6%‐40%; precision bias by 7.50×10−2, 95% CI 7.48×10−2 to 7.51×10−2; bias reduction of 94.6%, 95% CI 94.5%‐94.7%; false discovery rate by 94.5%; absolute bias reduction=3.50×10−2, 95% CI 3.49×10−2 to 3.50×10−2). Similarly, AEquity-guided data collection demonstrated bias reduction of up to 80% on mortality prediction with the National Health and Nutrition Examination Survey (bias reduction absolute=0.08, 95% CI 0.07-0.09). Then, we wanted to compare AEquity to state-of-the-art data-guided debiasing measures such as balanced empirical risk minimization and calibration. Consequently, we benchmarked against balanced empirical risk minimization and calibration and showed that AEquity-guided data collection outperforms both standard approaches. Moreover, we demonstrated that AEquity works on fully connected networks; convolutional neural networks such as ResNet-50; transformer architectures such as VIT-B-16, a vision transformer with 86 million parameters; and nonparametric methods such as Light Gradient-Boosting Machine. Conclusions In short, we demonstrated that AEquity is a robust tool by applying it to different datasets, algorithms, and intersectional analyses and measuring its effectiveness with respect to a range of traditional fairness metrics.","Researchers develop AEquity, a metric to detect and mitigate bias in healthcare datasets, demonstrating its effectiveness in reducing bias by 29-96.5% in chest X-rays and 80% in mortality prediction, outperforming state-of-the-art debiasing measures."
"Ethical AI in medical text generation: balancing innovation with privacy in public health","https://scispace.com/paper/ethical-ai-in-medical-text-generation-balancing-innovation-bjd8c7cbvmz1","2025","Journal Article","Frontiers in Public Health","Mingpei Liang","10.3389/fpubh.2025.1583507","","The integration of artificial intelligence (AI) into medical text generation is transforming public health by enhancing clinical documentation, patient education, and decision support. However, the widespread deployment of AI in this domain introduces significant ethical challenges, including fairness, privacy protection, and accountability. Traditional AI-driven medical text generation models often inherit biases from training data, resulting in disparities in healthcare communication across different demographic groups. Moreover, ensuring patient data confidentiality while maintaining transparency in AI-generated content remains a critical concern. Existing approaches either lack robust bias mitigation mechanisms or fail to provide interpretable and privacy-preserving outputs, compromising ethical compliance and regulatory adherence. To address these challenges, this paper proposes an innovative framework that combines privacy-preserving AI techniques with interpretable model architectures to achieve ethical compliance in medical text generation. The method employs a hybrid approach that integrates knowledge-based reasoning with deep learning, ensuring both accuracy and transparency. Privacy-enhancing technologies, such as homomorphic encryption and secure multi-party computation, are incorporated to safeguard sensitive medical data throughout the text generation process. Fairness-aware training protocols are introduced to mitigate biases in generated content and enhance trustworthiness. The proposed approach effectively addresses critical challenges of bias, privacy, and interpretability in medical text generation. By combining symbolic reasoning with data-driven learning and embedding ethical principles at the system design level, the framework ensures regulatory alignment and improves public trust. This methodology lays the groundwork for broader deployment of ethically sound AI systems in healthcare communication. ","This paper proposes a hybrid AI framework for medical text generation that balances innovation with privacy, addressing fairness, bias, and accountability concerns through a combination of knowledge-based reasoning, deep learning, and privacy-enhancing technologies."
"UnBias: Unveiling Bias Implications in Deep Learning Models for Healthcare Applications","https://scispace.com/paper/unbias-unveiling-bias-implications-in-deep-learning-models-77lu5d5a9118","2024","Journal Article","IEEE Journal of Biomedical and Health Informatics","Asmaa AbdulQawy
Elsayed A. Sallam
Amr Elkholy","10.1109/jbhi.2024.3484951","","The rapid integration of deep learningpowered artificial intelligence systems in diverse applications such as healthcare, credit assessment, employment, and criminal justice has raised concerns about their fairness, particularly in how they handle various demographic groups. This study delves into the existing biases and their ethical implications in deep learning models. It introduces an UnBias approach for assessing bias in different deep neural network architectures and detects instances where bias seeps into the learning process, shifting the model's focus away from the main features. This contributes to the advancement of equitable and trustworthy AI applications in diverse social settings, especially in healthcare. A case study on COVID-19 detection is carried out, involving chest X-ray scan datasets from various publicly accessible repositories and five well-represented and underrepresented gender-based models across four deep-learning architectures: ResNet50V2, DenseNet121, InceptionV3, and Xception. ","This study investigates biases in deep learning models for healthcare applications, introducing the UnBias approach to detect and assess bias in various neural network architectures, promoting equitable and trustworthy AI in healthcare, particularly in COVID-19 detection using chest X-ray scans."
"Race Against the Machine Learning Courses","https://scispace.com/paper/race-against-the-machine-learning-courses-px4e792u7t5w","2025","Journal Article","ACM Transactions on Intelligent Systems and Technology","R. Deshpande
Donald Mlombwa
Leo Anthony Celi
Jack Gallifant
Helen D’Couto","10.1145/3737650","","Despite the rapid integration of AI in healthcare, a critical gap exists in current machine learning courses: the lack of education on identifying and mitigating bias in datasets. This oversight risks perpetuating existing health disparities through biased AI models. Analyzing 11 prominent online courses, we found only 5 addressed dataset bias, often dedicating minimal time compared to technical aspects. This paper urges course developers to prioritize education on data context, equipping learners with the tools to critically evaluate the origin, collection methods, and potential biases inherent in the data. This approach fosters the creation of fair algorithms and the incorporation of diverse data sources, ultimately mitigating the harmful effects of bias in healthcare AI. While this analysis focused on publicly available courses, it underscores the urgency of addressing bias in all healthcare machine learning education. Early intervention in algorithm development is crucial to prevent the amplification of dataset and model bias, ensuring responsible and equitable AI implementation in healthcare. ","Current machine learning courses in healthcare neglect education on dataset bias, risking perpetuation of health disparities through biased AI models; prioritizing data context education is crucial to create fair algorithms and mitigate bias in healthcare AI."