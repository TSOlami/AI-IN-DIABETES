"Paper Title","Paper Link","Publication Year","Publication Type","Publication Title","Author Names","DOI","PDF Link","Abstract","Relevant Excerpt"
"Reducing Domain Gap in Frequency and Spatial domain for Cross-modality
  Domain Adaptation on Medical Image Segmentation","https://scispace.com/paper/reducing-domain-gap-in-frequency-and-spatial-domain-for-199dn4a5","2022","Posted Content","","","10.48550/arxiv.2211.15235","https://scispace.com/pdf/reducing-domain-gap-in-frequency-and-spatial-domain-for-199dn4a5.pdf","Unsupervised domain adaptation (UDA) aims to learn a model trained on source domain and performs well on unlabeled target domain. In medical image segmentation field, most existing UDA methods depend on adversarial learning to address the domain gap between different image modalities, which is ineffective due to its complicated training process. In this paper, we propose a simple yet effective UDA method based on frequency and spatial domain transfer uner multi-teacher distillation framework. In the frequency domain, we first introduce non-subsampled contourlet transform for identifying domain-invariant and domain-variant frequency components (DIFs and DVFs), and then keep the DIFs unchanged while replacing the DVFs of the source domain images with that of the target domain images to narrow the domain gap. In the spatial domain, we propose a batch momentum update-based histogram matching strategy to reduce the domain-variant image style bias. Experiments on two cross-modality medical image segmentation datasets (cardiac, abdominal) show that our proposed method achieves superior performance compared to state-of-the-art methods. ","Title: Reducing Domain Gap in Frequency and Spatial domain for Cross-modality Domain Adaptation on Medical Image Segmentation Authors: Shaolei Liu,Siqi Yin,Linhao Qu,Manning Wang Introduction:
 Deep learning has a wide range of applications in the medical field, including image segmentation , classification , detection , etc. The success of deep learning heavily depends on large amount of annotated data for model training, but the pixel-level annotation of medical images for segmentation is expensive and time-consuming, because the annotation usually can only be done by experienced radiologists. To tackle the difficulty of obtaining new annotated datasets, one solution is to utilize other annotated datasets . However, the differences on imaging principles and equipment parameters (scanners, protocols and modalities) lead to domain gap between datasets, and the performance of a model trained on one domain tends to deteriorate when it is directly used in another domain . Unsupervised domain adaptation (UDA) is the technique to address this problem by training a network with labeled source domain data and unlabeled target domain data, aiming at improving the performance on target domain . Introduction:
 The main objective of UDA is to minimize the influence of domain gap on the performance of the trained model in the target domain, which can be realized by either generating target-like images for model training or encouraging the model to focus on domain-invariant information instead of domain-variant information. In the UDA for medical image segmentation, most studies leverage adversarial learning to stylize source-domain images at the image level  to generate target-like images for model training. Other studies apply adversarial learning at the feature level to maximize the confusion between representations in source and target domains to make the model learn domain invariant representations . Although satisfactory results have been achieved, the adversarial training process is complicated and is prone to collapse. Introduction:
 In recent years, several UDA methods without using adversarial learning have been developed in the CV field, which can be divided into two categories: spatial domainbased methods and frequency domain-based methods. Spatial domain-based methods generate target-like images by simple cutmix-like region replacement ), statistical information adjustment  or histogram matching . Frequency domain-based methods first transform images into frequency components by Discrete Fourier Transform (DFT)  or Discrete Cosine Transform (DCT) ) and narrow domain gap by manipulating the frequency components. These methods have yielded promising results in the CV field, but no studies have been explored in medical image analysis."
"Application of Unsupervised Domain Adaptation for Structural MRI
  Analysis","https://scispace.com/paper/application-of-unsupervised-domain-adaptation-for-structural-37ryqicu","2022","Posted Content","","Rafael Harnos","10.48550/arxiv.2212.12986","https://scispace.com/pdf/application-of-unsupervised-domain-adaptation-for-structural-37ryqicu.pdf","The primary goal of this work is to study the effectiveness of an unsupervised domain adaptation approach for various applications such as binary classification and anomaly detection in the context of Alzheimer's disease (AD) detection for the OASIS datasets. We also explore image reconstruction and image synthesis for analyzing and generating 3D structural MRI data to establish performance benchmarks for anomaly detection. We successfully demonstrate that domain adaptation improves the performance of AD detection when implemented in both supervised and unsupervised settings. Additionally, the proposed methodology achieves state-of-the-art performance for binary classification on the OASIS-1 dataset. ","Title: Application of Unsupervised Domain Adaptation for Structural MRI Analysis Authors: Pranath Reddy Keywords: ML in Medicine, Deep Learning, Classification, Anomaly Detection, Domain Adaptation I. INTRODUCTION:
 Alzheimer's disease (AD), one of the most prevalent degenerative conditions, is a degenerative dementia that begins with minor memory loss and gradually escalates to a complete loss of mental and physical capacities. For the patient's health, a diagnosis should be made as soon as possible so that treatment and preventative measures can be commenced. A thorough and comprehensive medical evaluation involving an array of psychological and physical testing is necessary to diagnose AD. The ability and expertise of the clinician have a direct impact on the precision of the psychological and cognitive tests, and a magnetic resonance imaging (MRI) medical data analysis is necessary for a conclusive medical diagnosis of AD. Medical specialists are in responsibility of evaluating and interpreting medical data, however due to the data subjectivity and complexity, this procedure is particularly complex and constrained for a medical specialist. Consequently, there is a need for the development of quick and efficient diagnostic tools, and machine learning (ML) can be employed for this. I. INTRODUCTION:
 The ability of the computer to learn from experience without explicit programming has improved thanks to machine learning approaches. For classification, regression, clustering, and dimensionality reduction in a variety of applications like image processing, predictive analytics, and data mining, machine learning approaches are applied. In the field of medical imaging research, machine learning (ML) is frequently used and produces outstanding results in tasks like segmentation, regression, classification, etc. Medical image analysis tasks seldom have access to such expansive datasets, unlike some visual image datasets like ImageNet, which offer large-scale annotated examples for developing models for tasks like object detection. Using pre-trained models for some related domains using the transfer learning method is a practical and effective solution. I. INTRODUCTION:
 This study deals with a specific type of transfer learning approach called domain adaptation. Domain adaptation is a very effective learning technique that has been used for analyzing medical image data . In this project, we will be focusing on studying 3D structural MRI brain scans in the context of Alzheimer's disease classification. Real-world medical datasets are prone to being filled with unlabeled and ill-labeled samples, which makes working with them very challenging. The process of labeling medical images is typically expensive, time-consuming, and laborintensive, necessitating the involvement of doctors, radiologists, and other experts. Where transfer learning strategies like domain adaptation may be useful is in this situation. While there has been a lot of work related to the development of novel classification algorithms for Alzheimer's detection and classification, the study of interoperability and reusability of such models is often ignored, and this is very important since a model that performs well on one dataset might not replicate that performance when tested on a different dataset, and the samples themselves can be very different with a large domain gap between them, and domain adaptation could be useful in mitigating problems associated with domain shift."
"Why does my medical AI look at pictures of birds? Exploring the efficacy
  of transfer learning across domain boundaries","https://scispace.com/paper/why-does-my-medical-ai-look-at-pictures-of-birds-exploring-1g3as0u5","2023","Posted Content","","","10.48550/arxiv.2306.17555","https://scispace.com/pdf/why-does-my-medical-ai-look-at-pictures-of-birds-exploring-1g3as0u5.pdf","It is an open secret that ImageNet is treated as the panacea of pretraining. Particularly in medical machine learning, models not trained from scratch are often finetuned based on ImageNet-pretrained models. We posit that pretraining on data from the domain of the downstream task should almost always be preferred instead. We leverage RadNet-12M, a dataset containing more than 12 million computed tomography (CT) image slices, to explore the efficacy of self-supervised pretraining on medical and natural images. Our experiments cover intra- and cross-domain transfer scenarios, varying data scales, finetuning vs. linear evaluation, and feature space analysis. We observe that intra-domain transfer compares favorably to cross-domain transfer, achieving comparable or improved performance (0.44% - 2.07% performance increase using RadNet pretraining, depending on the experiment) and demonstrate the existence of a domain boundary-related generalization gap and domain-specific learned features. ","scans. However, this limitation is only of theoretical concern, as all practical tasks in image-guided medical machine learning come with this caveat. Conclusions & Outlook: We summarize a general set of rules for optimal transfer learning performance, based on our experiments: Conclusions & Outlook:
 • Intra-domain transfer learning typically outperforms cross-domain transfer learning by at least a small margin in performance. Occasionally, this is accompanied by increased convergence speed. In short, ""My medical AI should be looking at pictures of tumors instead of birds to learn about tumors."" • The higher the downstream task's complexity, the more valuable intra-domain pretraining becomes. • Large-scale pretraining almost always results in performance gains. We observed no performance saturation for further upscaling of the pretraining in many scenarios, with RadNet-12M-pretrained models often outperforming ImageNet-1k-pretrained models. Conclusions & Outlook:
 This insight is particularly relevant for clinical settings, where plentiful data from the same imaging modality are often available but very costly to annotate. This data should be leveraged in unsupervised pretraining where possible. • There is no ""one size fits all"" solution. Using ImageNet as a default pretraining dataset is suitable. However, if the best possible performance on that task is more relevant than a quick solution, there is no substitute for testing multiple pretraining datasets, particularly from the same data domain as the task. • We observed that the generalization gap varies very nuancedly; Even the target class or metric, e.g. liver vs. lesion on the LiTS task, can be relevant for the choice of pretraining dataset. Conclusions & Outlook:
 In future work, we plan to extend RadNet-12M with additional data from other modalities, such as MRI and X-rays. We also plan to pretrain additional architectures using RadNet and publish their model weights. Further, we believe that research extending our work to other pretraining algorithms and supervised scenarios constitutes a promising research direction, which would address some of the limitations of this work. Creating large-scale models pretrained on a previously untapped wealth of unlabeled clinical images will be of great value to the exponentially growing, yet notoriously data-starved  field of medical computer vision."
"Explainable, Domain-Adaptive, and Federated Artificial Intelligence in
  Medicine","https://scispace.com/paper/explainable-domain-adaptive-and-federated-artificial-12vynjtr","2022","Posted Content","","","10.48550/arxiv.2211.09317","https://scispace.com/pdf/explainable-domain-adaptive-and-federated-artificial-12vynjtr.pdf","Artificial intelligence (AI) continues to transform data analysis in many domains. Progress in each domain is driven by a growing body of annotated data, increased computational resources, and technological innovations. In medicine, the sensitivity of the data, the complexity of the tasks, the potentially high stakes, and a requirement of accountability give rise to a particular set of challenges. In this review, we focus on three key methodological approaches that address some of the particular challenges in AI-driven medical decision making. (1) Explainable AI aims to produce a human-interpretable justification for each output. Such models increase confidence if the results appear plausible and match the clinicians expectations. However, the absence of a plausible explanation does not imply an inaccurate model. Especially in highly non-linear, complex models that are tuned to maximize accuracy, such interpretable representations only reflect a small portion of the justification. (2) Domain adaptation and transfer learning enable AI models to be trained and applied across multiple domains. For example, a classification task based on images acquired on different acquisition hardware. (3) Federated learning enables learning large-scale models without exposing sensitive personal health information. Unlike centralized AI learning, where the centralized learning machine has access to the entire training data, the federated learning process iteratively updates models across multiple sites by exchanging only parameter updates, not personal health data. This narrative review covers the basic concepts, highlights relevant corner-stone and state-of-the-art research in the field, and discusses perspectives. ","cross-modality DA between MRI and CT images has been applied using the synergistic image and feature alignment framework (SIFA) . Despite the practicability of DA, there are always remaining risks of inherent biases in the data or unfairness in models . B. Domain-adaptive artificial intelligence:
 c) Single-and multi-source domain adaptation: Depending on the number of source domains, it is possible to classify DA into single-source DA (SDA), multi-source DA (MDA) and source-free DA . For example, trained models using single-source domains lack generalization. Multi-source DA collects labeled data from multiple sources with different distributions  and implicitly learn to be robust. For example, MDA learning can alleviate the difficulties caused by aligning multimodal characteristics between multiple domains . Although MDA is advanced, MDA applications are challenging due to the heterogeneity of data between domains. For this reason, most existing studies are based on SDA . Another issue could be related to the source domains, cause to data privacy or lack of storage space for small devices . A generalized source-free domain adaptation (G-SFDA) was proposed to possess source domain features via transfer learning . B. Domain-adaptive artificial intelligence:
 d) One-and multi-step DA: On the basis of the distance between the source and the target domain, domain adaptation methods can be roughly categorized into two types: one-step and multistep domain adaptation methods. The one-step DA methods often simply align the feature distributions of the two domains by minimizing their distances. But it is difficult for the network to minimize the huge domain discrepancy. Specifically, in remote domain transfer learning, when the distance between the source domain and the target domain is too long , it is impossible to achieve and can even affect the performance of the learning model in the target domain. This situation is called negative transfer . In order to avoid this situation, it is necessary to find an ""intermediate bridge"" to connect the source and the target domain. That is called multistep DA. It can be transformed into one-step domain adaptation by choosing an appropriate intermediate domain. B. Domain-adaptive artificial intelligence:
 For example, through efficient instance selection, transfer features are successfully applied between remote domains and effectively solve the remote domain problem . B. Domain-adaptive artificial intelligence:
 e) Shallow and deep DA: According to the type of model, we can divide the DA method into shallow and deep DA. Usually, shallow DA leverages feature engineering and is used with traditional machine learning. But few timely reviews the emerging deep learning based methods. Unlike the shallow DA, deep DA methods can leverage deep networks to learn more transferable representations by embedding DA in a deep learning pipeline . In addition, deep DA integrates a feature into the learning model to improve performance metrics . B. Domain-adaptive artificial intelligence:
 These methods may reduce the differences in the domain that can have a positive impact in clinical research. Specifically, more effort is required to fully apply such methods in clinical topics to avoid bias. C. Federated learning:
 Federated Learning (FL) is a way of distributed collaborative machine learning in which the training data is not shared. This concept of data-private collaborative learning , that initially was motivated in the context of mobile devices in which each one has only a small fraction of the data, equally applies to medical application in which the data is distributed across hospitals ."
"Curriculum-Based Augmented Fourier Domain Adaptation for Robust Medical
  Image Segmentation","https://scispace.com/paper/curriculum-based-augmented-fourier-domain-adaptation-for-80fhakkc","2023","Posted Content","","","10.48550/arxiv.2306.03511","https://scispace.com/pdf/curriculum-based-augmented-fourier-domain-adaptation-for-80fhakkc.pdf","Accurate and robust medical image segmentation is fundamental and crucial for enhancing the autonomy of computer-aided diagnosis and intervention systems. Medical data collection normally involves different scanners, protocols, and populations, making domain adaptation (DA) a highly demanding research field to alleviate model degradation in the deployment site. To preserve the model performance across multiple testing domains, this work proposes the Curriculum-based Augmented Fourier Domain Adaptation (Curri-AFDA) for robust medical image segmentation. In particular, our curriculum learning strategy is based on the causal relationship of a model under different levels of data shift in the deployment phase, where the higher the shift is, the harder to recognize the variance. Considering this, we progressively introduce more amplitude information from the target domain to the source domain in the frequency space during the curriculum-style training to smoothly schedule the semantic knowledge transfer in an easier-to-harder manner. Besides, we incorporate the training-time chained augmentation mixing to help expand the data distributions while preserving the domain-invariant semantics, which is beneficial for the acquired model to be more robust and generalize better to unseen domains. Extensive experiments on two segmentation tasks of Retina and Nuclei collected from multiple sites and scanners suggest that our proposed method yields superior adaptation and generalization performance. Meanwhile, our approach proves to be more robust under various corruption types and increasing severity levels. In addition, we show our method is also beneficial in the domain-adaptive classification task with skin lesion datasets. The code is available at https://github.com/lofrienger/Curri-AFDA. ","for Medical Image Classification: As shown in Table V, our method yields the best overall result of f1 score on the skin lesion datasets. This demonstrates that our methods are also supportive of the domain-adaptive medical image classification task in addition to segmentation. VII. CONCLUSION AND DISCUSSION:
 This work proposes the Curriculum-based Augmented Fourier Domain Adaptation (Curri-AFDA) and proves to achieve superior adaptation, generalization, and robustness performance for medical image segmentation. Specifically, we design a novel curriculum strategy to progressively transfer amplitude information in the Fourier space from the target domain to the source domain to mitigate domain gaps and incorporate the chained augmentation mixing to further improve the generalization and robustness ability. Our method is naturally modality-independent due to its independence on any particular properties of the imaging modality. Without additional trainable parameters, extensive experiments on two segmentation tasks with multiple-domain datasets of two image modalities demonstrate the efficacy of our method on top of both the classical CNN (UNet ) and recent transformer (Swin-UNet ) architectures. Specially, we consider the crucial yet rarely explored topic in medical image analysis, i.e., the robustness performance with the synthetic dataset generated by different types and levels of corruptions, and also observe the superior results of our method. Additionally, our method can also contribute to medical image classification besides segmentation, indicating its potential for broader medical applications. VII. CONCLUSION AND DISCUSSION:
 Future research may focus on designing more flexible and automatic scheduling functions to update the amplitude scaling coefficient which adjusts the amplitude fusion area. In addition, the weighting coefficient which controls the merging ratio between images can also be involved when designing the curriculum strategy. Besides the training-time Domain Adaptation, test-time Domain Adaptation ,  is also worth to be explored by integrating the Fourier-based cross-domain information fusion and the chained augmentation mixing."
"Adversarial and Random Transformations for Robust Domain Adaptation and
  Generalization","https://scispace.com/paper/adversarial-and-random-transformations-for-robust-domain-15657fyn","2022","Posted Content","","","10.48550/arxiv.2211.06788","https://scispace.com/pdf/adversarial-and-random-transformations-for-robust-domain-15657fyn.pdf","Data augmentation has been widely used to improve generalization in training deep neural networks. Recent works show that using worst-case transformations or adversarial augmentation strategies can significantly improve the accuracy and robustness. However, due to the non-differentiable properties of image transformations, searching algorithms such as reinforcement learning or evolution strategy have to be applied, which are not computationally practical for large scale problems. In this work, we show that by simply applying consistency training with random data augmentation, state-of-the-art results on domain adaptation (DA) and generalization (DG) can be obtained. To further improve the accuracy and robustness with adversarial examples, we propose a differentiable adversarial data augmentation method based on spatial transformer networks (STN). The combined adversarial and random transformations based method outperforms the state-of-the-art on multiple DA and DG benchmark datasets. Besides, the proposed method shows desirable robustness to corruption, which is also validated on commonly used datasets. ","the mean accuracy. The results of multi-source domain adaptation and domain generalization which take photo, cartoon and sketch as the source domains and art_painting as the target domain are reported in Figure . Resnet-18 is used as the base network. Ablation study on hyperparameters setting:
 From the figures, we can see that when both λ c and λ t are not too large, the accuracies are relatively stable, validating the insensitiveness of our proposed method to hyperparameters. However, when λ c and λ t grow too large, the performance decreases, especially with a small λ c and a large λ t . The reason maybe that overwhelmingly large weights for consistency loss and adversarial spatial transformer loss over the main classification loss make the learned feature less discriminative for the main classification task, therefore resulting in lower accuracy. And too large λ t may lead to excessive emphasis on the extreme geometric distortions, which may be harmful to the general cross domain performance. Visualization of learned deep features:
 To better understand the learned domain invariant feature representations, we use t-SNE  to conduct an embedding visualization. We conduct experiments on transfer task of photo, cartoon, sketch → art_painting with both DA and DG settings and visualize the feature embeddings. Figure4 shows the visualization on PACS DA setting, and Figure5 shows the visualization on PACS DG setting. On both figures, we visualize category alignment as well as domain alignment. We also compare to the baseline Deep All which does not apply any adaptation. Visualization of learned deep features:
 From the visualization of the embeddings, we can see that the clusters created by our model not only separates the categories but also mixes the domains. The visualization from DG model suggests that our proposed method is able to learn feature representations generalizable to unseen domains. It also implies that the proposed method can effectively learn domain invariant representation with unlabeled target domain examples. Conclusion:
 In this work, we proposed a unified framework for addressing both domain adaptation and generalization problems. Our domain adaptation and generalization methods are built upon random image transformation"
"Generalizability of Deep Adult Lung Segmentation Models to the Pediatric
  Population: A Retrospective Study","https://scispace.com/paper/generalizability-of-deep-adult-lung-segmentation-models-to-qd8cpnib","2022","Posted Content","","","10.48550/arxiv.2211.02475","https://scispace.com/pdf/generalizability-of-deep-adult-lung-segmentation-models-to-qd8cpnib.pdf","Lung segmentation in chest X-rays (CXRs) is an important prerequisite for improving the specificity of diagnoses of cardiopulmonary diseases in a clinical decision support system. Current deep learning models for lung segmentation are trained and evaluated on CXR datasets in which the radiographic projections are captured predominantly from the adult population. However, the shape of the lungs is reported to be significantly different across the developmental stages from infancy to adulthood. This might result in age-related data domain shifts that would adversely impact lung segmentation performance when the models trained on the adult population are deployed for pediatric lung segmentation. In this work, our goal is to (i) analyze the generalizability of deep adult lung segmentation models to the pediatric population and (ii) improve performance through a stage-wise, systematic approach consisting of CXR modality-specific weight initializations, stacked ensembles, and an ensemble of stacked ensembles. To evaluate segmentation performance and generalizability, novel evaluation metrics consisting of mean lung contour distance (MLCD) and average hash score (AHS) are proposed in addition to the multi-scale structural similarity index measure (MS-SSIM), the intersection of union (IoU), Dice score, 95% Hausdorff distance (HD95), and average symmetric surface distance (ASSD). Our results showed a significant improvement (p < 0.05) in cross-domain generalization through our approach. This study could serve as a paradigm to analyze the cross-domain generalizability of deep segmentation models for other medical imaging modalities and applications. ","sub-optimal generalization. Further, this domain shift might lead to poor performance, if not failure, when these models are deployed in expert systems, which can potentially degrade people's trust in the model, and threaten clinical decisions and bestpractice recommendations. INTRODUCTION:
 This study aims to analyze age-related domain generalization, i.e., the generalizability of deep adult lung segmentation models to the pediatric population. Fig.  illustrates the workflow of age-related domain generalization in the context of our study. Here, the widely used Shenzhen, Montgomery, and JSRT CXR collections represent the multiple source domains and a private pediatric CXR collection  represents the unseen target domain. We aim to evaluate age-related domain generalization, i.e., analyzing if the DL models trained on multiple source domains containing the CXRs captured from the adult population would generalize to an unseen pediatric CXR collection. RELATED LITERATURE AND CONTRIBUTIONS OF THE STUDY:
 Recent research has demonstrated the importance of domain generalization in medical image analysis and provided insight into the techniques used to achieve domain generalization in DL models, particularly when trained and evaluated on medical imaging datasets that often have limited sample sizes, high levels of variability, and complex underlying structures. A novel transfer learning method based on instance-level adaptation was proposed in the literature  to improve generalization. With limited data availability, the proposal demonstrated superior performance on unseen target domains compared to conventional transfer learning methods. In another study , the authors proposed a method to manipulate the data and improve domain generalization. The data was augmented to generate novel imaging domains that improved the training set's diversity. In another study , the authors measured the representation shift that quantified the magnitude of domain shift in a tumor classification task. The authors observed that the representation shift correlated with reduced performance during cross-domain evaluation and helped evaluate the sensitivity of the model to domain variations. RELATED LITERATURE AND CONTRIBUTIONS OF THE STUDY:
 A variety of domain generalization solutions are proposed in the literature in the context of CXR image analysis. In , the authors empirically demonstrated model generalization issues due to label shifts. They observed that the external test data was inadequately represented in the training and even high-performing models disagreed with their predictions. In another study , the authors trained CNN-based models on publicly available CXR datasets to classify them as showing pneumoniaconsistent manifestations or normal lungs. A significant reduction in performance was reported using crossinstitutional test sets compared to the internal test sets. In a recent study , the authors trained DL models to classify the pediatric CXRs as showing pneumonia-consistent manifestations or normal lungs and evaluated model generalization using external test sets. They observed that the models delivered a superior performance with the internal test set but had a significantly lower performance with an external test set. A semi-supervised regularization method was proposed in the literature . This method used the stability and orthogonality of the learned features to alleviate domain generalization issues in a CXR classification task. All these prior works demonstrate the importance of domain generalization in medical imaging and propose innovative methods for improving the generalization performance of DL models across different domains."
"A Review of Causality for Learning Algorithms in Medical Image Analysis","https://scispace.com/paper/a-review-of-causality-for-learning-algorithms-in-medical-moei42b1","2022","Posted Content","","","10.48550/arxiv.2206.05498","https://scispace.com/pdf/a-review-of-causality-for-learning-algorithms-in-medical-moei42b1.pdf","Medical image analysis is a vibrant research area that offers doctors and medical practitioners invaluable insight and the ability to accurately diagnose and monitor disease. Machine learning provides an additional boost for this area. However, machine learning for medical image analysis is particularly vulnerable to natural biases like domain shifts that affect algorithmic performance and robustness. In this paper we analyze machine learning for medical image analysis within the framework of Technology Readiness Levels and review how causal analysis methods can fill a gap when creating robust and adaptable medical image analysis algorithms. We review methods using causality in medical imaging AI/ML and find that causal analysis has the potential to mitigate critical problems for clinical translation but that uptake and clinical downstream research has been limited so far. ","discovery of medical biomarkers in brain MRI volumes. Generative methods: Finally ) use deep diffusion model to ask counterfactual questions and generate hard to obtain medical scans. These, in turn, are used to augment existing datasets for other downstream tasks. Domain Generalization:
 One of the most promising areas where causal reasoning can be applied in the field of medical imaging is Domain Adaptation and Out-of-Distribution detection, directly associated with TRL 6.4. If we model the generative process that results in a medical image and include factors like the medical history, the disease, imaging domain, etc. we can then go on and interpret domain generalization and adaptation as a model that is able to perform well under different treatments in the imaging domain parameter, as argued by . In their paper Huang et al model domain adaptation as a non stationary change in the underlying causal graph and propose methods to identify and resolve these changes.  analyze the domain shifts experienced in clinical deployment of AIML algorithms from a causal perspective and then proceed to investigate and benchmark eight popular methods of domain generalization. They find that domain generalization methods fail to provide any improvement in performance over empirical risk minimization in situations where we find sampling bias. Similarly  model the causal relationships leading to the medical images and create synthetic datasets in order to evaluate the transportability of methods to external settings where interventions on factors like ages, sex and medical metrics have been performed.  apply a causal analysis on the problem of domain generalization in segmentation of medical images. They first simulate shifted domain images via a randomly weighted shallow network; then they intervene upon the images such that spurious correlations are removed and finally train their segmentation model while enforcing a domain invariance condition.  develop a method to reuse adversarial mask discriminators for test-time training to combat distribution shifts in medical image segmentation tasks. In their discussion of their method they explain the good performance of their method under a causal lens. Finally  build a causal Bayesian prior to aide MRI tissue segmentation to generalize across different medical centers."
"Domain Generalization and Adaptation with Generative Modeling and Representation Learning","https://scispace.com/paper/domain-generalization-and-adaptation-with-generative-3netsdog2b","2021","","","Jaideep Vitthal Murkute","","https://scispace.com/pdf/domain-generalization-and-adaptation-with-generative-3netsdog2b.pdf","","adaptation, by considering the 'group' information as the domain related information. For this study, we will limit to the computer vision application and the task of object classification; however, we proposed methods and learning can be extended to other application areas. Objectives:
 The core objective of this work is to build a domain generalization and adaptation framework by leveraging generative modeling, and in particular VAEs, which is the area less explored. Not many in depth studies exist on this topic that visually analyze the representations learned by the networks in detail. We also plan to perform in-depth ablation study of the effectiveness of components of the framework developed. For the domain adaptation method, we aim to build a method that can adapt to any number of (≥ 1) of source and target domains. Although in this work, we'll be considering the computer vision application and the image classification task, we aim to develop a generic framework, which can be adapted to other applications and tasks. Background:
 In this section, we will briefly introduce the terminologies and concepts that are part of the proposed methodology. Domain Generalization:
 Generalization in machine learning refers to the models ability to perform well or generalize to the previously unseen, new data samples. While in typical setting of machine learning models, no special assumption is made about the data samples on which generalization ability of the model is tested. Sometimes, it's ensured that data distribution of the data samples on which generalization power is being measured is represented sufficiently in the seen or the training samples. Domain Generalization:
 The goal of the domain generalization methods is to perform well across the data domains and evaluated by the performance on the data samples from an unseen domain(s). The 'domain' refers to the particular setting in which images or data samples are recorded, and domain assumed to have significant effect on the characteristics of the input. Data within a domain assumed to reflect commonalities belonging to that domain while data samples from different domains differ significantly for the aspects influenced by the domain. A simple example of such domain can be images of cats taken in an common indoor setting vs images taken in the forest. Domain Adaptation:
 Domain adaptation(DA) approaches consider a scenario where models can learn from or adapt to the target domain data, but without the ground truth labels. Domain adaptation is considered as a sub-category of more popular transfer learning. Domain adaptation assumes that the source and the target domains share a same or similar feature space but they differ in the data distribution. Transfer learning includes all scenarios where the target dataset can have significantly different feature space or even task description. The shift in source and target feature distributions, mentioned above, is popularly termed as 'domain shift' in the DA setting. One of the essential desired characteristic of the domain adaptation methods is the capability to update the learned distribution to accommodate and perform well on this new distribution. Domain Adaptation:
 From learning methodology perspective, domain adaptation methods can be broadly classified as unsupervised, semi-supervised and weakly-supervised methods, while unsupervised setting being the most common setting studied. Representation learning:
 Representation learning refers to the task of learning representations of the data that makes it easier to extract useful information about the relationships between categories, and which in turn can make the easier to analyze and improve downstream tasks like classification . Our proposed approach is centered around the representation learning and disentangled representation learning."
"BayeSeg: Bayesian Modeling for Medical Image Segmentation with Interpretable Generalizability","https://scispace.com/paper/bayeseg-bayesian-modeling-for-medical-image-segmentation-b8ipt3ca","2023","Journal Article","Medical Image Analysis","Shangqi Gao
Hang Zhou
Yibo Gao
Xiahai Zhuang","10.48550/arXiv.2303.01710","https://scispace.compdf/bayeseg-bayesian-modeling-for-medical-image-segmentation-b8ipt3ca.pdf","","segmentation: The aim of domain generalization (DG) is to make a model trained on one or several source domains generalize well on unseen target domains. Different from domain adaptation (DA), DG has no access to target domain data during the training process. Domain generalization for medical image segmentation:
 This makes DG more challenging, but closer to real-world situations like clinical applications. In general, from the perspective of techniques, DG methods can be roughly categorized into three groups, i.e., data-based approaches, learning-based approaches and representation-based approaches . Domain generalization for medical image segmentation:
 enrich the diversity of data distribution in training by data augmentation or data generation to enhance model generalization ability. For medical image segmentation,  applied a series of stacked transformations to simulate domain shift for a specific medical imaging modality. Domain generalization for medical image segmentation:
 learned the bias-field and deformation field to generate plausible and realistic signal corruptions through adversarial learning, and  adapted adversarial data augmentation, and proposed a mutual information regularizer to promote the cross-domain semantic consistency. proposed a causalty-inspired data augmentation method to simulate different possible MRI imaging processes for domain-invariant segmentation networks. Domain generalization for medical image segmentation:
 Learning-based approaches focus on improving the learning strategy. adapted meta-learning for generalization, and introduced two complementary losses to encourage shape compactness and shape smoothness. further considered the boundary of prediction masks, proposing a shapeaware meta-learning scheme SAML for prostate MRI segmentation. Domain generalization for medical image segmentation:
 explicitly modeled and disentangled the representations to better approximate the domain shifts for meta-learning. Besides,  proposed an episodic learning strategy in continuous frequency space, which balances and fuses the information from multi-source domains to build a general model. Domain generalization for medical image segmentation:
 Representation-based approaches are devoted to extracting domain-invariant or domain-irrelevant features for the model to make decisions. Specifically, feature alignment methods aim to narrow the cross-domain feature distribution gap to make features domain-invariant. ;  adapted adversarial learning to align the distributions among different domains. To explicitly minimizing the distribution distance. Domain generalization for medical image segmentation:
 also imposed the Maximum Mean Discrepancy (MMD) measure, and  proposed an entropy regularization term for conditional distributions. Feature disentanglement methods aim to decompose a feature representation into domain-specific parts and domain-shared/irrelevant parts. Domain generalization for medical image segmentation:
 For instance,  disentangled the image features into domain-related, class-related, and residual-related variables for classification, and learned those independent latent subspaces through a domain-invariant variational auto-encoder (DIVA). Recently, causality-inspired models  focus on the causal relationship between latent features and outputs to find the most important features that cause the results. Domain generalization for medical image segmentation:
 Although these representation-based methods have achieved remarkable progress in many computer vision tasks, there are relatively few of them targeting medical image segmentation . Moreover, the interpretability of the domaininvariant features themselves remains a great challenge to promote. Domain generalization for medical image segmentation:
 To further enhance the interpretability of features, we combine statistical modeling with image decomposition and achieve generalizable segmentation through a Bayesian framework. Unlike DSU  that models domain shifts with the uncertain feature statistics for data augmentation, we aim to extract the shape information that represents the domain-invariant organ structures."
"Transductive transfer learning for visual recognition","https://scispace.com/paper/transductive-transfer-learning-for-visual-recognition-13tzx0na","2023","Dissertation","","Matteo Croci","10.32657/10356/164573","https://scispace.com/pdf/transductive-transfer-learning-for-visual-recognition-13tzx0na.pdf","","lie in exploring transductive transfer learning for visual recognition from different perspectives including domain generalization, unsupervised domain adaptation and source-free unsupervised domain adaptation, as illustrated in Fig. .1. We present more particulars in the consequent contents. Major Contributions:
 Learning general and transferrable models with labeled source domains (only source domain data are available in training) Domain Generalization:
 Transferring models from labeled source domains to unlabeled target domains (both source and target domains data are available in training) Unsupervised Domain Adaptation:
 Transferring source-trained models to unlabeled target domains (only target domain data are available in training) Source-free Unsupervised Domain Adaptation:
 Data: Source-domain and target-domain data distributions are different; Source-domain data is labeled while target-domain data is unlabeled. Task: Source and target tasks are the same. Domain Generalization:
 The objective of domain generalization is to explore transductive transfer learning by learning general and transferable features, which can generalize and work well on various unseen target domains. In other words, with domain generalization techniques, we can train general and domain-invariant models with labeled source domains only, where such general models can benefit other types of transductive transfer learning (e.g., unsupervised domain adaptation and source-free unsupervised domain adaptation) by serving as strong ""initialization models"" (or called source-only models or baseline models). We propose a novel domain randomization method for achieving effective domain generalization. Unsupervised Domain Adaptation:
 In the unsupervised domain adaptation setting, both labeled source domain data and unlabeled target domain data are available in training, where the major objective is to explore unlabeled target domain data conditioned on the source-domain knowledge for improving the transductive transfer learning. To this end, we investigate four innovative unsupervised domain adaptation methods to achieve better transfer learning performance, including three methods that propose three types of unsupervised learning objectives to explore unlabeled target domain data and one 1.2. Major Contributions method that introduces a regularization approach to regularize these unsupervised learning objectives. By considering instance contrastive learning as a dictionary look-up operation, we construct a semantics-aware dictionary with samples from both source and target domains where each target sample is assigned a (pseudo) category label based on the category priors of source samples. This allows category contrastive learning (between target queries and the category-level dictionary) for category-discriminative yet domain-invariant feature representations: samples of the same category (from either source or target domain) are pulled closer while those of different categories are pushed apart simultaneously. Extensive experiments in multiple visual tasks show that CaCo achieves superior performance against various state-of-the-arts."
"Cross-domain Collaborative Learning for Recognizing Multiple Retinal
  Diseases from Wide-Field Fundus Images","https://scispace.com/paper/cross-domain-collaborative-learning-for-recognizing-multiple-1ilqz87l","2023","Posted Content","","","10.48550/arxiv.2305.08078","https://scispace.com/pdf/cross-domain-collaborative-learning-for-recognizing-multiple-1ilqz87l.pdf","This paper addresses the emerging task of recognizing multiple retinal diseases from wide-field (WF) and ultra-wide-field (UWF) fundus images. For an effective reuse of existing labeled color fundus photo (CFP) data, we propose Cross-domain Collaborative Learning (CdCL). Inspired by the success of fixed-ratio based mixup in unsupervised domain adaptation, we re-purpose this strategy for the current task. Due to the intrinsic disparity between the field-of-view of CFP and WF/UWF images, a scale bias naturally exists in a mixup sample that the anatomic structure from a CFP image will be considerably larger than its WF/UWF counterpart. The CdCL method resolves the issue by Scale-bias Correction, which employs Transformers for producing scale-invariant features. As demonstrated by extensive experiments on multiple datasets covering both WF and UWF images, the proposed method compares favorably against a number of competitive baselines. ","Title: Cross-domain Collaborative Learning for Recognizing Multiple Retinal Diseases from Wide-Field Fundus Images Authors: Qijie Wei,Jingyuan Yang,Bo Wang,Jinrui Wang,Jianchun Zhao,Xinyu Zhao,Sheng Yang,Niranchana Manivannan,Youxin Chen,Dayong Ding,Xirong Li (Corresponding Author)"
"CDDSA: Contrastive Domain Disentanglement and Style Augmentation for
  Generalizable Medical Image Segmentation","https://scispace.com/paper/cddsa-contrastive-domain-disentanglement-and-style-3rbiutq0","2022","Posted Content","","","10.48550/arxiv.2211.12081","https://scispace.com/pdf/cddsa-contrastive-domain-disentanglement-and-style-3rbiutq0.pdf","Generalization to previously unseen images with potential domain shifts and different styles is essential for clinically applicable medical image segmentation, and the ability to disentangle domain-specific and domain-invariant features is key for achieving Domain Generalization (DG). However, existing DG methods can hardly achieve effective disentanglement to get high generalizability. To deal with this problem, we propose an efficient Contrastive Domain Disentanglement and Style Augmentation (CDDSA) framework for generalizable medical image segmentation. First, a disentangle network is proposed to decompose an image into a domain-invariant anatomical representation and a domain-specific style code, where the former is sent to a segmentation model that is not affected by the domain shift, and the disentangle network is regularized by a decoder that combines the anatomical and style codes to reconstruct the input image. Second, to achieve better disentanglement, a contrastive loss is proposed to encourage the style codes from the same domain and different domains to be compact and divergent, respectively. Thirdly, to further improve generalizability, we propose a style augmentation method based on the disentanglement representation to synthesize images in various unseen styles with shared anatomical structures. Our method was validated on a public multi-site fundus image dataset for optic cup and disc segmentation and an in-house multi-site Nasopharyngeal Carcinoma Magnetic Resonance Image (NPC-MRI) dataset for nasopharynx Gross Tumor Volume (GTVnx) segmentation. Experimental results showed that the proposed CDDSA achieved remarkable generalizability across different domains, and it outperformed several state-of-the-art methods in domain-generalizable segmentation. ","meta-learning with federated learning to achieve privacy-preserving generalizable segmentation through continuous frequency space interpolation across clients. However, meta-optimization process is highly time-consuming since all potential splitting results of metatrain and meta-test should be considered during training . Domain Generalization for Medical Image Analysis:
 Data-based approaches usually use different data augmentation strategies for improving the model's generalizability.  a deep stacked transformation assuming that the shift between different domains can be simulated by extensive data augmentation on a single domain.  utilized Cycle-GAN  to transform images from one certain domain to other domains for augmentation.  proposed Mixed Task Sampling (MTS) to enhance the variety of task-level training samples. Mixup in frequency domains  has also been used to synthesize new images for model generalization. However, the efficiency of data augmentation largely depends on the ability to cover the data distribution in unseen domains, hence requiring empirical settings and even data-specific modifications. Domain Generalization for Medical Image Analysis:
 Feature-based approaches use domain-adaptive feature calibration or learn domain-invariant features to deal with domain generalization .  introduced a domainoriented feature embedding framework that dynamically updates the domain-specific prior knowledge to make the semantic features more discriminative.  proposed a dynamic convolutional head to make the model's convolutional parameters adaptive to unseen target domains.  proposed a domain composition and attention method that calibrates the input feature based on attention coefficients represented by a representation bank. However, these methods did not explicitly obtain domain-invariant features for domain generalization, and they did not separate features into purely domain-specific and domain-invariant representations well, leading to limited performance on domain generalization. Disentanglement Representation Learning:
 Disentanglement explicitly decomposes features into domain-invariant contents and domain-specific styles . In addition to applications such as image synthesis , artifact removal and multi-task learning  for medical image analysis, it is widely adopted for domain adaptation .  used disentanglement to obtain domain-invariant content features for liver segmentation with domain adaptation.  used disentanglement to improve the performance of image translation for domain adaptation, and they disentangled the content features from domain information for both the source and translated images.  applied disentanglement-based domain adaptation for cardiac image segmentation, and introduced a zero loss to enhance disentanglement.  proposed a bidirectional unsupervised DA framework based on disentangled representation learning for equally competent two-way DA performances on cardiac image segmentation. Despite their good performance on DA, theses works achieve disentanglement based on GAN, where multiple discriminators are needed in the adversarial training process that is complex and tricky to optimize. What's more, they need to have access to images for target domains during training, and are not applicable to DG tasks that involves unseen domains.  proposed a GAN-free Spatial Decomposition Network (SDNet) that decomposes an input image into a spatial factor (anatomy) and a non-spatial factor (style), and applied it to semi-supervised segmentation and image synthesis. However, it performs disentanglement and reconstruction well only on seen domains can hardly deal with unseen domains that are not involved in training."
"Domain Transfer Through Image-to-Image Translation for Uncertainty-Aware
  Prostate Cancer Classification","https://scispace.com/paper/domain-transfer-through-image-to-image-translation-for-3j52f3av","2023","Posted Content","","","10.48550/arxiv.2307.00479","https://scispace.com/pdf/domain-transfer-through-image-to-image-translation-for-3j52f3av.pdf","Prostate Cancer (PCa) is often diagnosed using High-resolution 3.0 Tesla(T) MRI, which has been widely established in clinics. However, there are still many medical centers that use 1.5T MRI units in the actual diagnostic process of PCa. In the past few years, deep learning-based models have been proven to be efficient on the PCa classification task and can be successfully used to support radiologists during the diagnostic process. However, training such models often requires a vast amount of data, and sometimes it is unobtainable in practice. Additionally, multi-source MRIs can pose challenges due to cross-domain distribution differences. In this paper, we have presented a novel approach for unpaired image-to-image translation of prostate mp-MRI for classifying clinically significant PCa, to be applied in data-constrained settings. First, we introduce domain transfer, a novel pipeline to translate unpaired 3.0T multi-parametric prostate MRIs to 1.5T, to increase the number of training data. Second, we estimate the uncertainty of our models through an evidential deep learning approach; and leverage the dataset filtering technique during the training process. Furthermore, we introduce a simple, yet efficient Evidential Focal Loss that incorporates the focal loss with evidential uncertainty to train our model. Our experiments demonstrate that the proposed method significantly improves the Area Under ROC Curve (AUC) by over 20% compared to the previous work (98.4% vs. 76.2%). We envision that providing prediction uncertainty to radiologists may help them focus more on uncertain cases and thus expedite the diagnostic process effectively. Our code is available at https://github.com/med-i-lab/DT_UE_PCa ","to be similar to the distribution of s. The ACL-GAN can retain important features from the source images and overcomes the disadvantage of the cycle-consistency constraint. Therefore, we adapt the ACL-GAN model and build our framework based on it. Domain Adaptation:
 In medical imaging, domain shift problems usually fall into two variations: subjectrelated variation (age, gender, etc.), and acquisition-related variation (MRI vendor, field strength, imaging protocol, etc.) . To solve the problem, one intuitive approach is to fine-tune a model that is pre-trained on the source domain with the new data from the target domain.  propose to use the pre-trained VGG model on the ImageNet dataset  to learn robust high-level features of natural images, and then fine-tune it on the labeled MR images for the Alzheimer's Disease (AD) classification task to achieve state-of-the-art performance. Similarly, Ghafoorian et al. ( ) study the impact of the fine-tuning techniques on the brain lesion segmentation task, demonstrating that fine-tuning with only a small number of target domain training samples can outperform models trained from scratch. Another approach is to use domain adaptation as an intermediate step to reduce variance in image acquisition parameters from both domains and then use it for downstream tasks. Researchers have attempted to address the problem of acquisition variation in MRI data for several years.  propose a feature-level representation learning method to either extract acquisition-invariant features or remove acquisition-variant features from paired 1.5T and 3.0T brain MRIs. The learned features are then used for a downstream classification task. However, obtaining paired 1.5T and 3.0T MRI data in real-life scenarios is impractical. Another way to align acquisitioninvariant features is to synthesize images from different types of acquisition parameters using GAN-based adversarial reconstruction methods. GANs have been applied to perform crossmodality image translation between different medical images or generate synthetic images from random noise. The objective of such translation tasks is to retain the underlying structure while changing the appearance of the image . Researchers have attempted to estimate images in the target modality from the source modality, such as MRI-CT translation , and X-ray to CT translation . Other areas that have been explored include intra-modality translation, such as 3.0T-7.0T MRI translation , T1/T2-FLAIR translation  and pure data augmentation by generating synthetic images from random noise vectors . However, most of the works do not consider the real clinical practicality, for example, for 3.0T-7.0T MRI translation in , the training data is paired, which is not feasible in real clinical settings. Generating synthetic images from noise does not take advantage of the publicly available data and ignores a-priori information. The current limitations provide great potential for unpaired image translation for medical images, which we employ in this work."
"Generalization in Neural Networks: A Broad Survey","https://scispace.com/paper/generalization-in-neural-networks-a-broad-survey-2enb5iok","2022","Posted Content","","Simone Lombardo","10.48550/arxiv.2209.01610","https://scispace.com/pdf/generalization-in-neural-networks-a-broad-survey-2enb5iok.pdf","This paper reviews concepts, modeling approaches, and recent findings along a spectrum of different levels of abstraction of neural network models including generalization across (1) Samples, (2) Distributions, (3) Domains, (4) Tasks, (5) Modalities, and (6) Scopes. Results on (1) sample generalization show that, in the case of ImageNet, nearly all the recent improvements reduced training error while overfitting stayed flat; with nearly all the training error eliminated, future progress will require a focus on reducing overfitting. Perspectives from statistics highlight how (2) distribution generalization can be viewed alternately as a change in sample weights or a change in the input-output relationship; thus, techniques that have been successful in domain generalization have the potential to be applied to difficult forms of sample or distribution generalization. Transfer learning approaches to (3) domain generalization are summarized, as are recent advances and the wealth of domain adaptation benchmark datasets available. Recent breakthroughs surveyed in (4) task generalization include few-shot meta-learning approaches and the BERT NLP engine, and recent (5) modality generalization studies are discussed that integrate image and text data and that apply a biologically-inspired network across olfactory, visual, and auditory modalities. Recent (6) scope generalization results are reviewed that embed knowledge graphs into deep NLP approaches. Additionally, concepts from neuroscience are discussed on the modular architecture of brains and the steps by which dopamine-driven conditioning leads to abstract thinking. ","commonalities among sample, distribution, and domain generalization are suggestive of possible research directions. Researchers have found success applying different forms of transfer learning to problems of domain adaptation-such approaches could have potential for addressing sample and distribution generalization as well. Introduction:
 Section 4 describes key elements of transfer learning approaches for domain generalization, including feature embeddings and parameter sharing, recent progress in the area, and the diversity of problems and datasets available for learning adaptation across domains. The Photo-Art-Cartoon-Sketch (PACS) visual classification exercise-in which the network is trained to recognize the same objects in different visual vernaculars-is used as a motivating example. Section 5 describes exciting recent developments in the learning of task generalization, including few-shot meta-learning strategies, through which a network learns to recognize new classes from only a few examples, as well as the Bidirectional Encoder Representations from Transformers (BERT), a new and disruptive innovation in the Natural Language Processing (NLP) sphere. Introduction:
 Generalization across modalities remains a part of the field with substantial room for growth, and section 6 describes some recent innovative studies in the literature that generalize models across widely different problems and data types. Section 7 on generalization across scopes reviews knowledge graphs and their use to facilitate networkbased semantic understanding. Next, Section 8 reviews neuroscience research on the modular and dopamine-driven means by which abstraction occurs in animal brains. This biological perspective provides insights into modular elements of existing artificial neural networks as well as potential directions in which artificial systems might generalize more effectively. Section 9 concludes. Sample Generalization:
 One key variant of generalization that is used with nearly all deep learning models is the application of a trained model to new test cases. Across many areas of empirical research, models are known to forecast more accurately among in-sample observations used in the fitting process than on previously unseen out-of-sample observations, and the issue is particularly severe among complex models with large numbers of estimated parameters. This phenomenon is illustrated in Figure , which is modified slightly from . Training error, shown in blue, declines steadily with the number of model parameters, while test error, shown in red, declines at first but then rises at higher levels of complexity; hence, a model that is selected to minimize in-sample error is likely to be overly complex relative to a model that minimizes the error from out-ofsample forecasting."
"Why does my medical AI look at pictures of birds? Exploring the efficacy of transfer learning across domain boundaries","https://scispace.com/paper/why-does-my-medical-ai-look-at-pictures-of-birds-exploring-3du78kiq","2023","Journal Article","arXiv.org","F. Jonske
Moon Hyun Kim
Enrico Nasca
Jan Felix Evers
Johannes Haubold
René Hosch
Felix Nensa
Michael Kamp
Constantin Seibold
Jan Egger
Jens Kleesiek","10.48550/arXiv.2306.17555","https://scispace.compdf/why-does-my-medical-ai-look-at-pictures-of-birds-exploring-3du78kiq.pdf","It is an open secret that ImageNet is treated as the panacea of pretraining. Particularly in medical machine learning, models not trained from scratch are often finetuned based on ImageNet-pretrained models. We posit that pretraining on data from the domain of the downstream task should almost always be preferred instead. We leverage RadNet-12M, a dataset containing more than 12 million computed tomography (CT) image slices, to explore the efficacy of self-supervised pretraining on medical and natural images. Our experiments cover intra- and cross-domain transfer scenarios, varying data scales, finetuning vs. linear evaluation, and feature space analysis. We observe that intra-domain transfer compares favorably to cross-domain transfer, achieving comparable or improved performance (0.44% - 2.07% performance increase using RadNet pretraining, depending on the experiment) and demonstrate the existence of a domain boundary-related generalization gap and domain-specific learned features.","facto method for medical image classification problems, Raghu et al. observed across a range of different model architectures and medical imaging tasks, that pretraining with ImageNet yielded no significant performance boost and that meaningful feature reuse did not always occur after pretraining . Introduction:
 Newell & Deng experimentally probed the efficacy of self-supervised training using synthetic data and various pretraining algorithms . They observed an anti-proportional correlation between pretraining utility and the number fine-tuning images and noted that in some cases, pretraining offered only accelerated convergence or even no performance gain. Introduction:
 In a study on cross-domain generalization ,  found that many of their models could perform well after fine-tuning on the same downstream task, yet disagree in their predictions. Moreover, models that were in strong agreement nonetheless performed poorly, implying that generalization to other tasks is often a nuanced process. Introduction:
 A study by Gururangan et al.  explored intra-domain transfer learning on Natural Language Processing (NLP) tasks, using multi-phased pretraining on different datasets. They found that spending at least some time training on data from the domain of the downstream task led to consistent performance gains. Introduction:
 Based on these previous findings and our initial lines of reasoning, we formulate the following hypotheses: I) When applying transfer learning, any difference between the pretraining and fine-tuning task domains produces a generalization gap . Introduction:
 This occurs because the two underlying data distributions, and consequently learned feature subsets, are likely to be different at least to some degree , . Intuitively, we expect this gap to shrink or disappear in an intra-domain transfer learning scenario, since the data distributions are highly similar or identical. Introduction:
 In essence, we propose that ""there is no better data to train medical questions on than medical data"". II) The generalization gap effect is not symmetrical. A model trained on a specific pretraining dataset may learn a superset of the features it would learn from a different dataset. Introduction:
 While this is most likely not immediately true for every downstream task, it may well be true for a significant fraction of them. We posit that ""some pretraining datasets are principally more useful than others''. Introduction:
 Investigating these hypotheses not only provides fundamental insights into the nature of finetuning, domain adaptation and transfer learning, but will, more importantly, yield practical strategies to improve our medical AI models. Our main contributions are as follows::
 • To validate these hypotheses, we leverage RadNet-1.28M and RadNet-12M, two pretraining datasets comprising over 12 million CT image slices from our local hospital, the largest dataset on which this kind of analysis has been performed to date. Our main contributions are as follows::
 We also leverage four publicly available datasets from the natural and medical image domain. • We design an experimental setup (cf. Fig.  and the Methods section) to compare unsupervised pretraining efficacy depending on the fine-tuning dataset domain. Our main contributions are as follows::
 We additionally extend previous work by exploring the dependency of the downstream performance on task complexity, and in a linear evaluation vs fine-tuning scenario. • We present our results and uncertainty estimations, discuss them with respect to our hypotheses, and perform novel feature space analysis for our models. Our main contributions are as follows::
 Here, we demonstrate the existence of unique, explainable image features learned only during intra-domain pretraining that are preserved during fine-tuning."
"Using Transfer Learning for Diabetic Retinopathy Stages Classification","https://scispace.com/paper/using-transfer-learning-for-diabetic-retinopathy-stages-5tn79ax67i","2021","Posted Content","Applied Computing and Informatics","Enas M.F. El Houby","10.21203/RS.3.RS-384006/V1","https://www.researchsquare.com/article/rs-384006/v1.pdf","","Title: Using Transfer Learning for Diabetic Retinopathy Stages Classi cation Authors: Enas M F El Houby (Corresponding Author) Keywords: Deep learning, Diabetic retinopathy, Machine Learning Techniques, Retinal fundus images, Transfer learning"
"Fourier Test-time Adaptation with Multi-level Consistency for Robust
  Classification","https://scispace.com/paper/fourier-test-time-adaptation-with-multi-level-consistency-14nb4euh","2023","Posted Content","","","10.48550/arxiv.2306.02544","https://scispace.com/pdf/fourier-test-time-adaptation-with-multi-level-consistency-14nb4euh.pdf","Deep classifiers may encounter significant performance degradation when processing unseen testing data from varying centers, vendors, and protocols. Ensuring the robustness of deep models against these domain shifts is crucial for their widespread clinical application. In this study, we propose a novel approach called Fourier Test-time Adaptation (FTTA), which employs a dual-adaptation design to integrate input and model tuning, thereby jointly improving the model robustness. The main idea of FTTA is to build a reliable multi-level consistency measurement of paired inputs for achieving self-correction of prediction. Our contribution is two-fold. First, we encourage consistency in global features and local attention maps between the two transformed images of the same input. Here, the transformation refers to Fourier-based input adaptation, which can transfer one unseen image into source style to reduce the domain gap. Furthermore, we leverage style-interpolated images to enhance the global and local features with learnable parameters, which can smooth the consistency measurement and accelerate convergence. Second, we introduce a regularization technique that utilizes style interpolation consistency in the frequency space to encourage self-consistency in the logit space of the model output. This regularization provides strong self-supervised signals for robustness enhancement. FTTA was extensively validated on three large classification datasets with different modalities and organs. Experimental results show that FTTA is general and outperforms other strong state-of-the-art methods. ","Yang,Xiaoqiong Huang,Xinrui Zhou,Haozhe Chi,Haoran Dou,Xindi Hu,Jian Wang,Xuedong Deng,Dong Ni (Corresponding Author) Keywords: Classifier robustness, Testing-time adaptation, Consistency Introduction:
 Domain shift (see Fig. ) may cause deep classifiers to struggle in making plausible predictions during testing . This risk seriously limits the reliable deployment of these deep models in real-world scenarios, especially for clinical analysis. Collecting data from the target domain to retrain from scratch or fine-tune the trained model is the potential solution to handle the domain shift risks. However, obtaining adequate testing images with manual annotations is laborious and impracticable in clinical practice. Thus, different solutions have been proposed to conquer the problem and improve the model robustness. Introduction:
 Fig. . From left to right: 1) four-chamber views of heart from Vendor A&B, 2) abdomen planes from Vendor C&D, 3) fundus images with diabetic retinopathy of grade 3 from Center E-G. Appearance and distribution differences can be seen in each group. Introduction:
 Unsupervised Domain Adaptation (UDA) refers to training the model with labeled source data and adapting it with target data without annotation . Recently, Fourier domain adaptation was proposed in , with the core idea of achieving domain transfer by replacing the low-frequency spectrum of source data with that of the target one. Although effective, they require obtaining sufficient target data in advance, which is challenging for clinical practice. Introduction:
 Domain generalization (DG) aims to generalize models to the unseen domain not presented during training. Adversarial learning-based DG is one of the most popular choices that require multi-domain information for learning domain-invariant representations . Recently, Liu et al.  proposed to construct a continuous frequency space to enhance the connection between different domains. Atwany et al.  imposed a regularization to reduce gradient variance from different domains for diabetic retinopathy classification. One drawback is that they require multiple types of source data for extracting rich features. Other alternatives proposed using only one source domain to perform DG . However, they still heavily rely on simulating new domains via various data augmentations, which can be challenging to control. Introduction:
 Test-time Adaptation (TTA) adapts the target data or pre-trained models during testing . Test-time Training (TTT)  and TTT++  proposed to minimize a self-supervised auxiliary loss. Wang et al.  proposed the TENT framework that focused on minimizing the entropy of its predictions by modulating features via normalization statistics and transformation parameters estimation. Instead of batch input like the above-mentioned methods, Single Image TTA (SITA)  was proposed with the definition that having access to only one given test image once. Recently, different mechanisms were developed to optimize the TTA including distribution calibration , dynamic learning rate , and normalizing flow . Most recently, Gao et al.  proposed projecting the test image back to the source via the source-trained diffusion models. Although effective, these methods often suffer from the problems of unstable parameter estimation, inaccurate proxy tasks/pseudo labels, difficult training, etc. Thus, a simple yet flexible approach is highly desired to fully mine and combine information from test data for online adaptation."
"Resource-efficient domain adaptive pre-training for medical images","https://scispace.com/paper/resource-efficient-domain-adaptive-pre-training-for-medical-1i3lxnn8","2022","Posted Content","","","10.48550/arxiv.2204.13280","https://scispace.com/pdf/resource-efficient-domain-adaptive-pre-training-for-medical-1i3lxnn8.pdf","The deep learning-based analysis of medical images suffers from data scarcity because of high annotation costs and privacy concerns. Researchers in this domain have used transfer learning to avoid overfitting when using complex architectures. However, the domain differences between pre-training and downstream data hamper the performance of the downstream task. Some recent studies have successfully used domain-adaptive pre-training (DAPT) to address this issue. In DAPT, models are initialized with the generic dataset pre-trained weights, and further pre-training is performed using a moderately sized in-domain dataset (medical images). Although this technique achieved good results for the downstream tasks in terms of accuracy and robustness, it is computationally expensive even when the datasets for DAPT are moderately sized. These compute-intensive techniques and models impact the environment negatively and create an uneven playing field for researchers with limited resources. This study proposed computationally efficient DAPT without compromising the downstream accuracy and robustness. This study proposes three techniques for this purpose, where the first (partial DAPT) performs DAPT on a subset of layers. The second one adopts a hybrid strategy (hybrid DAPT) by performing partial DAPT for a few epochs and then full DAPT for the remaining epochs. The third technique performs DAPT on simplified variants of the base architecture. The results showed that compared to the standard DAPT (full DAPT), the hybrid DAPT technique achieved better performance on the development and external datasets. In contrast, simplified architectures (after DAPT) achieved the best robustness while achieving modest performance on the development dataset . ","Title: Resource-efficient domain adaptive pre-training for medical images Authors Authors: Yasar Mehmood,Xianfang Sun Keywords: Computational efficiency, Domain adaptation, Medical images, Transfer learning"
"Diabetic retinopathy identification system based on transfer learning","https://scispace.com/paper/diabetic-retinopathy-identification-system-based-on-transfer-57ylnopoui","2020","Journal Article","","Lingling Li
Xueqi Yan
Heng Peng
Ying Xiu
Yiyang Gao
Xin Wang","10.1088/1742-6596/1544/1/012133","https://scispace.com/pdf/diabetic-retinopathy-identification-system-based-on-transfer-57ylnopoui.pdf","Diabetic retinopathy, a complication of diabetes, is a significant cause of vision loss and blindness. Early detection of diabetic retinopathy can help reduce the risk of blinding. However, automatic diabetic retinopathy identification is a challenging task due to their different morphology during a different stage. Aiming at the problem of low efficiency for most of the existing methods, we developed a diabetic retinopathy recognition system based on transfer learning. The system utilizes transfer learning, which trains a neural network based on the DenseNet201 network model and Messidor Data Set, and can not only train an active network quickly, but also has a reasonable effect on the classification of diabetic retinopathy.","Title: Diabetic retinopathy identification system based on transfer learning Authors: Lingling Li,Xueqi Yan,Heng Peng,Ying Xiu,Yiyang Gao,Xin Wang Introduction:
 Diabetic retinopathy (DR) is one of the most critical manifestations of microangiopathy lesions in diabetes. DR stages were classified as non-DR, nonproliferative DR (NPDR), or proliferative DR (PDR) . DR is a systemic disease, which affects up to 80 percent of all patients who have had diabetes for 20 years or more. Despite these intimidating statistics, research indicates that at least 90% of these new cases could be reduced if there were proper and vigilant treatment and monitoring of the eyes . Introduction:
 The quality of retina images is essential for the diagnosis and treatment of eye diseases. Blur, distortion, low contrast, among other artifacts, inhibit the viewing of regions of interest , and the subtle features in the image that are difficult to distinguish by the naked eye are obstacles to the actual diagnosis of diabetic retinopathy. At the same time, classification accuracy is hugely dependent on the clinical experience of the physician. Introduction:
 The rapid development of deep learning today is expected to change this situation. Deep learning has been frequently used for image recognition and classification, and many developed algorithms have reached expert-level accuracy in DR recognition. In 2016, Gulshan et al.  and others used the Inception-V3 network to train on a dataset of 128,175 fundus images, eventually achieved a sensitivity of 97.5% on the three classification tasks of DR. The following year, Gargeya and Leng  obtained an AUC score of 97% on 75,137 color fundus images in Kaggle Data Set through 5x cross-validation. Introduction:
 Our work has the following contributions: Introduction:
 • We proposed a neural network model based on transfer learning to classify diabetic retinopathy. •:
 We utilized data pre-processing to improve the accuracy of our network. •:
 By comparing three methods commonly used in transfer learning, we find that the most suitable method for diabetic retinopathy grade recognition is to freeze the bottleneck layer and retrain the fullconnection layer. Assisted diagnosis of fundus images::
 Various machine learning algorithms are now used to develop high-performance medical image processing systems such as computer-aided detection (CADe) system . The application of the CAD system can significantly reduce the pressure on doctors to diagnose medical pictures and improve work efficiency. The most important thing is to reduce the influence of personal subjective factors on doctors' diagnoses and improve the accuracy of clinical diagnosis. Therefore, we have developed this system to assist doctors in diagnosing diabetic retinopathy. Transfer learning:
 Transfer learning has achieved promising results by leveraging knowledge from the source domain to annotate the target domain, which has few or none labels . Therefore, with the limited amount of data obtained, we chose transfer learning technology to develop a Diabetic retinopathy identification system. Figure  is a schematic diagram of transfer learning used in this article. The convolution layer is frozen and transferred to a new network. In contrast, the full-connection layer is recreated and randomly initialized retraining, which significantly reduces the training duration and improves the accuracy of the network trained in the Messidor Data Set. Data Pre-processing:
 Before implementing transfer learning, we need to do some pre-processing on the original image."
"DomainATM: Domain Adaptation Toolbox for Medical Data Analysis","https://scispace.com/paper/domainatm-domain-adaptation-toolbox-for-medical-data-1j86s8kx","2022","Posted Content","","","10.48550/arxiv.2209.11890","https://scispace.com/pdf/domainatm-domain-adaptation-toolbox-for-medical-data-1j86s8kx.pdf","Domain adaptation (DA) is an important technique for modern machine learning-based medical data analysis, which aims at reducing distribution differences between different medical datasets. A proper domain adaptation method can significantly enhance the statistical power by pooling data acquired from multiple sites/centers. To this end, we have developed the Domain Adaptation Toolbox for Medical data analysis (DomainATM) - an open-source software package designed for fast facilitation and easy customization of domain adaptation methods for medical data analysis. The DomainATM is implemented in MATLAB with a user-friendly graphical interface, and it consists of a collection of popular data adaptation algorithms that have been extensively applied to medical image analysis and computer vision. With DomainATM, researchers are able to facilitate fast feature-level and image-level adaptation, visualization and performance evaluation of different adaptation methods for medical data analysis. More importantly, the DomainATM enables the users to develop and test their own adaptation methods through scripting, greatly enhancing its utility and extensibility. An overview characteristic and usage of DomainATM is presented and illustrated with three example experiments, demonstrating its effectiveness, simplicity, and flexibility. The software, source code, and manual are available online. ","Title: DomainATM: Domain Adaptation Toolbox for Medical Data Analysis Authors: Hao Guan,Mingxia Liu (Corresponding Author) Keywords: Domain adaptation, medical image analysis, medical image processing toolbox, open source software I. INTRODUCTION:
 M EDICAL data analysis is nowadays being boosted by modern statistical analysis tools, i.e., machine learning - . Classic machine learning typically assumes that training dataset (source domain) and test dataset (target domain) follow an independent but identical distribution . In real-world practice, however, this assumption can hardly hold due to the well-known ""domain shift"" problem - . In medical imaging, domain shift or data heterogeneity is widespread and caused by different scanning parameters (i.e., between-scanner variability) and subject populations in multiple imaging sites. It may increase the test error along with the distribution difference between training and test data , . Thus the domain shift/difference may greatly degrade statistical power of multi-site/multi-center studies and hinder the building of effective machine learning models. I. INTRODUCTION:
 For handling the domain shift problem among datasets and enhancing the generalization ability of machine learning models, domain adaptation has gradually come under the spotlight of the research community - . In the Fig. . Illustration of the ""domain shift"" phenomenon  (top row) and the fundamental of domain adaptation (distribution of source and target samples before and after adaptation). field of medical data analysis, domain adaptation has gained considerable attention and increasing interest recently , . Briefly, domain adaptation can be defined as follows. Let X × Y represent the joint feature space of samples and their corresponding category labels. A source domain S and a target domain T are defined on the joint feature space, with different distributions P S and P T , respectively. Suppose there are n s samples (subjects) with or without category labels in the source domain, as well as n t samples in the target domain without category labels. Then the problem is how to reduce the distribution differences/variability between source and target domains so as to increase the performance of down-streaming tasks such as classification or segmentation. I. INTRODUCTION:
 Many domain adaptation methods have been proposed or utilized in the field of medical data analysis which shows tremendous applicability. Most solutions, however, are implemented independently for very specific scenarios or target applications. Researchers often need to re-implement an algorithm or do methodological tailoring. The differences in implementation will often cause inconsistent experiment and analysis results. There is a lack of a unified platform for extensive comparison of different domain adaptation methods, helping avoiding hand-crafted re-implementation for specific medical data analysis research. Thus a software toolbox that provides a platform of different adaptation methods is quite beneficial and necessary for researchers to compare, evaluate and select the proper method for their research project."
"Assessment of transfer learning models for grading of diabetic retinopathy","https://scispace.com/paper/assessment-of-transfer-learning-models-for-grading-of-2rwy4wwj","2023","Journal Article","THE SCIENTIFIC TEMPER","","10.58414/scientifictemper.2023.14.2.17","https://scispace.com/pdf/assessment-of-transfer-learning-models-for-grading-of-2rwy4wwj.pdf","Diabetic retinopathy is a potentially mortal diabetic complication. The severity level of DR must be identified earlier to reduce the medical complications. Effective automated ways for identifying DR and classifying its severity stage are necessary to reduce the burden on ophthalmologists. Transfer learning methods are utilized to automatically grade the  severity of diabetic retinopathy in this study. The stages of DR are diagnosed using pretrained VGG16, Inception v3, and ResNet50 models on pre-processed retinal images of DDR dataset. Out of three implemented models, Inception v3 achieved higher validation accuracy of 76.47% and testing accuracy of 67% compared to VGG16 and ResNet50 models. This research contributes to the analysis of deep learning architectures for the creation of automated diabetic retinopathy stage diagnosis and grading","Title: Assessment of transfer learning models for grading of diabetic retinopathy Authors: Banu Rekha (Corresponding Author) Keywords: Transfer learning, retinal image, diabetic retinopathy, VGG16, Inception v3, ResNet50"
"Pseudo Supervised Metrics: Evaluating Unsupervised Image to Image
  Translation Models In Unsupervised Cross-Domain Classification Frameworks","https://scispace.com/paper/pseudo-supervised-metrics-evaluating-unsupervised-image-to-ks6meoo3","2023","Posted Content","","","10.48550/arxiv.2303.10310","https://scispace.com/pdf/pseudo-supervised-metrics-evaluating-unsupervised-image-to-ks6meoo3.pdf","The ability to classify images accurately and efficiently is dependent on having access to large labeled datasets and testing on data from the same domain that the model is trained on. Classification becomes more challenging when dealing with new data from a different domain, where collecting a large labeled dataset and training a new classifier from scratch is time-consuming, expensive, and sometimes infeasible or impossible. Cross-domain classification frameworks were developed to handle this data domain shift problem by utilizing unsupervised image-to-image (UI2I) translation models to translate an input image from the unlabeled domain to the labeled domain. The problem with these unsupervised models lies in their unsupervised nature. For lack of annotations, it is not possible to use the traditional supervised metrics to evaluate these translation models to pick the best-saved checkpoint model. In this paper, we introduce a new method called Pseudo Supervised Metrics that was designed specifically to support cross-domain classification applications contrary to other typically used metrics such as the FID which was designed to evaluate the model in terms of the quality of the generated image from a human-eye perspective. We show that our metric not only outperforms unsupervised metrics such as the FID, but is also highly correlated with the true supervised metrics, robust, and explainable. Furthermore, we demonstrate that it can be used as a standard metric for future research in this field by applying it to a critical real-world problem (the boiling crisis problem). ","domain to the target domain by inputting the image into the model and generating a synthetic output image in the target domain. This translation capability inspired a number of approaches that leverage GANs and UI2I models to address the cross-domain classification problem. Introduction:
 For example,  translated the source dataset to the target domain and then trained a new model on the features of the translated images.  synthesized a dataset and fine-tuned the model using the synthesized dataset.  utilized GANs and attention to leverage the information available from the source domain to the target domain.  developed a framework using UI2I translation models that expanded the classification capability to include the target domain.  Proposed a framework that utilizes a guided transfer learning approach to select layers for fine-tuning, enhancing feature transferability, and minimizing domain discrepancies using JS-Divergence."
"Segmentation of Portal Vein in Multiphase CTA Image Based on Unsupervised Domain Transfer and Pseudo Label","https://scispace.com/paper/segmentation-of-portal-vein-in-multiphase-cta-image-based-on-2l1emox7","2023","Journal Article","Diagnostics","Ziyue Xie
Shiman Li
De Min Yao
Shiyao Chen
Yonghong Shi","10.3390/diagnostics13132250","https://scispace.com/pdf/segmentation-of-portal-vein-in-multiphase-cta-image-based-on-2l1emox7.pdf","Background: Clinically, physicians diagnose portal vein diseases on abdominal CT angiography (CTA) images scanned in the hepatic arterial phase (H-phase), portal vein phase (P-phase) and equilibrium phase (E-phase) simultaneously. However, existing studies typically segment the portal vein on P-phase images without considering other phase images. Method: We propose a method for segmenting portal veins on multiphase images based on unsupervised domain transfer and pseudo labels by using annotated P-phase images. Firstly, unsupervised domain transfer is performed to make the H-phase and E-phase images of the same patient approach the P-phase image in style, reducing the image differences caused by contrast media. Secondly, the H-phase (or E-phase) image and its style transferred image are input into the segmentation module together with the P-phase image. Under the constraints of pseudo labels, accurate prediction results are obtained. Results: This method was evaluated on the multiphase CTA images of 169 patients. The portal vein segmented from the H-phase and E-phase images achieved DSC values of 0.76 and 0.86 and Jaccard values of 0.61 and 0.76, respectively. Conclusion: The method can automatically segment the portal vein on H-phase and E-phase images when only the portal vein on the P-phase CTA image is annotated, which greatly assists in clinical diagnosis.","contrast and morphological changes of intertemporal blood vessels. Therefore, we believe that the key to portal vein segmentation in H-phase and E-phase CTA images is to solve the unsupervised domain transfer problem, which is limited by vessel labeling. Introduction:
 It is known that Generative Adversarial Network (GAN)  provides a way to learn deep representations without a large amount of annotated training data. And it can generalize the models trained on the source domain (annotated training datasets) to the target domain (test datasets) through transfer learning  and visual domain transfer techniques . In particular, the technological breakthrough around GAN, such as Cy-cleGAN , can generate high-quality images under unsupervised conditions and is often applied to multimodality image annotation problems. Recently, Xu et al.  used noise-labeled data for the challenging task of liver vessel segmentation. Jiang et al.  used unpaired CT and MRI images for domain adaptive transformations and guided the student CT networks with the help of an informative teacher MRI network to extract features indicating foreground and background differences. Other studies have explored domain transfer segmentation on CT and MRI images of the heart . In general, obtaining the similarity between different images by domain transfer is a feasible annotation method. Introduction:
 In addition, the lack of sufficient public datasets has led to an increasing interest in the study of unsupervised and semi-supervised deep learning . Medical images are usually presented in multiple modalities, such as MRI and CT, so unsupervised domain adaptive learning using annotations of one modality to obtain annotations of another mo- It is known that Generative Adversarial Network (GAN)  provides a way to learn deep representations without a large amount of annotated training data. And it can generalize the models trained on the source domain (annotated training datasets) to the target domain (test datasets) through transfer learning  and visual domain transfer techniques . In particular, the technological breakthrough around GAN, such as CycleGAN , can generate high-quality images under unsupervised conditions and is often applied to multimodality image annotation problems. Recently, Xu et al.  used noise-labeled data for the challenging task of liver vessel segmentation. Jiang et al.  used unpaired CT and MRI images for domain adaptive transformations and guided the student CT networks with the help of an informative teacher MRI network to extract features indicating foreground and background differences. Other studies have explored domain transfer segmentation on CT and MRI images of the heart . In general, obtaining the similarity between different images by domain transfer is a feasible annotation method."
"SLIViT: a general AI framework for clinical-feature diagnosis from limited 3D biomedical-imaging data","https://scispace.com/paper/slivit-a-general-ai-framework-for-clinical-feature-diagnosis-2dq98svy","2023","Posted Content","","","10.21203/rs.3.rs-3044914/v1","https://scispace.com/pdf/slivit-a-general-ai-framework-for-clinical-feature-diagnosis-2dq98svy.pdf","Abstract We present SLIViT, a deep-learning framework that accurately measures disease-related risk factors in volumetric biomedical imaging, such as magnetic resonance imaging (MRI) scans, optical coherence tomography (OCT) scans, and ultrasound videos. To evaluate SLIViT, we applied it to five different datasets of these three different data modalities tackling seven learning tasks (including both classification and regression) and found that it consistently and significantly outperforms domain-specific state-of-the-art models, typically improving performance (ROC AUC or correlation) by 10-40%. Notably, compared to existing approaches, SLIViT can be applied even when only a small number of annotated training samples is available, which is often a constraint in medical applications. When trained on less than 700 annotated volumes, SLIViT obtained accuracy comparable to trained clinical specialists while reducing annotation time by a factor of 5,000 demonstrating its utility to automate and expedite ongoing research and other practical clinical scenarios. ","Wykoff,Elior Rahmani,Corey W Arnold,Bolei Zhou,Noah Zaitlen,Ilan Gronau,Sriram Sankararaman,Jeffrey N Chiang,Srinivas R Sadda,Eran Halperin + Main:
 Biomedical imaging analysis is a critical component of clinical care with widespread use across multiple domains. For example, analyzing optical coherence tomography (OCT) images of the retina allows ophthalmologists to diagnose and follow up on ocular diseases, such as age-related macular degeneration (AMD), and tailor appropriate and personalized interventions to delay the progression of retinal atrophy and irreversible vision loss  . Another example is the analysis of heart function using cardiac imaging, such as heart computed tomography and ultrasound. Monitoring heart function can help cardiologists assess potential cardiac issues, prescribe medications to improve a medical condition, e.g., reduced heart ejection fraction, and guide treatment decisions  . Lastly, radiologists' analysis and regular monitoring of breast imaging such as mammography and magnetic resonance imaging (MRI) help detect early breast cancers, initiate a consequent interventive therapy, and determine the effectiveness of such therapeutics  . Main:
 These medical insights and actionable information are obtained following an expert's time-intensive manual analysis. The automation of these analyses using artificial intelligence may further improve healthcare as it reduces costs and treatment burden. Main:
 Deep-vision models, such as Convolutional Neural Networks (CNNs) and their derivatives, are considered state-of-the-art methods to tackle computer vision tasks in general and medical-related vision tasks in particular  . In order to train a deep-vision model to accurately learn and predict a target variable in a general vision task (excluding segmentation tasks) from scratch, a very large number of annotated training samples are needed. Transfer learning addresses this challenge by pre-training a vision model for a general learning task on a very large data set, and then using this general model as a starting point for training a specialized model on a much smaller data set  . The key advantage of transfer learning is that the pre-training can be done on a large dataset in another domain, where annotated data are abundant, and then the fine-tuning can be done using a small dataset in the domain of interest. Using a transfer learning approach, a plethora of previously developed deep-vision models for 2D medical-imaging analysis  , were first pre-trained on over a million labeled natural images taken from ImageNet  , and then, fine-tuned to a specific medical-learning task on a much smaller number of biomedical images (typically fewer than 10,000). The availability of the labeled ImageNet dataset and the understanding that pre-trained weights can be leveraged as 'prior knowledge' for fine-tuning other learning tasks, were major factors in the fruitfulness of these 2D medical-imaging deep-vision models."
"Weakly Supervised Learning with Automated Labels from Radiology Reports
  for Glioma Change Detection","https://scispace.com/paper/weakly-supervised-learning-with-automated-labels-from-2l99cbv6","2022","Posted Content","","","10.48550/arxiv.2210.09698","https://scispace.com/pdf/weakly-supervised-learning-with-automated-labels-from-2l99cbv6.pdf","Gliomas are the most frequent primary brain tumors in adults. Glioma change detection aims at finding the relevant parts of the image that change over time. Although Deep Learning (DL) shows promising performances in similar change detection tasks, the creation of large annotated datasets represents a major bottleneck for supervised DL applications in radiology. To overcome this, we propose a combined use of weak labels (imprecise, but fast-to-create annotations) and Transfer Learning (TL). Specifically, we explore inductive TL, where source and target domains are identical, but tasks are different due to a label shift: our target labels are created manually by three radiologists, whereas our source weak labels are generated automatically from radiology reports via NLP. We frame knowledge transfer as hyperparameter optimization, thus avoiding heuristic choices that are frequent in related works. We investigate the relationship between model size and TL, comparing a low-capacity VGG with a higher-capacity ResNeXt model. We evaluate our models on 1693 T2-weighted magnetic resonance imaging difference maps created from 183 patients, by classifying them into stable or unstable according to tumor evolution. The weak labels extracted from radiology reports allowed us to increase dataset size more than 3-fold, and improve VGG classification results from 75% to 82% AUC. Mixed training from scratch led to higher performance than fine-tuning or feature extraction. To assess generalizability, we ran inference on an open dataset (BraTS-2015: 15 patients, 51 difference maps), reaching up to 76% AUC. Overall, results suggest that medical imaging problems may benefit from smaller models and different TL strategies with respect to computer vision datasets, and that report-generated weak labels are effective in improving model performances. Code, in-house dataset and BraTS labels are released. ","inherently imperfect, they are drastically faster to obtain with respect to manual labels, and are also more scalable since they potentially allow to leverage tens of thousands of retrospective exams that would otherwise remain unused in hospital PACS (Picture Archiving and Communication Systems). Introduction:
 Transfer Learning (TL) is the branch of machine learning where knowledge acquired from a specific task or domain (source) is exploited to solve a downstream, related task (target) . Since datasets used by most research groups in medical imaging are typically small  (especially compared to datasets in Computer Vision (CV)), TL holds great potential to overcome data scarcity in the field. Adopting the notation from , we can formally define a domain D and a task T as D = {X, P(x)} and T = {Y, f(•)}, where X is the feature space, P(x) is the corresponding marginal probability distribution, Y is the label space, and f(•) is the objective predictive function. Moreover, we use the notations Ds, Dt, Ts, and Tt to indicate source domain, target domain, source task and target task, respectively. Most papers dealing with medical TL focused on the choice of the source domain Ds, trying to understand which is the best Ds from which we should transfer knowledge. For instance, several works investigated the use of natural images (e.g. the ImageNet dataset ) for pre-training - . Conversely, more recent works showed that the use of natural images could lead to negligible performance improvements , and rather suggested that using a medical domain as source is preferable - . Differently from these previous studies, in our work we explore the TL scenario where source and target domain are identical (Ds = Dt), but the tasks are different (Ts ≠ Tt) because of distinct label spaces (Ys ≠ Yt). In other words, we aim to understand to what extent it is possible to transfer knowledge from a source domain which has a different label distribution from the target domain, a scenario called inductive TL . We address the task of glioma change detection with difference maps as input samples (Ds = Dt, details in section 2.4), with Ys consisting of weak labels generated automatically from radiology reports, and Yt consisting of manual labels created by human experts, again from radiology reports. Introduction:
 Once domains (Ds, Dt) and tasks (Ts, Tt) have been defined, TL can be further subdivided into three main types : Introduction:
 • Fine-tuning: the DL model is pre-trained on the source domain and then all its weights are finetuned on the target domain. • Feature Extraction: the DL model is pre-trained on the source domain and then only some of its weights (typically the last linear layers) are fine-tuned on the target domain. Instead, the convolutional backbone layers are usually ""frozen"" (i.e. not trained again)."
"Augmentation based unsupervised domain adaptation","https://scispace.com/paper/augmentation-based-unsupervised-domain-adaptation-ug58xfwt","2022","Posted Content","","","10.48550/arxiv.2202.11486","https://scispace.com/pdf/augmentation-based-unsupervised-domain-adaptation-ug58xfwt.pdf","The insertion of deep learning in medical image analysis had lead to the development of state-of-the art strategies in several applications such a disease classification, as well as abnormality detection and segmentation. However, even the most advanced methods require a huge and diverse amount of data to generalize. Because in realistic clinical scenarios, data acquisition and annotation is expensive, deep learning models trained on small and unrepresentative data tend to outperform when deployed in data that differs from the one used for training (e.g data from different scanners). In this work, we proposed a domain adaptation methodology to alleviate this problem in segmentation models. Our approach takes advantage of the properties of adversarial domain adaptation and consistency training to achieve more robust adaptation. Using two datasets with white matter hyperintensities (WMH) annotations, we demonstrated that the proposed method improves model generalization even in corner cases where individual strategies tend to fail. ","ADAPTATION Authors: A Preprint,Mauricio Orbes-Arteaga,Thomas Varsavsky,Lauge Sørensen,Mads Nielsen,Akshay Pai,Sébastien Ourselin,Marc Modat,Jorge Cardoso Introduction:
 Domain adaptation has become an important area of research in medical image analysis. In applications such a segmentation or classification, domain adaptation aims to improve machine learning models' generalization making them more applicable in clinical practice. Introduction:
 Machine learning models trained under empirical risk minimization tend to under-perform under distribution shift between training (source domain) and testing data (target domain). When analysing medical images such as magnetic resonance images, differences in the acquisition scanners and protocols can result in differences distribution shift that affect the performance of even most sophisticated deep learning models. Introduction:
 Deep domain adaptation is the research area that focuses in adaptation of CNN based models, by learning representations that are domain invariant but still meaningful for the task at hand. Several research works have been proposed for Deep domain adaptation, a detailed review with focus in computer vision applications can be found in , also  discussed domain adaptation methods in medical image analysis. Both works provide a similar taxonomy for domain adaptation methods which grouped them into reconstruction based methods, adversarial based methods and discrepancy based methods. Introduction:
 Reconstruction based methods rely on the idea that a model can be trained to accomplish both, an objective task (e.g segmentation/classification) and an auxiliary task such a image reconstruction which can be learned from unlabeled data. By doing this, the auxiliary task acts as a regulariser that promotes inter domain representations learning and consequently improving the performance on the target domain set. A disadvantage of this methods is that usually the reconstruction of medical images is troublesome due to presence of abnormalities which also are diverse in morphology and localisation. Auxiliary tasks that are more complicated than the objective task usually lead to bad distribution of the feature representation and consequently low performance. On the other hand, adversarial and domain adaptation arXiv:2202.11486v1 [eess.IV] 23 Feb 2022 methods have shown to be more efficient and therefore more popular in domain adaptation tasks. One disadvantage of adversarial domain adaptation is the need for an auxiliary model (generator/discriminator), which adds complexity to the training hyper-parameter tuning."
"Positive-Unlabeled Domain Adaptation","https://scispace.com/paper/positive-unlabeled-domain-adaptation-3qqz42dl","2022","Proceedings Article","2022 IEEE 9th International Conference on Data Science and Advanced Analytics (DSAA)","","10.1109/dsaa54385.2022.10032409","https://scispace.com/pdf/positive-unlabeled-domain-adaptation-3qqz42dl.pdf","Domain Adaptation methodologies have shown to effectively generalize from a labeled source domain to a label scarce target domain. Previous research has either focused on unlabeled domain adaptation without any target supervision or semi-supervised domain adaptation with few labeled target examples per class. On the other hand Positive-Unlabeled (PU-) Learning has attracted increasing interest in the weakly supervised learning literature since in quite some real world applications positive labels are much easier to obtain than negative ones. In this work we are the first to introduce the challenge of Positive-Unlabeled Domain Adaptation where we aim to generalise from a fully labeled source domain to a target domain where only positive and unlabeled data is available. We present a novel two-step learning approach to this problem by firstly identifying reliable positive and negative pseudo-labels in the target domain guided by source domain labels and a positive-unlabeled risk estimator. This enables us to use a standard classifier on the target domain in a second step. We validate our approach by running experiments on benchmark datasets for visual object recognition. Furthermore we propose real world examples for our setting and validate our superior performance on parking occupancy data. ","full labels in retrospective. This effort is much better invested when the labeled dataset can be used together with images on all kinds of scanners and scanner parameters. We therefore think it would be highly interesting to apply our methodology to medical image classification in future research. Conclusion:
 In this work we introduced a novel domain adaptation scenario were we aim to generalize from a fully labeled source domain to a target domain with only few positive and unlabeled data. Since there are naturally no methodologies designed for this setting yet, we proposed a relatively simple yet effective two-step learning approach inspired by recent work from semi-supervised learning and unsupervised domain adaptation. We thereby set the first baseline for positiveunlabeled domain adaptation. We benchmarked our approach on various publicly available datasets from image classification and found that our approach outperforms the baselines from (semi-supervised)-domain adaptation and PU-learning even in few shot scenarios with 5 and 10 positive target examples. We also proposed real-world applications for our novel setting and validated our methodology on the use case of parking availability predictions. In future works we plan to develop further methodologies in our new setting inspired by approaches that are commonly used in DA-and PU-learning like GANs."
"Learning Self-Supervised Representations for Label Efficient Cross-Domain Knowledge Transfer on Diabetic Retinopathy Fundus Images","https://scispace.com/paper/learning-self-supervised-representations-for-label-efficient-13cfy2lmfd","2023","Journal Article","","Ekta Gupta
Vineeta Gupta
Muskaan Chopra
Prakash Chandra Chhipa
Marcus Liwicki","10.1109/ijcnn54540.2023.10191796","https://scispace.compdf/learning-self-supervised-representations-for-label-efficient-13cfy2lmfd.pdf","This work presents a novel label-efficient self-supervised representation learning-based approach for classifying diabetic retinopathy (DR) images in cross-domain settings. Most of the existing DR image classification methods are based on supervised learning which requires a lot of time-consuming and expensive medical domain experts-annotated data for training. The proposed approach uses the prior learning from the source DR image dataset to classify images drawn from the target datasets. The image representations learned from the unlabeled source domain dataset through contrastive learning are used to classify DR images from the target domain dataset. Moreover, the proposed approach requires a few labeled images to perform successfully on DR image classification tasks in cross-domain settings. The proposed work experiments with four publicly available datasets: EyePACS, APTOS 2019, MESSIDOR-I, and Fundus Images for self-supervised representation learning-based DR image classification in cross-domain settings. The proposed method achieves state-of-the-art results on binary and multi-classification of DR images, even in cross-domain settings. The proposed method outperforms the existing DR image binary and multi-class classification methods proposed in the literature. The proposed method is also validated qualitatively using class activation maps, revealing that the method can learn explainable image representations. The source code and trained models are published on GitHub <sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">1</sup> <sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">1</sup> https://github.com/prakashchhipa/Learning-Self-Supervised-Representations-for-Label-Efficient-Cross-Domain-Knowledge-Transfer-on-DRF. ","dataset (a subset of EyePACS) (ii) downstream task, i.e., classification of DR(Diabetic Retinopathy) images from the target domain datasets ( APTOS 2019, Messidor-I and Fundus Images). IV. SELF-SUPERVISED CROSS DOMAIN KNOWLEDGE TRANSFER FRAMEWORK:
 In the pretext task, the proposed approach applies various augmentations like flipping, affine transformations, jitter, grayscale, etc., to create different views from the images. The different views created from the same image act as positive pairs, and views from different images act as negative pairs. IV. SELF-SUPERVISED CROSS DOMAIN KNOWLEDGE TRANSFER FRAMEWORK:
 Then, image representations are learned through contrastive learning from positive and negative pairs of images. These learned representations of images act as input to the downstream task. This task does not require labeled images for representation learning as shown in Figure . IV. SELF-SUPERVISED CROSS DOMAIN KNOWLEDGE TRANSFER FRAMEWORK:
 The downstream task involves binary as well as multi-class classification of DR images. The model pretrained for learning image representation during the pretext task act as initialization for performing the downstream task, i.e., classification of DR images. Now the downstream task requires fewer labeled images for performing DR classification. IV. SELF-SUPERVISED CROSS DOMAIN KNOWLEDGE TRANSFER FRAMEWORK:
 Figure  provides a detailed architecture of the proposed approach. The objective of the proposed approach is to obtain representations that are robust to domain shift and generalizable to the downstream task. IV. SELF-SUPERVISED CROSS DOMAIN KNOWLEDGE TRANSFER FRAMEWORK:
 The proposed approach uses an unlabeled source dataset to learn the representations and a labeled target dataset to solve the classification task by reusing these features learned from the source dataset. The representations have been learned using the SimCLR (simple framework for contrastive learning) method . IV. SELF-SUPERVISED CROSS DOMAIN KNOWLEDGE TRANSFER FRAMEWORK:
 As discussed, positive and negative pairs of DR images are created from unlabelled DR images using different augmentations like a Gaussian blur, flipping, translation, rotation, jitter, etc. These positive and negative pairs of images are fed to the encoder network. IV. SELF-SUPERVISED CROSS DOMAIN KNOWLEDGE TRANSFER FRAMEWORK:
 The encoder network consists of a ResNet-50 backbone and a projection head containing two fully connected layers of 2048 and 1024 neurons, respectively. IV. SELF-SUPERVISED CROSS DOMAIN KNOWLEDGE TRANSFER FRAMEWORK:
 This network is trained on positive and negative views of images using Normalized Temperature-scaled Cross-Entropy (NT-Xent) as the loss function, which tries to pull positive pairs close and push away the negative pairs. This loss function is defined as: IV. SELF-SUPERVISED CROSS DOMAIN KNOWLEDGE TRANSFER FRAMEWORK:
 Where z i and z j are representations of positive pairs, T is the temperature parameter, n is the number of images, and sim() represents the similarity function. IV. SELF-SUPERVISED CROSS DOMAIN KNOWLEDGE TRANSFER FRAMEWORK:
 This loss function is the negative log-likelihood of similarity between positive pairs to the ratio of similarities between all possible positive and negative pairs. This loss function is a softmax function normalized using a temperature parameter."
"Pre-text Representation Transfer for Deep Learning with Limited Imbalanced Data : Application to CT-based COVID-19 Detection","https://scispace.com/paper/pre-text-representation-transfer-for-deep-learning-with-1x70nsos","2023","Proceedings Article","Image and Vision Computing New Zealand","Fouzia Altaf
Syed Ms Islam
Naeem Khalid Janjua
Naveed Akhtar","10.48550/arXiv.2301.08888","https://scispace.compdf/pre-text-representation-transfer-for-deep-learning-with-1x70nsos.pdf","Annotating medical images for disease detection is often tedious and expensive. Moreover, the available training samples for a given task are generally scarce and imbalanced. These conditions are not conducive for learning effective deep neural models. Hence, it is common to 'transfer' neural networks trained on natural images to the medical image domain. However, this paradigm lacks in performance due to the large domain gap between the natural and medical image data. To address that, we propose a novel concept of Pre-text Representation Transfer (PRT). In contrast to the conventional transfer learning, which fine-tunes a source model after replacing its classification layers, PRT retains the original classification layers and updates the representation layers through an unsupervised pre-text task. The task is performed with (original, not synthetic) medical images, without utilizing any annotations. This enables representation transfer with a large amount of training data. This high-fidelity representation transfer allows us to use the resulting model as a more effective feature extractor. Moreover, we can also subsequently perform the traditional transfer learning with this model. We devise a collaborative representation based classification layer for the case when we leverage the model as a feature extractor. We fuse the output of this layer with the predictions of a model induced with the traditional transfer learning performed over our pre-text transferred model. The utility of our technique for limited and imbalanced data classification problem is demonstrated with an extensive five-fold evaluation for three large-scale models, tested for five different class-imbalance ratios for CT based COVID-19 detection. Our results show a consistent gain over the conventional transfer learning with the proposed method.","Representation Transfer for Deep Learning with Limited & Imbalanced Data : Application to CT-based COVID-19 Detection Authors: Fouzia Altaf,Naeem K Janjua,Naveed Akhtar Keywords: Transfer learning, Imbalanced data, COVID-19 Introduction:
 In the medical imaging domain, data labelling requires medical experts, who must carefully analyse the samples to provide the correct annotation. Not only that this process is tedious, expensive and strongly reliant on the availability arXiv:2301.08888v1 Introduction:
 [eess.IV] 21 Jan 2023 of medical experts, the data itself suffers from plenty of challenges. First, it is common that the positive samples of a disease are much rarer than the negative samples. Introduction:
 This naturally creates an imbalance in the data, which is particularly challenging to induce unbiased computational models using that data. Second, for the geographically constrained facilities, both positive and negative samples are often too few to effectively train a computational model that can facilitate automated disease detection. Introduction:
 Incidentally, global data sharing through public repositories also fails to fully resolve these issues due to the data privacy constraints. Whereas medical images are easily searchable content on the internet, their annotations related to a specific diagnostic task are seldom available. Introduction:
 It is well-established that deep learning  can induce computational models that can achieve expert-level accuracy for many disease detection tasks using medical images . This fact has led to a wave of deploying deep learning solutions in medical image analysis . Introduction:
 However, this technology can only perform effective computational modelling if it is provided with a large amount of training data (e.g., a million samples). For the medical tasks, these samples need to be appropriately annotated by the experts. Introduction:
 Thus, the challenges noted in the preceding paragraph present a bottleneck for fully exploiting deep learning in medical image analysis. Currently, Transfer Learning (TL)  is a common strategy to side-step this bottleneck , , . Introduction:
 Transfer Learning takes a deep learning model pre-trained for a source domain, and fine-tunes it with a target domain data. For the medical tasks, natural images usually form the source domain  due to their convenient annotations. Introduction:
 The central idea behind TL is that by using a large amount of training images, the pre-trained model (a.k.a. source model) learns a detailed representation of the source domain. This representation also encodes the primitive patterns that form the fundamental image ingredients. Introduction:
 Since the target medical domain also comprises images, it is likely that a slight modification to this encoding can already be sufficient to represent the target domain samples reasonably well. Transfer learning seeks to induce the desired modification with the scarcely available data for the medical task at hand. Introduction:
 Altaf et al.  recently noted that the large domain gap between the natural and medical images compromises the performance of TL for the medical tasks. They argued that this large gap requires proportionally large data of the target domain for an effective model transfer. Introduction:
 Hence, they proposed to first transfer the source model to the target domain with a large-scale annotated medical data, albeit under a different auxiliary imaging modality. Introduction:
 Their assumption is that, for a target data modality (e.g., CT scans), large-scale annotated samples are available for a related auxiliary modality (e.g., radiographs) in the medical domain."
"Multimodal Machine Learning in Image-Based and Clinical Biomedicine: Survey and Prospects","https://scispace.com/paper/multimodal-machine-learning-in-image-based-and-clinical-dij7ctr98y","2024","Journal Article","International Journal of Computer Vision","Elisa Warner
Joon Sang Lee
William Hsu
Tanveer Syeda-Mahmood
Charles Η. Kahn
Olivier Gevaert
Arvind Rao","10.1007/s11263-024-02032-8","https://scispace.compdf/multimodal-machine-learning-in-image-based-and-clinical-dij7ctr98y.pdf","Abstract Machine learning (ML) applications in medical artificial intelligence (AI) systems have shifted from traditional and statistical methods to increasing application of deep learning models. This survey navigates the current landscape of multimodal ML, focusing on its profound impact on medical image analysis and clinical decision support systems. Emphasizing challenges and innovations in addressing multimodal representation, fusion, translation, alignment, and co-learning, the paper explores the transformative potential of multimodal models for clinical predictions. It also highlights the need for principled assessments and practical implementation of such models, bringing attention to the dynamics between decision support systems and healthcare providers and personnel. Despite advancements, challenges such as data biases and the scarcity of “big data” in many biomedical domains persist. We conclude with a discussion on principled innovation and collaborative efforts to further the mission of seamless integration of multimodal ML models into biomedical practice. ","the source and target output to be as similar as possible to each other. Domain Adaptation: In the final loss function, the edge generator is used to constrain the segmenter in such a way as to promote better edge consistency in the target domain. Domain Adaptation:
 In yet another, simpler example, domain adaptation to a target domain is performed in  by taking a network trained on the source domain and simply adjusting the parameters of the batch normalization layer. Domain Adaptation:
 Domain adaptation in biomedicine can be a common problem where instrument models or parameters change. Among multimodal co-learning methods, most networks are constructed as segmentation networks for MRI and CT because they are similar imaging domains, although measuring different things. Domain Adaptation:
 While CT carries distinct meaning in its pixels (measured in Hounsfield Units), MRI pixel intensities are not standardized and usually require normalization, which could pose challenges to this multimodal problem. Domain Adaptation:
 Additionally, MRI carries much more detail than CT scans, which necessitates the model to understand contextual boundaries of objects much more than a unimodal case with only CT or MRI. Discussion:
 The rapidly evolving landscape of artificial intelligence (AI) both within the biomedical field and beyond has posed a substantial challenge in composing this survey. Our aim is to provide the reader with a comprehensive overview of the challenges and contemporary approaches to multimodal machine learning in image-based, clinically relevant biomedicine. Discussion:
 However, it is essential to acknowledge that our endeavor cannot be fully comprehensive due to the dynamic nature of the field and the sheer volume of emerging literature within the biomedical domain and its periphery. Discussion:
 This robust growth has led to a race among industry and research institutions to integrate the latest cutting-edge models into the healthcare sector, with a particular emphasis on the introduction of ""large language models"" (LLMs). Discussion:
 In recent years, there has been an emergence of market-level insights into the future of healthcare and machine learning, as exemplified by the incorporation of machine learning models into wearable devices such as the Apple Watch and Fitbit devices for the detection of atrial fibrillation . Discussion:
 This begs the question: where does this transformative journey lead us? Discussion:
 Healthcare professionals and physicians already embrace the concept of multimodal cognitive models in their diagnostic and prognostic practices, signaling that such computer models based on multimodal frameworks are likely to endure within the biomedical landscape. Discussion:
 However, for these models to be effectively integrated into clinical settings, they must exhibit flexibility that aligns with the clinical environment. If the ultimate goal is to seamlessly incorporate these AI advancements into clinical practice, a fundamental question arises: how can these models be practically implemented on-site? Discussion:
 Presently, most available software tools for clinicians are intended as auxiliary aids, but healthcare professionals have voiced concerns regarding the potential for increased computational workload, alert fatigue, and the limitations imposed by Electronic Health Record (EHR) interfaces . Discussion:
 Therefore, it is paramount to ensure that any additional software introduced into clinical settings serves as an asset rather than a hindrance. Discussion:
 Another pertinent issue emerging from these discussions pertains to the dynamics between clinical decision support systems (CDSS) and healthcare providers. What occurs when a computer-generated recommendation contradicts a physician's judgment?"
"Generalizing Across Domains in Diabetic Retinopathy via Variational Autoencoders","https://scispace.com/paper/generalizing-across-domains-in-diabetic-retinopathy-via-5lk2jphg2e","2023","Journal Article","","Sharon Chokuwa
Muhammad Haris Khan","10.48550/arxiv.2309.11301","https://scispace.compdf/generalizing-across-domains-in-diabetic-retinopathy-via-5lk2jphg2e.pdf","Domain generalization for Diabetic Retinopathy (DR) classification allows a model to adeptly classify retinal images from previously unseen domains with various imaging conditions and patient demographics, thereby enhancing its applicability in a wide range of clinical environments. In this study, we explore the inherent capacity of variational autoencoders to disentangle the latent space of fundus images, with an aim to obtain a more robust and adaptable domain-invariant representation that effectively tackles the domain shift encountered in DR datasets. Despite the simplicity of our approach, we explore the efficacy of this classical method and demonstrate its ability to outperform contemporary state-of-the-art approaches for this task using publicly available datasets. Our findings challenge the prevailing assumption that highly sophisticated methods for DR classification are inherently superior for domain generalization. This highlights the importance of considering simple methods and adapting them to the challenging task of generalizing medical images, rather than solely relying on advanced techniques. ","Title: Generalizing Across Domains in Diabetic Retinopathy via Variational Autoencoders Authors: Sharon Chokuwa,Muhammad H Khan Keywords: Domain Generalization, Diabetic Retinopathy, Variational Autoencoder"
"Unsupervised Domain Adaptation with Shape Constraint and Triple Attention for Joint Optic Disc and Cup Segmentation","https://scispace.com/paper/unsupervised-domain-adaptation-with-shape-constraint-and-3hmvesey","2022","Journal Article","Sensors","Fengming Zhang
Shuiwang Li
Jian Song Deng","10.3390/s22228748","https://scispace.com/pdf/unsupervised-domain-adaptation-with-shape-constraint-and-3hmvesey.pdf","Currently, glaucoma has become an important cause of blindness. At present, although glaucoma cannot be cured, early treatment can prevent it from getting worse. A reliable way to detect glaucoma is to segment the optic disc and cup and then measure the cup-to-disc ratio (CDR). Many deep neural network models have been developed to autonomously segment the optic disc and the optic cup to help in diagnosis. However, their performance degrades when subjected to domain shift. While many domain-adaptation methods have been exploited to address this problem, they are apt to produce malformed segmentation results. In this study, it is suggested that the segmentation network be adjusted using a constrained formulation that embeds prior knowledge about the shape of the segmentation areas that is domain-invariant. Based on IOSUDA (i.e., Input and Output Space Unsupervised Domain Adaptation), a novel unsupervised joint optic cup-to-disc segmentation framework with shape constraints is proposed, called SCUDA (short for Shape-Constrained Unsupervised Domain Adaptation). A shape constrained loss function is novelly proposed in this paper which utilizes domain-invariant prior knowledge concerning the segmentation region of the joint optic cup–optical disc of fundus images to constrain the segmentation result during network training. In addition, a convolutional triple attention module is designed to improve the segmentation network, which captures cross-dimensional interactions and provides a rich feature representation to improve the segmentation accuracy. Experiments on the RIM-ONE_r3 and Drishti-GS datasets demonstrate that the algorithm outperforms existing approaches for segmenting optic discs and cups.","Figure 1.: The remainder of the paper is structured as follows: we review related work and describe our methodology in Sections 2 and 3, respectively; experimental findings are discussed in Section 4; and the work is concluded in Section 5. Unsupervised Domain Adaptation:
 A fairly common type of transfer learning is domain adaptation, which generally refers to using a model from one domain and apply it another domain that is only subtly different . Unsupervised domain adaptation in classification is generally built on image and feature alignment  between source and target domains. For instance, Long et al.  proposed a new network architecture, Deep Adaptation Network (DAN), that used an optimal multi-core selection method for average embedding matching and was able to reduce domain differences. Bousmalis et al.  considered shared and private representations of each domain. Unsupervised domain adaptive segmentation has been used for many scenarios, including across various medical images. For example, according to Chen et al. , the network can be trained using images from the source domain, with the target domain's image style being the same as that of the source domain. Huo et al.  proposed a Synthetic Segmentation Network (SynSegNet) in order to stylize images from the source domain into those from the target domain. Song et al.  introduced several assumptions for feature space extraction; based on this, each loss function was derived and optimized. In addition, to compare the feature spaces of the source, target, and output domains with one another, Chen et al.  proposed Synergistic Image and Feature Alignment (SIFA)."
"Structure Preserving Cycle-GAN for Unsupervised Medical Image Domain
  Adaptation","https://scispace.com/paper/structure-preserving-cycle-gan-for-unsupervised-medical-1tb8qhdh","2023","Posted Content","","","10.48550/arxiv.2304.09164","https://scispace.com/pdf/structure-preserving-cycle-gan-for-unsupervised-medical-1tb8qhdh.pdf","The presence of domain shift in medical imaging is a common issue, which can greatly impact the performance of segmentation models when dealing with unseen image domains. Adversarial-based deep learning models, such as Cycle-GAN, have become a common model for approaching unsupervised domain adaptation of medical images. These models however, have no ability to enforce the preservation of structures of interest when translating medical scans, which can lead to potentially poor results for unsupervised domain adaptation within the context of segmentation. This work introduces the Structure Preserving Cycle-GAN (SP Cycle-GAN), which promotes medical structure preservation during image translation through the enforcement of a segmentation loss term in the overall Cycle-GAN training process. We demonstrate the structure preserving capability of the SP Cycle-GAN both visually and through comparison of Dice score segmentation performance for the unsupervised domain adaptation models. The SP Cycle-GAN is able to outperform baseline approaches and standard Cycle-GAN domain adaptation for binary blood vessel segmentation in the STARE and DRIVE datasets, and multi-class Left Ventricle and Myocardium segmentation in the multi-modal MM-WHS dataset. SP Cycle-GAN achieved a state of the art Myocardium segmentation Dice score (DSC) of 0.7435 for the MR to CT MM-WHS domain adaptation problem, and excelled in nearly all categories for the MM-WHS dataset. SP Cycle-GAN also demonstrated a strong ability to preserve blood vessel structure in the DRIVE to STARE domain adaptation problem, achieving a 4% DSC increase over a default Cycle-GAN implementation. ","unsupervised domain adaptation models for clinicians to use in the context of medical image segmentation. The domain adaptation approach highlighted in this work can be used when training data from a target domain is difficult to obtain due to resource limitations, such as remote or marginalized communities. Literature Review:
 As mentioned earlier, often it is common for different portions of data to suffer from domain shift, where data is split into multiple different domains with distinct distributions and feature domains . This domain shift issue can lead predictive models trained on one domain to struggle when it is exposed to unseen data in a different domain. The main concept of domain adaptation is to learn a model which can generalize labelled source domain data to a target domain by minimizing the distribution differences between the domains . Several approaches to domain adaptation have been devised in the past, depending on the form of the data. The problem of domain shift can be categorized into three sections: prior shift (class imbalance), covariate shift, and concept shift . Most of the attempts to create domain adaptation models focus on the covariate shift problem, which is defined when two probability distributions differ, while the conditional probability distributions are equivalent across domains . Domain adaptation falls into several approach categories, including instance-based adaptation, feature-based adaptation and deep domain adaptation . The focus of this work is on deep domain adaptation, which revolves around the use of neural deep learning models for domain adaptation, and more specifically on the adversarial approach to deep domain adaptation. Deep adversarial domain adaptation is mainly inspired by the use of Generative Adversarial Networks (GANs), which are deep-learning models which learn based on a two player game, where a Generator creates an artificial image in the target domain which attempts to fool the Discriminator into misclassifying it as a sample of the ground truth target domain class . In effect, GANs in a domain adaptation application are attempting to convert images in the source domain to images with as small a discrepancy as possible to the target domain , through a pixel-level image translation process. Attempts to build GANs for domain adaptation include Domain Transfer Network  and Cycle-GAN , the latter of which inspired the domain adaptation approach for this work. Domain adaptation has an important role in the medical image analysis, due to the common presence of domain shift in medical data and images . This domain shift is caused by factors such as the use of different imaging technologies, the variable properties of different scanners, use of different scanner protocols or even different subject groups . These factors can often result in a large domain shift in medical images, for example, brain MR scans using T1-weighted scans vs. T2-weighted scans can cause a large intensity distribution difference in scans . Additionally, different imaging modalities such as contrast enhancing T1 (ceT1) or high-resolution T2 (hrT2) images can result in a large domain shift between images . The categorization of the domain adaptation approach taken is based on factors such as label availability, presence of cross-modality, and model type. For example, domain adaptation can be supervised, semi-supervised and unsupervised depending on label availability and the use of ground truth data during the domain adaptation model training . Domain adaptation is important in the field of medical image analysis because the domain shift problem can greatly impact predictive model efficacy on unseen image domains . An example of the efficacy of GANs for medical domain adaptation was highlighted in the Cross-Modality Domain Adaptation for Medical Image Segmentation (CrossMoDa) 2021 challenge, where GAN-style domain adaptation approaches were effective in domain adaptation between ceT1 and hrT2 style brain MR scans for Vestibular Schwannoma and Choclea segmentation ."
"Structure Preserving Cycle-GAN for Unsupervised Medical Image Domain Adaptation","https://scispace.com/paper/structure-preserving-cycle-gan-for-unsupervised-medical-119s7w08","2023","Journal Article","arXiv.org","Naimul Mefraz Khan","10.48550/arXiv.2304.09164","https://scispace.compdf/structure-preserving-cycle-gan-for-unsupervised-medical-119s7w08.pdf","The presence of domain shift in medical imaging is a common issue, which can greatly impact the performance of segmentation models when dealing with unseen image domains. Adversarial-based deep learning models, such as Cycle-GAN, have become a common model for approaching unsupervised domain adaptation of medical images. These models however, have no ability to enforce the preservation of structures of interest when translating medical scans, which can lead to potentially poor results for unsupervised domain adaptation within the context of segmentation. This work introduces the Structure Preserving Cycle-GAN (SP Cycle-GAN), which promotes medical structure preservation during image translation through the enforcement of a segmentation loss term in the overall Cycle-GAN training process. We demonstrate the structure preserving capability of the SP Cycle-GAN both visually and through comparison of Dice score segmentation performance for the unsupervised domain adaptation models. The SP Cycle-GAN is able to outperform baseline approaches and standard Cycle-GAN domain adaptation for binary blood vessel segmentation in the STARE and DRIVE datasets, and multi-class Left Ventricle and Myocardium segmentation in the multi-modal MM-WHS dataset. SP Cycle-GAN achieved a state of the art Myocardium segmentation Dice score (DSC) of 0.7435 for the MR to CT MM-WHS domain adaptation problem, and excelled in nearly all categories for the MM-WHS dataset. SP Cycle-GAN also demonstrated a strong ability to preserve blood vessel structure in the DRIVE to STARE domain adaptation problem, achieving a 4% DSC increase over a default Cycle-GAN implementation.","unsupervised depending on label availability and the use of ground truth data during the domain adaptation model training . Literature Review: Domain adaptation is important in the field of medical image analysis because the domain shift problem can greatly impact predictive model efficacy on unseen image domains . Literature Review:
 An example of the efficacy of GANs for medical domain adaptation was highlighted in the Cross-Modality Domain Adaptation for Medical Image Segmentation (CrossMoDa) 2021 challenge, where GAN-style domain adaptation approaches were effective in domain adaptation between ceT1 and hrT2 style brain MR scans for Vestibular Schwannoma and Choclea segmentation . Literature Review:
 The ability to account for domain shift between medical datasets can allow for predictive models to be trained on imaging modalities that are cheaper, safer and more accessible to clinicians in remote or developing communities. Literature Review:
 For example, hrT2 brain MRI scans are much more cost effective  as well as less intrusive than ceT1 scans, which require administration of a contrast enhancing agent to the patient . Literature Review:
 Cycle-GAN was introduced in Zhu et al.'s, ""Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks"" . This paper presented a cyclical GAN model which translates images from a source domain X to a target domain Y, without the use of a paired image dataset . Literature Review:
 The proposed method implements two mapping functions, G and F, which maps the domain X to Y and the domain of Y to X, respectively . Literature Review:
 The authors used adversarial loss terms for Discriminators D Y and D X which attempt to classify translated images from G and F versus their source domain counterparts. Literature Review:
 Additionally, the authors posit that learned mapping functions for image translation should by cycle-consistent, such that for a source domain image x and domain mapping functions F and G, x → G(x) → F (G(x)) ≈ x. Literature Review:
 This culminates in a model architecture that is capable of bi-directional image translation between two domains of images. The Cycle-GAN loss combining the discussed adversarial and cyclical losses is defined as : Literature Review:
 Where the L GAN terms in Equation 1 denote the Discriminator-based Mean Squared Error loss for the X and Y domains, and the L cyc term is an L1 norm loss based on the difference between original and recovered source and target domain images to enforce cyclical consistency . Literature Review:
 Cycle-GAN based DA techniques have been shown to be effective approaches on medical segmentation problems , including DA between STARE and DRIVE datasets for unsupervised DA for medical segmentation . Literature Review:
 Vo and Khan (2021) introduced an edge-preserving loss function for Cycle-GAN based medical image domain adaptation for STARE and DRIVE segmentation. Literature Review:
 The work introduced the EdgeCycleGAN model which preserved the original Cycle-GAN architecture, but added an edge preserving term to the loss function of Equation , by introducing an L1 norm of the difference between the canny edge extractor (C e ) output of the recovered and original input image for each domain of the Cycle-GAN training process . Literature Review:
 This resulted in increased segmentation performance on unsupervised DA for blood vessel segmentation over the baseline method without any domain adaptation, and increased performance over the use of a standard Cycle-GAN ."
"Structure Preserving Cycle-GAN for Unsupervised Medical Image Domain Adaptation","https://scispace.com/paper/structure-preserving-cycle-gan-for-unsupervised-medical-13brnsib","2023","Posted Content","","","10.32920/22734377.v1","https://scispace.com/pdf/structure-preserving-cycle-gan-for-unsupervised-medical-13brnsib.pdf","&lt;p&gt;The presence of domain shift in medical imaging is a common issue, which can greatly impact the performance of segmentation models when dealing with unseen image domains. Adversarial-based deep learning models, such as Cycle-GAN, have become a common model for approaching unsupervised domain adaptation of medical images. These models however, have no ability to enforce the preservation of structures of interest when translating medical scans, which can lead to potentially poor results for unsupervised domain adaptation within the context of segmentation. This work introduces the Structure Preserving Cycle-GAN (SP Cycle-GAN), which promotes medical structure preservation during image translation through the enforcement of a segmentation loss term in the overall Cycle-GAN training process. We demonstrate the structure preserving capability of the SP Cycle-GAN both visually and through comparison of Dice score segmentation performance for the unsupervised domain adaptation models. The SP Cycle-GAN is able to outperform baseline approaches and standard Cycle-GAN domain adaptation for binary blood vessel segmentation in the STARE and DRIVE datasets, and multi-class Left Ventricle and Myocardium segmentation in the multi-modal MM-WHS dataset. SP Cycle-GAN achieved a state of the art Myocardium segmentation Dice score (DSC) of 0.7435 for the MR to CT MM-WHS domain adaptation problem, and excelled in nearly all categories for the MM-WHS dataset. SP Cycle-GAN also demonstrated a strong ability to preserve blood vessel structure in the DRIVE to STARE domain adaptation problem, achieving a 4% DSC increase over a default Cycle-GAN implementation.&lt;/p&gt; ","unsupervised domain adaptation models for clinicians to use in the context of medical image segmentation. The domain adaptation approach highlighted in this work can be used when training data from a target domain is difficult to obtain due to resource limitations, such as remote or marginalized communities. Literature Review:
 As mentioned earlier, often it is common for different portions of data to suffer from domain shift, where data is split into multiple different domains with distinct distributions and feature domains . This domain shift issue can lead predictive models trained on one domain to struggle when it is exposed to unseen data in a different domain. The main concept of domain adaptation is to learn a model which can generalize labelled source domain data to a target domain by minimizing the distribution differences between the domains . Several approaches to domain adaptation have been devised in the past, depending on the form of the data. The problem of domain shift can be categorized into three sections: prior shift (class imbalance), covariate shift, and concept shift . Most of the attempts to create domain adaptation models focus on the covariate shift problem, which is defined when two probability distributions differ, while the conditional probability distributions are equivalent across domains . Domain adaptation falls into several approach categories, including instance-based adaptation, feature-based adaptation and deep domain adaptation . The focus of this work is on deep domain adaptation, which revolves around the use of neural deep learning models for domain adaptation, and more specifically on the adversarial approach to deep domain adaptation. Deep adversarial domain adaptation is mainly inspired by the use of Generative Adversarial Networks (GANs), which are deep-learning models which learn based on a two player game, where a Generator creates an artificial image in the target domain which attempts to fool the Discriminator into misclassifying it as a sample of the ground truth target domain class . In effect, GANs in a domain adaptation application are attempting to convert images in the source domain to images with as small a discrepancy as possible to the target domain , through a pixel-level image translation process. Attempts to build GANs for domain adaptation include Domain Transfer Network  and Cycle-GAN , the latter of which inspired the domain adaptation approach for this work. Domain adaptation has an important role in the medical image analysis, due to the common presence of domain shift in medical data and images . This domain shift is caused by factors such as the use of different imaging technologies, the variable properties of different scanners, use of different scanner protocols or even different subject groups . These factors can often result in a large domain shift in medical images, for example, brain MR scans using T1-weighted scans vs. T2-weighted scans can cause a large intensity distribution difference in scans . Additionally, different imaging modalities such as contrast enhancing T1 (ceT1) or high-resolution T2 (hrT2) images can result in a large domain shift between images . The categorization of the domain adaptation approach taken is based on factors such as label availability, presence of cross-modality, and model type. For example, domain adaptation can be supervised, semi-supervised and unsupervised depending on label availability and the use of ground truth data during the domain adaptation model training . Domain adaptation is important in the field of medical image analysis because the domain shift problem can greatly impact predictive model efficacy on unseen image domains . An example of the efficacy of GANs for medical domain adaptation was highlighted in the Cross-Modality Domain Adaptation for Medical Image Segmentation (CrossMoDa) 2021 challenge, where GAN-style domain adaptation approaches were effective in domain adaptation between ceT1 and hrT2 style brain MR scans for Vestibular Schwannoma and Choclea segmentation ."
"A Comparative Study of Deep Learning and Transfer Learning in Detection of Diabetic Retinopathy","https://scispace.com/paper/a-comparative-study-of-deep-learning-and-transfer-learning-jhy5yi0z","2022","Journal Article","International Journal of Computer Applications Technology and Research","J. F. Kamiri
Geoffrey Wambugu
Aaron M. Oirere","10.7753/ijcatr1107.1001","https://scispace.com/pdf/a-comparative-study-of-deep-learning-and-transfer-learning-jhy5yi0z.pdf","Computer vision has gained momentum in medical imaging tasks. Deep learning and Transfer learning are some of the approaches used in computer vision. The aim of this research was to do a comparative study of deep learning and transfer learning in the detection of diabetic retinopathy. To achieve this objective, experiments were conducted that involved training four state-of-the-art neural network architectures namely; EfficientNetB0, DenseNet169, VGG16, and ResNet50. Deep learning involved training the architectures from scratch. Transfer learning involved using the architectures which are pre-trained using the ImageNet dataset and then fine-tuning them to solve the task at hand. The results show that transfer learning outperforms learning from scratch in all three models. VGG16 achieved the highest accuracy of 84.12% in transfer learning. Another notable finding is that transfer learning is able to not only achieve high accuracy with very few epochs but also starts higher than deep learning in the first epoch. This study has also demonstrated that in image processing tasks there are a lot of transferrable features since the ImageNet weights worked well in the Diabetic retinopathy detection task.","Title: A Comparative Study of Deep Learning and Transfer Learning in Detection of Diabetic Retinopathy Authors: Jackson Kamiri Keywords: Meta-Learning, Transfer learning, Deep learning, Medical Image processing, Diabetic Retinopathy INTRODUCTION:
 The evolution of machine learning has greatly contributed to solving some of the major problems in the world. Of particular interest is deep learning which has become a game-changer in computer vision due to its representation learning capabilities . Under representational learning, a machine is fed with raw data and it develops its own representation needed to extract the data . This is made possible by convolutional neural networks since they can extract features from an image using the convolutional layer and thus a separate feature extractor is not needed. INTRODUCTION:
 Deep learning has been applied in a variety of image processing tasks in various fields to solve image processing problems . Deep learning has promising results in complex medical diagnostics. It helps physicians by providing a second opinion and flagging concerning areas in images . INTRODUCTION:
 Meta-Learning is also known as learning-to-learn makes it possible for deep learning models to do multitask learning and use transfer learning to enable them to solve a new but related task with just a few data samples also known as fewshot learning . This helps in addressing the challenges of data shortage and increases the robustness of models developed using this approach. INTRODUCTION:
 Transfer learning involves training a deep learning architecture with huge amounts of data. This training involves feature extraction from the training dataset. Once the training is done the weights are then transferred and finetuned to a smaller dataset . Thus, this makes it possible for the transfer learning model to leverage on previously acquired knowledge. INTRODUCTION:
 The main aim of this study was to do a comparative study of deep learning and transfer learning in the detection of diabetic retinopathy. Deep learning, involved training a model from scratch using the dataset. In transfer learning, a model is first pre-trained using the ImageNet dataset then the weights are transferred to the Diabetic retinopathy dataset. Further to this, the model is finetuned and the results of the two approaches are compared. INTRODUCTION:
 The other sections are organized as follows. 2.0 related works, 3.0 Methodology, 4.0 results, and discussion, 5.0 conclusion and future work. RELATED WORKS:
 Diagnosis based on medical images has been very successful in using convolutional neural network-based methods. This is largely motivated by the fact that CNN has achieved human-level capabilities in tasks involving object classification . CNN networks have also demonstrated strong performance in transfer learning in medical imagebased diagnostics . During transfer learning the weights apart from the last SoftMax layer were finetuned to the target dataset using the Adam optimizer and its default parameters (learning rate 0.001, β1 = 0.9, β2 = 0.999, and a batch size of 32) ."
"DGM-DR: Domain Generalization with Mutual Information Regularized Diabetic Retinopathy Classification","https://scispace.com/paper/dgm-dr-domain-generalization-with-mutual-information-3kpajmce18","2023","Journal Article","arXiv.org","A. Matsun
Dana O. Mohamed
Sharon Chokuwa
Muhammad Nafis Ridzuan
Mohammad Yaqub","10.48550/arxiv.2309.09670","https://scispace.compdf/dgm-dr-domain-generalization-with-mutual-information-3kpajmce18.pdf","The domain shift between training and testing data presents a significant challenge for training generalizable deep learning models. As a consequence, the performance of models trained with the independent and identically distributed (i.i.d) assumption deteriorates when deployed in the real world. This problem is exacerbated in the medical imaging context due to variations in data acquisition across clinical centers, medical apparatus, and patients. Domain generalization (DG) aims to address this problem by learning a model that generalizes well to any unseen target domain. Many domain generalization techniques were unsuccessful in learning domain-invariant representations due to the large domain shift. Furthermore, multiple tasks in medical imaging are not yet extensively studied in existing literature when it comes to DG point of view. In this paper, we introduce a DG method that re-establishes the model objective function as a maximization of mutual information with a large pretrained model to the medical imaging field. We re-visit the problem of DG in Diabetic Retinopathy (DR) classification to establish a clear benchmark with a correct model selection strategy and to achieve robust domain-invariant representation for an improved generalization. Moreover, we conduct extensive experiments on public datasets to show that our proposed method consistently outperforms the previous state-of-the-art by a margin of 5.25% in average accuracy and a lower standard deviation. Source code available at https://github.com/BioMedIA-MBZUAI/DGM-DR","Title: DGM-DR: Domain Generalization with Mutual Information Regularized Diabetic Retinopathy Classification Authors: Aleksandr Matsun,Dana O Mohamed,Sharon Chokuwa,Muhammad Ridzuan,Mohammad Yaqub Keywords: Domain Generalization, Diabetic Retinopathy, Mutual Information Regularization"
"Cross-Domain Multi-disease Ocular Disease Recognition via Data Enhancement","https://scispace.com/paper/cross-domain-multi-disease-ocular-disease-recognition-via-1uvwfsa1","2023","Posted Content","","","10.21203/rs.3.rs-2970818/v1","https://scispace.com/pdf/cross-domain-multi-disease-ocular-disease-recognition-via-1uvwfsa1.pdf","Abstract Ophthalmic diseases afflict many people, and can even lead to irreversible blindness. Therefore, the search for effective early diagnosis methods has attracted the attention of many researchers and clinicians. At present, although there are some ways for the early screening of ophthalmic diseases, the early screening of fundus images based on deep learning is generally favored by the medical community due to its non-contact characteristic, non-invasive characteristic and high recognition accuracy. However, the generalization performance of a common model and cross-domain identification is usually weak due to different collection equipment, race, and patient conditions. Although the existing fundus image recognition technology has achieved some results, the effect is still in the cross-domain problem and is not satisfactory. In this paper, a cross-domain fundus image recognition framework based on deep neural networks with data enhancement is proposed. First, the ResNeXt101 model is chosen as the basic framework. Second, some data enhancement methods and focal loss are used to improve recognition performance. Finally, the results of experiment show that the final score of the framework is improved by about 10% using ordinary data enhancement methods and focal loss. In summary, the method proposed in this paper can effectively solve the problem of poor generalization ability for cross-domain early fundus screening and can provide inspiration and ideas for future related works. ","Title: Cross-Domain Multi-disease Ocular Disease Recognition via Data Enhancement Authors: Qiong Wang,Jun Yao,Nan Yan Keywords: Cross-Domain, Ocular Disease, focal loss, fundus image, data enhancement"
"Detecting abnormal fundus images by employing deep transfer learning","https://scispace.com/paper/detecting-abnormal-fundus-images-by-employing-deep-transfer-29q82saoku","2020","Posted Content","","Yan Yu
Xiao Chen
Xiang-bing Zhu
Peng-fei Zhang
Yin-fen Hou
Rong-rong Zhang
Changfan Wu","10.21203/RS.2.24133/V1","https://scispace.com/pdf/detecting-abnormal-fundus-images-by-employing-deep-transfer-29q82saoku.pdf","
 Background: To develop and validate a deep transfer learning (DTL) algorithm for detecting abnormalities in fundus images from nonmydriatic fundus photography examinations.
Methods : A total of 1,295 fundus images from January 2017 to December 2018 at Yijishan Hospital of Wannan Medical College were collected for developing and validating the deep transfer learning algorithm in detecting abnormal fundus images. The DTL model was developed by using 929 (normal 254, abnormal 402) fundus images, including normal fundus images and abnormal fundus images, the latter including maculopathy, optic neuropathy, vascular lesion, choroidal lesions, vitreous disease, and cataracts. We tested our model using a subset of the publicly available Messidor dataset (using 366 images) and evaluated the testing performance of the DTL model for detecting abnormal fundus images.
Results : In the internal validation dataset (n=273 images), the AUC, sensitivity, accuracy, and specificity of the DTL for correctly classified fundus images were 0.997, 97.41%, 97.07%, and 96.82%, respectively. For the test dataset (n=273 images), the AUC, sensitivity, accuracy, and specificity of the DTL for correctly classifying fundus images were 0.926, 88.17%, 87.18%, and 86.67%, respectively.
Conclusion : In the evaluation, the DTL presented high sensitivity and specificity for detecting abnormal fundus-related diseases. Further research is necessary to improve this method and evaluate the applicability of the DTL in the community health care center.
Key words : Fundus images; Deep transfer learning; Developing and validation; Artificial intelligence.","data. In this study, the transfer learning algorithm shows a well-applied prospect in community health care centers for screening retinal disease. The techniques described in this study, with great potential, apply in other medical eld image classi cations. Discussion:
 DTL is surprisingly effective in image classi cation. However, our study in its current state has several limitations. First, due to a training set in which our experts labeled mild myopic fundus as normal, the DTL trained on this set accessed a higher than normal prior probability for eye disease detection, which may cause a high false-negative rate. Second, our study dataset is not large and includes only patients from a local clinical setting. At present, the algorithm cannot be independent or matched with professional evaluation, but it can provide abnormal fundus images with obvious diagnoses so that ophthalmologists can focus on more di cult cases. Conclusions:
 In conclusion, the current project demonstrated that deep transfer learning presented a promising future in the diagnosis of various diseases with higher accuracy and robustness based on multidomain data. In future work, we will be dedicated to adding more auxiliary domain information to our model and explore a screening algorithm for classifying retinal pathologic lesions and providing treatment recommendations. Further steps include improving this method and validating and evaluating its applicability in the community health care center. Declarations:
 Tables Due to technical limitations, all table les are only available for download from the Supplementary Files section.    The accuracy and the learning rate of the training process."
"Student Becomes Decathlon Master in Retinal Vessel Segmentation via Dual-Teacher Multi-target Domain Adaptation","https://scispace.com/paper/student-becomes-decathlon-master-in-retinal-vessel-27tb3t1k","2022","Journal Article","Lecture Notes in Computer Science","Siti Wahidah, Dian Hartati
Siti Wahidah, Dian Hartati","10.1007/978-3-031-21014-3_4","https://scispace.com/pdf/student-becomes-decathlon-master-in-retinal-vessel-27tb3t1k.pdf","Unsupervised domain adaptation has been proposed recently to tackle the so-called domain shift between training data and test data with different distributions. However, most of them only focus on single-target domain adaptation and cannot be applied to the scenario with multiple target domains. In this paper, we propose RVms, a novel unsupervised multi-target domain adaptation approach to segment retinal vessels (RVs) from multimodal and multicenter retinal images. RVms mainly consists of a style augmentation and transfer (SAT) module and a dual-teacher knowledge distillation (DTKD) module. SAT augments and clusters images into source-similar domains and source-dissimilar domains via Bézier and Fourier transformations. DTKD utilizes the augmented and transformed data to train two teachers, one for source-similar domains and the other for source-dissimilar domains. Afterwards, knowledge distillation is performed to iteratively distill different domain knowledge from teachers to a generic student. The local relative intensity transformation is employed to characterize RVs in a domain invariant manner and promote the generalizability of teachers and student models. Moreover, we construct a new multimodal and multicenter vascular segmentation dataset from existing publicly-available datasets, which can be used to benchmark various domain adaptation and domain generalization methods. Through extensive experiments, RVms is found to be very close to the target-trained Oracle in terms of segmenting the RVs, largely outperforming other state-of-the-art methods. ","Adaptation Authors: Linkai Peng,Li Lin,Pujin Cheng,Huaqing He,Xiaoying Tang (Corresponding Author) Keywords: Multi-target domain adaptation, Dual teacher, Knowledge distillation, Style transfer, Retinal vessel segmentation Introduction:
 Multimodal ophthalmic images can be effectively employed to extract retinal structures, identify biomarkers, and diagnose diseases. For example, fundus im-ages can depict critical anatomical structures such as the macula, optic disc, and retinal vessels (RVs) . Optical coherence tomography angiography (OCTA) can efficiently and accurately generate volumetric angiography images . These two modalities can both deliver precise representations of vascular structures within the retina, making them popular for the diagnoses of eye-related diseases. These years, advance in the imaging technologies brings new ophthalmic modalities including optical coherence tomography (OCT) , widefield fundus , and photoacoustic images . These new modalities also provide important information of RVs and are likely to enhance the disease diagnosis accuracy. In retinal disease analysis, RV segmentation is a very important pre-requisite. Manual delineation is the most accurate yet highly labor-intensive and timeconsuming manner, especially for RV segmentation across multiple modalities. One plausible solution is to manually label RVs on images of a single modality and then transfer the labels to other modalities of interest via advanced image processing and deep learning techniques. However, researches have shown that deep learning models trained on one domain generally perform poorly when tested on another domain with different data distribution, because of domain shift . This indicates that segmentation models trained on existing modalities will have low generalizability on new modalities. Furthermore, there also exist variations within images of the same modality because of different equipments and imaging settings, posing more challenges to a generalized RV segmentation model for images with cross-center or cross-modality domain shift. Introduction:
 To address this issue, domain adaptation has been explored, among which unsupervised domain adaptation (UDA) has gained the most favor in recent years. For instance, Javanmardi et al.  combined U-net  with a domain discriminator and introduced adversarial training based on DAAN . Wang et al.  adopted this design to joint optic disc and cup segmentation from fundus images. These two methods focus on domain shift with small variations (i.e., cross-center domain shift). To tackle domain shift with large variations (i.e., cross-modality domain shift), Cai et al.  incorporated CycleGAN  into its own network and employed a shape consistency loss to ensure a correct translation of semantics in medical images. Dou et al.  designed a cross-modality UDA framework for cardiac MR and CT image segmentation by performing DA only at low-level layers, under the assumption that domain shift mainly lies in low-level characteristics. Zhang et al.  presented a Noise Adaptation Generative Adversarial Network for RV segmentation and utilized a style discriminator enforcing the translated images to have the same noise patterns as those in the target domain. Peng et al.  employed disentangled representation learning to disentangle images into content space and style space to extract domain-invariant features. These approaches are nevertheless designed to handle only a specific type of domain shift and may have limitations when applied to multimodal and multicenter (m&m) ophthalmic images."
"Cross-Dataset Generalization For Retinal Lesions Segmentation","https://scispace.com/paper/cross-dataset-generalization-for-retinal-lesions-2p26oi9gyk","2024","Preprint","","Clément Playout
Foued Cheriet","10.48550/arxiv.2405.08329","https://scispace.compdf/cross-dataset-generalization-for-retinal-lesions-2p26oi9gyk.pdf","Identifying lesions in fundus images is an important milestone toward an automated and interpretable diagnosis of retinal diseases. To support research in this direction, multiple datasets have been released, proposing groundtruth maps for different lesions. However, important discrepancies exist between the annotations and raise the question of generalization across datasets. This study characterizes several known datasets and compares different techniques that have been proposed to enhance the generalisation performance of a model, such as stochastic weight averaging, model soups and ensembles. Our results provide insights into how to combine coarsely labelled data with a finely-grained dataset in order to improve the lesions segmentation. ","Title: CROSS-DATASET GENERALIZATION FOR RETINAL LESIONS SEGMENTATION Authors: Clément Playout,Farida Cheriet Keywords: Fundus, Retina, Lesions, Segmentation, Generalization INTRODUCTION:
 The American Academy of Ophthalmology estimates that approximately one-third of Americans risk developing diabetes mellitus , an alarming estimation corroborated by studies in many countries . Among its potential complications, diabetic retinopathy (DR) is the leading cause of legal blindness in the working-age population worldwide. INTRODUCTION:
 Effective screening strategies are needed to treat the disease, increasingly involving tele-screening initiatives . AI-based models have been actively researched to reduce costs, expand screening opportunities and scale to nation-wide populations. INTRODUCTION:
 In parallel, the need for clinically interpretable models has led to developing methods to automatically identify lesions in the retina from fundus images. To do so, many datasets have been published in recent years to foster research on segmentation algorithms. INTRODUCTION:
 However, given that labelling at a pixel level is a time-consuming and expensive task, strategies are required to mitigate these costs. For large datasets, adopting a coarser granularity for the manual labelling of lesions is often the only choice. INTRODUCTION:
 This leads to important discrepancies in terms of raw images and labelling styles between datasets and naturally raises the question of cross-dataset generalization, the latter question being relevant in most applications of computer vision. INTRODUCTION:
 In the past decade, the field of segmentation has seen an almost complete shift from traditional computer vision algorithms to a new de-facto standard built on fully convolutional neural networks (CNNs). Numerous architectures have been designed to segment one or multiple lesion types relevant to the detection of DR . INTRODUCTION:
 Even with these modern architectures, training a deep learning model generally requires a large amount of data. Recently, multiple datasets have been released, usually with accompanying architectures for segmentation and grading. However, these datasets are not standardized, thus each has its specificities. INTRODUCTION:
 The IDRID challenge  introduced a fundus dataset composed of 81 images with mask annotations for four lesion types. DDR was released  alongside a study on both segmentation and grading of DR. That work reported high accuracy results for DR grading but highlighted the difficulties posed by segmentation. INTRODUCTION:
 More recently, the FGADR dataset has been released : with 1852 annotated images, it is to our knowledge the largest dataset for fundus segmentation. Their study proposes a thorough analysis of the benefits of segmentation masks for DR grading as well as for the detection of other pathologies. INTRODUCTION:
 A similar work was proposed in , introducing another large dataset (we refer to it as Retinal Lesions) as well as a new segmentation architecture called Lesion-Net. INTRODUCTION:
 In this paper, we study the generalization ability of different segmentation models over these four datasets and include a fifth one composed of publicly available images annotated by a team of experts. We first characterize each dataset, then use them to analyse the generalization properties of different architectures. INTRODUCTION:
 We then compare different approaches that have been proposed to improve a model's generalization: Stochastic Weight Averaging , Model soups  and an ensemble of models. To our knowledge, neither of the first two have been experimented with segmentation."
"Multimodal Machine Learning for Clinically-Assistive Imaging-Based Biomedical Applications","https://scispace.com/paper/multimodal-machine-learning-for-clinically-assistive-imaging-1abmcjroz3","2023","Journal Article","arXiv.org","Elisa Warner
Joonsang Lee
William Hsu
Tanveer Syeda-Mahmood
Charles Kahn
Arvind Rao","10.48550/arxiv.2311.02332","https://scispace.compdf/multimodal-machine-learning-for-clinically-assistive-imaging-1abmcjroz3.pdf","Machine learning (ML) applications in medical artificial intelligence (AI) systems have shifted from traditional and statistical methods to increasing application of deep learning models and even more recently generative models. Recent years have seen a rise in the discovery of widely-available deep learning architectures that support multimodal data integration, particularly with images. The incorporation of multiple modalities into these models is a thriving research topic, presenting its own unique challenges. In this work, we discuss five challenges to multimodal AI as it pertains to ML (representation, fusion, alignment, translation, and co-learning) and survey recent approaches to addressing these challenges in the context of medical image-based clinical decision support models. We conclude with a discussion of the future of the field, suggesting directions that should be elucidated further for successful clinical models and their translation to the clinical setting.","domain into a segmentation network . Domain Adaptation: In other applications, data augmentation for domain generalization may be executed utilizing simpler affine transformations . This demonstrates the utility of data augmentation strategies in more broadly defining decision boundaries where target domains differ from the source. Domain Adaptation:
 A second strategy for domain adaptation involves constraining neural network functions trained on a target domain by creating loss functions which require alignment with a source domain model. Domain Adaptation:
 In , a framework for adapting segmentation models at test-time is proposed, whereby an adversarial loss trains a target-based U-Net to be as similar to a source-based U-Net as possible. Then a paired-consistency loss with adversarial examples is utilized to fine-tune the decision boundary to include morphologically similar data points. Domain Adaptation:
 In a specificially multimodal segmentation-based model,  attempts to create two side-by-side networks, a segmenter and an edge generator, which both encourage the source and target output to be as similar as possible to each other. Domain Adaptation:
 In the final loss function, the edge generator is used to constrain the segmenter in such a way as to promote better edge consistency in the target domain. Domain Adaptation:
 In yet another, simpler example, domain adaptation to a target domain is performed in  by taking a network trained on the source domain and simply adjusting the parameters of the batch normalization layer. Domain adaptation in biomedicine can be a common problem where instrument models or parameters change. Domain Adaptation:
 Among multimodal co-learning methods, most networks are constructed as segmentation networks for MRI and CT because they are similar imaging domains, although measuring different things. Domain Adaptation:
 While CT carries distinct meaning in its pixels (measured in Hounsfield Units), MRI pixel intensities are not standardized and usually require normalization, which could pose challenges to this multimodal problem. Domain Adaptation:
 Additionally, MRI carries much more detail than CT scans, which necessitates the model to understand contextual boundaries of objects much more than a unimodal case with only CT or MRI. Discussion:
 The rapidly evolving landscape of artificial intelligence (AI) both within the biomedical field and beyond has posed a substantial challenge in composing this survey. Our aim is to provide the reader with a comprehensive overview of the challenges and contemporary approaches to multimodal machine learning in image-based, clinically relevant biomedicine. Discussion:
 However, it is essential to acknowledge that our endeavor cannot be fully comprehensive due to the dynamic nature of the field and the sheer volume of emerging literature within the biomedical domain and its periphery. Discussion:
 This robust growth has led to a race among industry and research institutions to integrate the latest cutting-edge models into the healthcare sector, with a particular emphasis on the introduction of ""large language models"" (LLMs). Discussion:
 In recent years, there has been an emergence of market-level insights into the future of healthcare and machine learning, as exemplified by the incorporation of machine learning models into wearable devices such as the Apple Watch and Fitbit devices for the detection of atrial fibrillation . Discussion:
 This begs the question: where does this transformative journey lead us? Discussion:
 Healthcare professionals and physicians already embrace the concept of multimodal cognitive models in their diagnostic and prognostic practices, signaling that such computer models based on multimodal frameworks are likely to endure within the biomedical landscape."
"Sharpness-Aware Model-Agnostic Long-Tailed Domain Generalization","https://scispace.com/paper/sharpness-aware-model-agnostic-long-tailed-domain-2hvc1fqug1","2024","Proceedings Article","Proceedings of the ... AAAI Conference on Artificial Intelligence","Houcheng Su
Weihao Luo
Daixian Liu
Mengzhu Wang
Jing Tang
Junyang Chen
Cong Wang
Zhenghan Chen","10.1609/aaai.v38i13.29431","https://scispace.compdf/sharpness-aware-model-agnostic-long-tailed-domain-2hvc1fqug1.pdf","Domain Generalization (DG) aims to improve the generalization ability of models trained on a specific group of source domains, enabling them to perform well on new, unseen target domains. Recent studies have shown that methods that converge to smooth optima can enhance the generalization performance of supervised learning tasks such as classification. In this study, we examine the impact of smoothness-enhancing formulations on domain adversarial training, which combines task loss and adversarial loss objectives. Our approach leverages the fact that converging to a smooth minimum with respect to task loss can stabilize the task loss and lead to better performance on unseen domains. Furthermore, we recognize that the distribution of objects in the real world often follows a long-tailed class distribution, resulting in a mismatch between machine learning models and our expectations of their performance on all classes of datasets with long-tailed class distributions. To address this issue, we consider the domain generalization problem from the perspective of the long-tail distribution and propose using the maximum square loss to balance different classes which can improve model generalizability. Our method's effectiveness is demonstrated through comparisons with state-of-the-art methods on various domain generalization datasets. Code: https://github.com/bamboosir920/SAMALTDG. ","obstructs the training of samples that are difficult to transfer. Introduction: This issue in entropy minimization is known as probability imbalance: classes that are easy to transfer have higher probabilities, resulting in much larger gradients than classes that are difficult to transfer. Introduction:
 In this paper, we introduce a new loss, the maximum squares loss , to tackle the probability imbalance problem. Since the loss of the maximum square has a linearly increasing gradient, it can prevent high-confident areas from producing excessive gradients. Introduction:
 We leverage the popular method DG via ER  and minimize the sharpness measure of the classification loss as our baseline. We also demonstrate the effectiveness of our approach by conducting comprehensive experiments on several benchmarks. Related Work:
 Domain Generalization: Domain generalization (DG) aims to transfer the learning task from multiple source domains and generalize to unseen target domains . Early research in this field concentrated on the concept of distribution alignment, akin to domain adaptation, utilizing kernel methods ) and domain-adversarial learning  to tackle the issue. Related Work:
 Later investigations shifted the focus towards the extraction of domain-invariant features across multiple source domains to establish domain invariance . A number of strategies have employed meta-learning for the derivation of regularization strategies to address the DG problem . Related Work:
 found the direct application of contrastivebased methods, though used to resolve domain generalization, could prove ineffective , suggesting the substitution of original sample-to-sample relations with proxy-to-sample relations. Related Work:
 A myriad of techniques make up the recent advancements in domain generalization. forth the proposal of Adversarial Teacher-Student Representation Learning to create domain-generalizable representations by exploring and generating out-of-source data distributions . Related Work:
 hypothesized that Fourier phase information, which encompasses high-level semantics, is resistant to domain shifts, leading to the introduction of a novel Fourierbased data augmentation strategy . an entropy regularization term to calculate the dependency between class labels and learned features . Related Work:
 suggested that domain generalization could be resolved by matching exact feature distributions ). Wang et al. adopted a multi-task learning paradigm to learn feature embedding that generalizes across domains simultaneously from extrinsic relationship supervision and intrinsic self-supervision for images from multi-source domains . Related Work:
 Zhang et al. offered a method to quantify and enhance transferability with an efficient algorithm for the learning of transferable features . Related Work:
 Recent studies in DG have expanded into the area of Single Domain Generalization, which concentrates on generalization from a lone source domain to unseen target domains . Related Work:
 LDMI ) propose a style-complement module to enhance the generalization power of the model by synthesizing images from diverse distributions that are complementary to the source ones. Related Work:
 TASD ) present a novel approach to address the challenging single domain generalization problem for medical image segmentation, by explicitly exploiting the general semantic shape priors that are extractable from single-domain data and are generalizable across domains to assist domain generalization under the worst-case scenario. Related Work:
 This particular line of research has shown promise in utilizing a single domain to achieve effective generalization, a factor that is particularly relevant when faced with limited data or the unavailability of multiple source domains. Related Work:
 Recently, the task of Multi-Domain Long-Tailed Recognition (MDLT) was formalized by Yang et al. . MDLT tackles the challenges associated with label imbalance, domain shift, and varying label distributions across domains."
"Consecutive Knowledge Meta-Adaptation Learning for Unsupervised Medical Diagnosis","https://scispace.com/paper/consecutive-knowledge-meta-adaptation-learning-for-rql2ns40","2022","Journal Article","arXiv.org","Yu-Min Zhang
Yawen Hou
Xiuyi Chen
Hongyuan Yu
Long Xia","10.48550/arXiv.2209.10425","https://scispace.compdf/consecutive-knowledge-meta-adaptation-learning-for-rql2ns40.pdf","Deep learning-based Computer-Aided Diagnosis (CAD) has attracted appealing attention in academic researches and clinical applications. Nevertheless, the Convolutional Neural Networks (CNNs) diagnosis system heavily relies on the well-labeled lesion dataset, and the sensitivity to the variation of data distribution also restricts the potential application of CNNs in CAD. Unsupervised Domain Adaptation (UDA) methods are developed to solve the expensive annotation and domain gaps problem and have achieved remarkable success in medical image analysis. Yet existing UDA approaches only adapt knowledge learned from the source lesion domain to a single target lesion domain, which is against the clinical scenario: the new unlabeled target domains to be diagnosed always arrive in an online and continual manner. Moreover, the performance of existing approaches degrades dramatically on previously learned target lesion domains, due to the newly learned knowledge overwriting the previously learned knowledge (i.e., catastrophic forgetting). To deal with the above issues, we develop a meta-adaptation framework named Consecutive Lesion Knowledge Meta-Adaptation (CLKM), which mainly consists of Semantic Adaptation Phase (SAP) and Representation Adaptation Phase (RAP) to learn the diagnosis model in an online and continual manner. In the SAP, the semantic knowledge learned from the source lesion domain is transferred to consecutive target lesion domains. In the RAP, the feature-extractor is optimized to align the transferable representation knowledge across the source and multiple target lesion domains.","translation from MR to CT domain for heart segmentation problems. Medical Imaging Analysis: Dou et al. proposed two parallel domain-specific encoders and decoders where the weights are shared between domains to boost the model performance in both single domain and cross-domain scenarios. Medical Imaging Analysis:
 More recently, under the privacy-preserving condition, Tian et al.  proposed a novel gradient aggregation method, which can better extract the shareable information among the data from multiple local servers. Besides, self-supervised learning also has been widely applied in medical image tasks. Medical Imaging Analysis:
 For example, Xie et al.  introduced a triplet loss for self-supervised learning in nuclei segmentation task. By appending a classification branch to discriminate the high-level feature, Haghighi et al.  improved Model Genesis . Medical Imaging Analysis:
 Moreover, Dong et al.  leveraged the dependencies between the target lesion datasets to propose a new weakly-supervised semantic transfer model in an unsupervised situation, which alternatively explores transferable domain-invariant knowledge between the source and target domains. Medical Imaging Analysis:
 Nevertheless, the medical imaging methods mentioned above, neglect the real-clinical environment where the target lesion data to be diagnosed arrives continuously, and they are not capable of continual learning ability. Methodology:
 In this section, we first formulate the problem of adaptation on consecutive unlabeled target lesion domains and present the overall architecture of the model framework. Then, we introduce the details of the implementation, including the comprehensive theoretical derivation. Problem Setting and Overview:
 Problem Setting: Consider a source lesion domain D s and M consecutive target lesion domains Problem Setting and Overview:
 , where the D s = {x s i , y s i } N s i=1 ⊂ P s consists of N s pairs of samples x s i and their one-hot encoding labels y s i , and P s denotes the source lesion distribution. Define Problem Setting and Overview:
 ) as the m-th unlabeled target lesion domain with N t m target samples x t m,j , where the P t m is the corresponding distribution of the m-th target lesion domain. Problem Setting and Overview:
 Different from traditional domain adaptation methods  that only transfer knowledge learned from a source domain to a single target domain, our model aims to learn transferable knowledge from well-labeled source lesion domain D s , and achieve knowledge adaptation to consecutive multiple unlabeled target lesion domains Problem Setting and Overview:
 Note that the target lesion domain {D t m } M m=1 arrives continuously in real-clinical environment, and we have no any priors about total number of target domains and each target distribution. Problem Setting and Overview:
 In summary, our method focuses on learning a diagnosis model to recognize medical lesions from novel target domains continuously via exploring transferable knowledge from the source lesion domain, while alleviating catastrophic forgetting during adaptation. Problem Setting and Overview:
 Overview: The overview of our proposed Consecutive Lesion Knowledge Meta-Adaptation (CLKM) is depicted in Figure . Specifically, CLKM mainly consists of two phases: Semantic Adaptation Phase (SAP) and Representation Adaptation Phase (RAP). Given the m-th target lesion domain D t m , we construct the support set respectively."
"Machine Learning for Quantification of Small Vessel Disease Imaging Biomarkers","https://scispace.com/paper/machine-learning-for-quantification-of-small-vessel-disease-2tc85o4pzx","2018","Dissertation","","Mohsen Ghafoorian","","https://scispace.com/pdf/machine-learning-for-quantification-of-small-vessel-disease-2tc85o4pzx.pdf","This thesis is devoted to developing fully automated methods for quantification of
small vessel disease imaging bio-markers, namely WMHs and lacunes, using vari-
ous machine learning/deep learning and computer vision techniques. The rest of the
thesis is organized as follows: Chapter 2 describes a conventional machine learning
method for automated detection of WMHs. It should be noted that this method is
optimized to detect WMHs of all size, including small lesions which are much more
difficult to spot, rather than accurately delineating the WMH boundaries. Chap-
ter 3 describes a customized deep learning method for automated segmentation of
WMHs. In Chapter 4, we develop and experiment with a biologically inspired sam-
pling method combined with deep neural networks. Chapter 5 is devoted for delv-
ing deep into transfer learning of the trained deep networks on different domains
for the WMH segmentation task. Finally, in Chapter 6, we describe a two-stage deep
learning method for detection of lacunes.","data from various scanners and protocols, domain adaptation remains a valuable field of study. Introduction: This becomes even more important when dealing with Magnetic Resonance Imaging (MRI), which demonstrates high variations in soft tissue appearances and contrasts among different protocols and settings. Introduction:
 Mathematically, a domain D can be expressed by a feature space χ and a marginal probability distribution P (X), where X = {x 1 , ..., x n } ∈ χ 166 . A supervised learn- ing task on a specific domain D = { χ , P (X)}, consists of a pair of a label space Y and an objective predictive function f (.) (denoted by T = {Y, f (.)}). The objective function f (.) can be learned from the training data, which consists of pairs {x i , y i }, where x i ∈ X and y i ∈ Y . After the training process, the learned model denoted by f (.) is used to predict the label for a new instance x. Given a source domain D S with a learning task T S and a target domain D T with learning task T T , transfer learning is defined as the process of improving the learning of the target predictive function f T (.) in D T using the information in D S and T S , where D S = D T , or T S = T T 166 . We denote fST (.) as the predictive model initially trained on the source domain D S , and domain-adapted to the target domain D T . Introduction:
 In the medical image analysis literature, transfer classifiers such as adaptive SVM and transfer AdaBoost, are shown to outperform the common supervised learning approaches in segmenting brain MRI, trained only on a small set of target domain images  . In another study a machine learning based sample weighting strategy was shown to be capable of handling multi-center chronic obstructive pulmonary disease images  . Recently, also several studies have investigated transfer learning methodologies on deep neural networks applied to medical image analysis tasks. A number of studies used networks pre-trained on natural images to extract features and followed by another classifier, such as a Support Vector Machine (SVM) or a random forest  . Other studies  performed layer fine-tuning on the pre-trained networks for adapting the learned features to the target domain."
"Constrained Maximum Cross-Domain Likelihood for Domain Generalization","https://scispace.com/paper/constrained-maximum-cross-domain-likelihood-for-domain-j750yndn","2022","Journal Article","arXiv.org","Jianxin Lin
Yongqiang Tang
Junping Wang
Wensheng Zhang","10.48550/arXiv.2210.04155","https://scispace.compdf/constrained-maximum-cross-domain-likelihood-for-domain-j750yndn.pdf","—As a recent noticeable topic, domain generalization aims to learn a generalizable model on multiple source domains, which is expected to perform well on unseen test domains. Great efforts have been made to learn domain-invariant features by aligning distributions across domains. However, existing works are often designed based on some relaxed conditions which are generally hard to satisfy and fail to realize the desired joint distribution alignment. In this paper, we propose a novel domain generalization method, which originates from an intuitive idea that a domain-invariant classiﬁer can be learned by minimizing the KL-divergence between posterior distributions from different domains. To enhance the generalizability of the learned classiﬁer, we formalize the optimization objective as an expectation com- puted on the ground-truth marginal distribution. Nevertheless, it also presents two obvious deﬁciencies, one of which is the side-effect of entropy increase in KL-divergence and the other is the unavailability of ground-truth marginal distributions. For the former, we introduce a term named maximum in-domain likelihood to maintain the discrimination of the learned domain- invariant representation space. For the latter, we approximate the ground-truth marginal distribution with source domains under a reasonable convex hull assumption. Finally, a Constrained Maximum Cross-domain Likelihood (CMCL) optimization problem is deduced, by solving which the joint distributions are naturally aligned. An alternating optimization strategy is carefully designed to approximately solve this optimization problem. Extensive experiments on four standard benchmark datasets, i.e. , Digits-DG, PACS, Ofﬁce-Home and miniDomainNet, highlight the superior performance of our method.","used for domain generalization, which trains models by minimizing the worst-case loss over pre-defined groups. B. Domain Generalization: Sagawa et al find that coupling DRO with stronger regularization achieves higher worst-case accuracy in the over-parameterized regime. B. Domain Generalization:
 The core idea of data augmentation based methods is to increase the diversity of training data. MixStyle  is motivated that the visual domain is closely related to image style, which is encoded by feature statistics. The domain diversity can be increased by randomly combining feature statistics between two training instances. B. Domain Generalization:
 Deep Domain-Adversarial Image Generation (DDAIG)  is proposed to fool the domain classifier by augmenting images. A domain transformation network is designed to automatically change image style. Seo et al  propose a Domain-Specific Optimized Normalization (DSON) to remove domain-specific style. B. Domain Generalization:
 Wang et al  design a feature-based style randomization module, which randomizes image style by introducing random noise into feature statistics. These style augmentation based methods actually exploit the prior knowledge about domain shift, that is, the difference across source domains lies in image style. B. Domain Generalization:
 Though they work well in existing benchmarks, style augmentation based methods would probably fail when the domain shift is caused by other potential factors. Methods which do not rely on prior knowledge deserve further study. B. Domain Generalization:
 Domain-invariant representation based methods often achieve domain invariance by aligning distributions of different domains as they did in domain adaptation. B. Domain Generalization:
 Li et al  impose MMD to an adversarial autoencoder to align the marginal distributions P (Z) among domains, and the aligned distribution is matched with a pre-defined prior distribution by adversarial training. Motiian et al  try to align the class-conditional distributions P (Z|Y ) for finer alignment. B. Domain Generalization:
 However, class-conditional distributions alignment based methods hardly deal with the domain shift caused by the label shift, which requires that categorical distribution P (Y ) remains unchanged among domains. Another important branch attempts to achieve domain-invariant representation via domain-invariant classifier learning. B. Domain Generalization:
 IRM  tries to learn a domain-invariant classifier by constraining that the classifier is simultaneously optimal for all domains. But this optimization problem is hard to solve. B. Domain Generalization:
 Our method CMCL learns domaininvariant classifier via posterior distribution alignment, an effective alternating optimization strategy is proposed to solve our optimization problem leading to excellent performance. Zhao et al  propose an entropy regularization term to align posterior distributions. B. Domain Generalization:
 According to our analysis, the proposed entropy term is a side-effect of minimizing KLdivergence, severely damaging classification performance. In our method, a term of maximum in-domain likelihood is proposed to eliminate this side-effect. A. Overview:
 In this paper, we focus on domain generalization for image classification. Suppose the sample and label spaces are represented by X and Y respectively, then a domain can be represented by a joint distribution defined on X × Y."
"MedViT: A Robust Vision Transformer for Generalized Medical Image
  Classification","https://scispace.com/paper/medvit-a-robust-vision-transformer-for-generalized-medical-g2smvu74","2023","Posted Content","","","10.48550/arxiv.2302.09462","https://scispace.com/pdf/medvit-a-robust-vision-transformer-for-generalized-medical-g2smvu74.pdf","Convolutional Neural Networks (CNNs) have advanced existing medical systems for automatic disease diagnosis. However, there are still concerns about the reliability of deep medical diagnosis systems against the potential threats of adversarial attacks since inaccurate diagnosis could lead to disastrous consequences in the safety realm. In this study, we propose a highly robust yet efficient CNN-Transformer hybrid model which is equipped with the locality of CNNs as well as the global connectivity of vision Transformers. To mitigate the high quadratic complexity of the self-attention mechanism while jointly attending to information in various representation subspaces, we construct our attention mechanism by means of an efficient convolution operation. Moreover, to alleviate the fragility of our Transformer model against adversarial attacks, we attempt to learn smoother decision boundaries. To this end, we augment the shape information of an image in the high-level feature space by permuting the feature mean and variance within mini-batches. With less computational complexity, our proposed hybrid model demonstrates its high robustness and generalization ability compared to the state-of-the-art studies on a large-scale collection of standardized MedMNIST-2D datasets. ","Image Classification Authors: Omid Nejati Manzari,Hamid Ahmadabadi,Hossein Kashiani,Shahriar B Shokouhi,Ahmad Ayatollahi,Medvit-T Medvit-S Medvit-L Introduction:
 Medical image classification is a critical step in medical image analysis that uses different factors such as clinical information or imaging modalities to differentiate across medical images. A dependable medical image classification may help clinicians evaluate medical images quickly and with less error. The healthcare industry has significantly benefited from recent Convolution Neural Networks (CNNs) advancements. Such advancements have prompted much research into the use of computer-aided diagnostic systems  based on artificial intelligence in clinical settings. CNNs are able to learn robust discriminative representation from vast volumes of medical data to generate accurate diagnostic performance in medical fields. They validate their satisfactory prediction capabilities and obtain comparable performance as clinicians. Introduction:
 However, the locality bias of CNNs makes it hard for them to learn long-range dependencies in visual data. The texture, shape, and size of many organs vary widely across people, making it difficult to correctly analyze medical data . As such, it is important to extract robust feature representation which can model long-range dependencies in different domains for medical image analysis. Recently, the Transformer architectures have adopted the self-attention mechanisms to model the long-range dependencies between input images and have achieved promising results. Different studies demonstrate their performance superiority compared to CNN architectures . However, a sizable amount of training data is crucial to their success. The construction of a large-scale dataset needs a significant amount of time and resources. Regarding the medical field, radiologist experts must manually annotate and verify medical data, which is costly and time-consuming. While the Transformer architecture mitigates the shortcomings of CNNs, its computational complexity grows quadratically with spatial or embedding dimensions, therefore making it infeasible for most image restoration tasks involving high-resolution images. That is to say, the Transformer architectures address the long-range dependency modeling in CNNs, yet their computational complexity increases quadratically with the spatial dimension . As a result, they cannot be used in realistic clinical settings. Moreover, the state-of-the-art studies assume that training and test data are identically distributed. Consequently, on out-of-domain target domains, they often suffer significant performance drops. The domain shift is more pronounced in healthcare areas since medical images can be captured by different devices at various sites. Consequently, due to different scanners and imaging protocols, their data distribution can greatly vary. In addition, variations in epidemiology at different sites could impact the distribution of ground truth labels between various populations ."
"Domain Generalization for Mammographic Image Analysis with Contrastive
  Learning","https://scispace.com/paper/domain-generalization-for-mammographic-image-analysis-with-2163ijf9","2023","Posted Content","","","10.48550/arxiv.2304.10226","https://scispace.com/pdf/domain-generalization-for-mammographic-image-analysis-with-2163ijf9.pdf","The deep learning technique has been shown to be effectively addressed several image analysis tasks in the computer-aided diagnosis scheme for mammography. The training of an efficacious deep learning model requires large data with diverse styles and qualities. The diversity of data often comes from the use of various scanners of vendors. But, in practice, it is impractical to collect a sufficient amount of diverse data for training. To this end, a novel contrastive learning is developed to equip the deep learning models with better style generalization capability. Specifically, the multi-style and multi-view unsupervised self-learning scheme is carried out to seek robust feature embedding against style diversity as a pretrained model. Afterward, the pretrained network is further fine-tuned to the downstream tasks, e.g., mass detection, matching, BI-RADS rating, and breast density classification. The proposed method has been evaluated extensively and rigorously with mammograms from various vendor style domains and several public datasets. The experimental results suggest that the proposed domain generalization method can effectively improve performance of four mammographic image tasks on the data from both seen and unseen domains, and outperform many state-of-the-art (SOTA) generalization methods. ","Title: Medical Image Analysis Authors: Zheren Li,Zhiming Cui,Lichi Zhang,Sheng Wang,Chenjin Lei,Xi Ouyang,Dongdong Chen,Xiangyu Zhao,Chunling Liu,Zaiyi Liu,Yajia Gu,Dinggang Shen,Jie-Zhi Cheng Keywords: 41A05, 41A10, 65D05, 65D17 Domain generalization, mammographic image analysis, contrastive learning"
"Structure Preserving Cycle-GAN for Unsupervised Medical Image Domain Adaptation","https://scispace.com/paper/structure-preserving-cycle-gan-for-unsupervised-medical-1h499moc","2023","Posted Content","","","10.32920/22734377","https://scispace.com/pdf/structure-preserving-cycle-gan-for-unsupervised-medical-1h499moc.pdf","&lt;p&gt;The presence of domain shift in medical imaging is a common issue, which can greatly impact the performance of segmentation models when dealing with unseen image domains. Adversarial-based deep learning models, such as Cycle-GAN, have become a common model for approaching unsupervised domain adaptation of medical images. These models however, have no ability to enforce the preservation of structures of interest when translating medical scans, which can lead to potentially poor results for unsupervised domain adaptation within the context of segmentation. This work introduces the Structure Preserving Cycle-GAN (SP Cycle-GAN), which promotes medical structure preservation during image translation through the enforcement of a segmentation loss term in the overall Cycle-GAN training process. We demonstrate the structure preserving capability of the SP Cycle-GAN both visually and through comparison of Dice score segmentation performance for the unsupervised domain adaptation models. The SP Cycle-GAN is able to outperform baseline approaches and standard Cycle-GAN domain adaptation for binary blood vessel segmentation in the STARE and DRIVE datasets, and multi-class Left Ventricle and Myocardium segmentation in the multi-modal MM-WHS dataset. SP Cycle-GAN achieved a state of the art Myocardium segmentation Dice score (DSC) of 0.7435 for the MR to CT MM-WHS domain adaptation problem, and excelled in nearly all categories for the MM-WHS dataset. SP Cycle-GAN also demonstrated a strong ability to preserve blood vessel structure in the DRIVE to STARE domain adaptation problem, achieving a 4% DSC increase over a default Cycle-GAN implementation.&lt;/p&gt; ","unsupervised domain adaptation models for clinicians to use in the context of medical image segmentation. The domain adaptation approach highlighted in this work can be used when training data from a target domain is difficult to obtain due to resource limitations, such as remote or marginalized communities. Literature Review:
 As mentioned earlier, often it is common for different portions of data to suffer from domain shift, where data is split into multiple different domains with distinct distributions and feature domains . This domain shift issue can lead predictive models trained on one domain to struggle when it is exposed to unseen data in a different domain. The main concept of domain adaptation is to learn a model which can generalize labelled source domain data to a target domain by minimizing the distribution differences between the domains . Several approaches to domain adaptation have been devised in the past, depending on the form of the data. The problem of domain shift can be categorized into three sections: prior shift (class imbalance), covariate shift, and concept shift . Most of the attempts to create domain adaptation models focus on the covariate shift problem, which is defined when two probability distributions differ, while the conditional probability distributions are equivalent across domains . Domain adaptation falls into several approach categories, including instance-based adaptation, feature-based adaptation and deep domain adaptation . The focus of this work is on deep domain adaptation, which revolves around the use of neural deep learning models for domain adaptation, and more specifically on the adversarial approach to deep domain adaptation. Deep adversarial domain adaptation is mainly inspired by the use of Generative Adversarial Networks (GANs), which are deep-learning models which learn based on a two player game, where a Generator creates an artificial image in the target domain which attempts to fool the Discriminator into misclassifying it as a sample of the ground truth target domain class . In effect, GANs in a domain adaptation application are attempting to convert images in the source domain to images with as small a discrepancy as possible to the target domain , through a pixel-level image translation process. Attempts to build GANs for domain adaptation include Domain Transfer Network  and Cycle-GAN , the latter of which inspired the domain adaptation approach for this work. Domain adaptation has an important role in the medical image analysis, due to the common presence of domain shift in medical data and images . This domain shift is caused by factors such as the use of different imaging technologies, the variable properties of different scanners, use of different scanner protocols or even different subject groups . These factors can often result in a large domain shift in medical images, for example, brain MR scans using T1-weighted scans vs. T2-weighted scans can cause a large intensity distribution difference in scans . Additionally, different imaging modalities such as contrast enhancing T1 (ceT1) or high-resolution T2 (hrT2) images can result in a large domain shift between images . The categorization of the domain adaptation approach taken is based on factors such as label availability, presence of cross-modality, and model type. For example, domain adaptation can be supervised, semi-supervised and unsupervised depending on label availability and the use of ground truth data during the domain adaptation model training . Domain adaptation is important in the field of medical image analysis because the domain shift problem can greatly impact predictive model efficacy on unseen image domains . An example of the efficacy of GANs for medical domain adaptation was highlighted in the Cross-Modality Domain Adaptation for Medical Image Segmentation (CrossMoDa) 2021 challenge, where GAN-style domain adaptation approaches were effective in domain adaptation between ceT1 and hrT2 style brain MR scans for Vestibular Schwannoma and Choclea segmentation ."
"Failure to Achieve Domain Invariance with Domain Generalization Algorithms: An Analysis in Medical Imaging","https://scispace.com/paper/failure-to-achieve-domain-invariance-with-domain-3nvnqfdn","2023","Journal Article","IEEE Access","","10.1109/access.2023.3268704","https://scispace.com/pdf/failure-to-achieve-domain-invariance-with-domain-3nvnqfdn.pdf","One prominent issue in the application of deep learning is the failure to generalize to data that lies on a different distribution to the training data. While many methods have been proposed to address this, prior work has shown that when operating under the same conditions most algorithms perform almost equally. As such, more work needs to be done to validate past and future methods before they are put into important scenarios like medical imaging. Our work analyses eight domain generalization algorithms across four important medical imaging classification datasets along with three standard natural image classification problems to discover the differences in how these methods operate in these different contexts. We assess these algorithms in terms of generalization capability, domain invariance, and representational sensitivity. Through this, we show that despite the differences between domain and content variations between natural and medical imaging there is little deviation in the operation of each method between natural images and medical images. Additionally, we show that all tested algorithms retain significant amounts of domain-specific information in their feature representations despite explicit training to remove it. Thus, revealing the failure point of all these methods is a lack of class-discriminative features extracted from out-of-distribution data. While these results show that methods that work well on natural imaging work similarly in medical imaging, no method outperforms baseline methods, highlighting the continuing gap of achieving adequate domain generalization. Similarly, the results also question the efficacy of optimizing for domain invariant representations as a method for generalizing to unseen domains. ","features then how important are domain invariant representations to domain generalization? Secondly, why do methods that aim to find domain invariant representations fail to do so? Both of which need to be answered if progress is to be made in the domain generalization field. VI. FUTURE WORK:
 This work shows that in terms of medical imaging, there needs to be a deeper dive into the explicit differences between domains for each modality. The domain differences in CT and MRI scans appears to be more related to the image reconstruction process, which may alter fine details and textures structurally, as opposed to pure style. Additionally, there are many more medical imaging modalities, which requires an open-access dataset for verifying domain generalization algorithms, such as ultrasounds, PET scans, visible spectrum photographs (for identifying skin issues for example), and X-Rays. With the wider variety of datasets also leads to a wider variety of tasks, pure image classification is limited compared to the vast number of tasks medical practitioners require; image segmentation being a significant example. Possibly the largest factor that needs to be explored is the impact of these methods on 3D datasets. While this is not within the scope of this work (as a comparison of natural imaging to medical imaging using the same methods), a majority of important medical imaging modalities are natively 3D, as such understanding if algorithms require significant changes to adapt from 2D to 3D is necessary. VI. FUTURE WORK:
 As stated in the previous section, this work has discovered that the most noticeable section of failure with current domain generalization algorithms appears to be the poor extraction of features that can be used to separate testing domain data, as opposed to ill-fitted classifiers. As such work could be focused on designing new methods which aim to generate a larger set of useful features. Likewise in terms of feature extraction, this work has raised questions regarding the importance of domain invariant features. As most methods appear to not be generating entirely domain invariant features to begin with, while those with fewer domain specific features perform equivalently regardless. Hence future work can be aimed at dissecting the importance of domain invariance in domain generalization applications. VI. FUTURE WORK:
 Similarly, the purpose of this work is to prompt the creation of more analysis tools that can be used to inspect models to discover how and why they are or are not working. Current tools are limited in this aspect, as what differences between models that can be found using current understandings appear to be limited. VI. FUTURE WORK:
 As stated in II-B there are more techniques which cannot be evaluated under the premise of domain invariant representations (such as those that leverage domain-specific components - ) and thus cannot be evaluated fairly under the framework proposed in this work. Future work should be performed in evaluating the generalization mechanisms of these methods, which may lead to insights into how important domain specific information is in the generalization process in contrast to this work's focus on domain invariant information. APPENDIX A TRAINING HYPER-PARAMETERS:
 Shared hyper-parameters: Learning rate: 5e-5, dropout: 0.0, weight decay: 0.0. APPENDIX A TRAINING HYPER-PARAMETERS:
 IRM: IRM lambda: 100.0, IRM penalty anneal iterations: 500."
"Detection and Diagnosis of Diabetic Retinopathy Using Transfer Learning Approach","https://scispace.com/paper/detection-and-diagnosis-of-diabetic-retinopathy-using-3bsd51ka","2023","Journal Article","International Journal of Intelligent Engineering and Systems","","10.22266/ijies2023.0630.05","https://scispace.com/pdf/detection-and-diagnosis-of-diabetic-retinopathy-using-3bsd51ka.pdf","","Title: Detection and Diagnosis of Diabetic Retinopathy Using Transfer Learning Approach Authors: M Vamsi,B Srinivasa Rao Keywords: Fundus images, Transfer learning, Multi-class classification, Hyperparameters"
"Knowledge-Informed Machine Learning for Cancer Diagnosis and Prognosis: A review","https://scispace.com/paper/knowledge-informed-machine-learning-for-cancer-diagnosis-and-16ncdvdo16","2024","Journal Article","arXiv.org","Lingchao Mao
Hairong Wang
Leland S. Hu
Nhan Tran
Peter D Canoll
Kristin R. Swanson
Jing Li","10.48550/arxiv.2401.06406","https://scispace.compdf/knowledge-informed-machine-learning-for-cancer-diagnosis-and-16ncdvdo16.pdf","Cancer remains one of the most challenging diseases to treat in the medical field. Machine learning has enabled in-depth analysis of rich multi-omics profiles and medical imaging for cancer diagnosis and prognosis. Despite these advancements, machine learning models face challenges stemming from limited labeled sample sizes, the intricate interplay of high-dimensionality data types, the inherent heterogeneity observed among patients and within tumors, and concerns about interpretability and consistency with existing biomedical knowledge. One approach to surmount these challenges is to integrate biomedical knowledge into data-driven models, which has proven potential to improve the accuracy, robustness, and interpretability of model results. Here, we review the state-of-the-art machine learning studies that adopted the fusion of biomedical knowledge and data, termed knowledge-informed machine learning, for cancer diagnosis and prognosis. Emphasizing the properties inherent in four primary data types including clinical, imaging, molecular, and treatment data, we highlight modeling considerations relevant to these contexts. We provide an overview of diverse forms of knowledge representation and current strategies of knowledge integration into machine learning pipelines with concrete examples. We conclude the review article by discussing future directions to advance cancer research through knowledge-informed machine learning.","on a small dataset to perform the target task. Transfer learning: Transfer learning proves particularly valuable in scenarios where there is limited labeled data for the target task, and a substantial amount of labeled data is accessible for other related tasks or from a similar domain. Transfer learning:
 Incorporating the concept of self-supervised learning, which leverages unlabeled data to learn meaningful representations, can further enhance the efficacy of transfer learning. Transfer learning:
 Self-supervised learning techniques, such as contrastive learning or predictive modeling, enable a model to understand and interpret the structure of unlabeled data by predicting missing parts or identifying similarities and differences within the data. Transfer learning:
 This approach can complement traditional transfer learning by providing a richer and more diverse set of feature representations derived from unlabeled datasets, which are often more readily available and scalable. Transfer learning:
 Thus, self-supervised pre-training, followed by transfer learning, offers a robust framework for leveraging both labeled and unlabeled data, significantly enhancing the model's performance, especially in domains where labeled data is scarce or expensive to obtain. Transfer learning:
 In cases where the auxiliary dataset lacks labels, unsupervised models like autoencoders can be employed for pre-training  , along with self-supervised techniques to maximize the utility of available data. Transfer learning:
 Transfer learning enables flexible transfer of knowledge learnt in different domains. Knowledge can be transferred across patients with the same disease, where a model trained on a comprehensive dataset of historical patients captures population-level patterns, and a smaller, patient-specific dataset is used to bias the model for each individual  . Transfer learning:
 Similarly, knowledge can be transferred across different diseases. Garcia et al.  conducted pre-training of a CNN model using gene expression profiles encompassing over 30 cancer types and fine-tuned the model on data specific to lung cancer patients. Knowledge can even be transferred across different data modalities. Transfer learning:
 For example, a model trained on commonly used imaging modalities (e.g., T1-Gd MRI) can be transferred to modalities with less training data  . Transfer learning:
 This has proven benefits in medical imaging applications where patients may undergo varying numbers of imaging exams based on factors like physician preference, cost considerations, and center availability. Transfer learning:
 Moreover, models trained for different ML tasks (e.g., tumor detection, tumor classification, tumor segmentation) using the same dataset can leverage shared learned feature representations  . Transfer learning:
 Large scale pre-trained language models, including BERT  , have gained popularity in various domains. Biomedical variations of BERT, such as BioBERT  and BioMegatron 26 trained on biomedical research articles, RadBERT 209 trained on radiology reports, and ClinicalBERT 27 on clinical notes, have been developed. Transfer learning:
 These pre-trained models are later fine-tuned using in-domain data, such as electronic health records from a small group of patients  . Some studies  even demonstrated value of pre-training CNN models on non-medical images from ImageNet. Transfer learning:
 While pre-training is typically seen in studies that use public large-scale datasets  , other types of transfer learning such as instance transfer and feature transfer are more used for unpublic datasets  . Transfer learning:
 The extent of knowledge transferred can be adaptive, contingent on the knowledge-or data-driven similarity between domains or patients. An effective transfer learning algorithm should determine which knowledge, and from which source domains or patients, is transferrable to the target domain or patient."
"Identifying Condition-action Statements in Medical Guidelines: Three Studies using Machine Learning and Domain Adaptation","https://scispace.com/paper/identifying-condition-action-statements-in-medical-4eajutngjy","2021","Posted Content","","Hossein Hematialam
Wlodek Zadrozny","10.21203/RS.3.RS-500521/V1","https://scispace.com/pdf/identifying-condition-action-statements-in-medical-4eajutngjy.pdf","
 Background: Medical guidelines provide the conceptual link between a diagnosis and a recommendation. They often disagree on their recommendations. There are over thirty five thousand guidelines indexed by PubMed, which creates a need for automated methods for analysis of recommendations, i.e., recommended actions, for similar conditions. Results: This article advances the state of the art in text understanding of medical guidelines by showing the applicability of transformer-based models and transfer learning (domain adaptation) to the problem of finding condition-action and other conditional sentences. We report results of three studies using syntactic, semantic and deep learning methods, with and without transformer-based models such as BioBERT and BERT. We perform in depth evaluation on a set of three annotated medical guidelines. Our experiments show that a combination of machine learning domain adaptation and transfer can improve the ability to automatically find conditional sentences in clinical guidelines. We show substantial improvements over prior art (up to 25%), and discuss several directions of extending this work, including addressing the problem of paucity of annotated data.Conclusion: Modern deep learning methods, when applied to the text of clinical guidelines, yield substantial improvements in our ability to find sentences expressing the relations of condition-consequence, condition-action and action.","but at different granularity, namely on classes of combined action and conditional sentences (CCA+A). Analysis of medical guidelines: As we show later in Section 6.1, our current methods provide about 5% -11% improvement over these results. Analysis of medical guidelines:
 Our own earlier work  reported lower results than . The difference was due to our using of completely automated feature selection when training on an annotated corpus, and not relying on manually created extraction rules. In addition, the results in  demonstrate recalls on specific patterns. Thus, if applied to all activities in their annotated corpus, their recall was shown to be 56%, and on our annotated corpus, it was 39%. As we show later in this article in Sections 5 and 6.1, we can achieve the F1 scores of 81% and higher, using completely automated methods; even purely transfer-based methods can produce a 67% F1 score and 68% recall. Deep learning methods, domain adaptation and transfer learning:
 There are plenty of overviews of deep learning methods, e.g. . In our experiments we use pretrained and transformer models such as BERT  and BioBERT , which are relatively well-known. However, we feel the need to discuss the concepts of transfer learning and domain adaptation, which are a focus of some experiments reported in this article. Deep learning methods, domain adaptation and transfer learning:
 We will start by observing that the two concepts overlap and are often used interchangeably. In particular, in natural language processing, as observed by , transfer learning is sometimes referred to as domain adaptation. Wikipedia tries to make a distinction. It explains that the basic idea of domain adaptation is to learn a model on a dataset, in a way that would make it applicable in other, related situations. For example, adapting spam filtering models from one set of users to another. [3] On the other hand, transfer learning ""focuses on storing knowledge gained while solving one problem and applying it to a different but related problem,"" for example, ""knowledge gained while learning to recognize cars could apply when trying to recognize trucks"". [4]  For the purpose of this article, we adopt the definition from a 2015 survey of the topic : ""domain adaptation is a subcategory of transfer learning. In domain adaptation, the source and target domains all have the same feature space (but different distributions); in contrast, transfer learning includes cases where the target domain's feature space is different from the source feature space or spaces"" (our emphasis). We can contrast with this the traditional machine learning which generally assumes that the data is in the i.i.s. form (independent and identically distributed), and that from sampled, labeled data we can train a good model for test data. Deep learning methods, domain adaptation and transfer learning:
 Domain adaptation and transfer learning are very active areas of research . We also observe their growing importance for natural language processing  and in clinical NLP. For example,  argue that ""researchers in clinical NLP should treat domain adaptation, transfer learning, etc. as a first-class problem rather than a niche area"", and  as 'worth exploring'."
"CauDR: A Causality-inspired Domain Generalization Framework for Fundus-based Diabetic Retinopathy Grading","https://scispace.com/paper/caudr-a-causality-inspired-domain-generalization-framework-n5lxqttq2n","2023","Journal Article","arXiv.org","Hao Wei
Peilun Shi
Juzheng Miao
Minqing Zhang
Guitao Bai
Jianing Qiu
Furui Liu
Wu Yuan","10.48550/arxiv.2309.15493","https://scispace.compdf/caudr-a-causality-inspired-domain-generalization-framework-n5lxqttq2n.pdf","Diabetic retinopathy (DR) is the most common diabetic complication, which usually leads to retinal damage, vision loss, and even blindness. A computer-aided DR grading system has a significant impact on helping ophthalmologists with rapid screening and diagnosis. Recent advances in fundus photography have precipitated the development of novel retinal imaging cameras and their subsequent implementation in clinical practice. However, most deep learning-based algorithms for DR grading demonstrate limited generalization across domains. This inferior performance stems from variance in imaging protocols and devices inducing domain shifts. We posit that declining model performance between domains arises from learning spurious correlations in the data. Incorporating do-operations from causality analysis into model architectures may mitigate this issue and improve generalizability. Specifically, a novel universal structural causal model (SCM) was proposed to analyze spurious correlations in fundus imaging. Building on this, a causality-inspired diabetic retinopathy grading framework named CauDR was developed to eliminate spurious correlations and achieve more generalizable DR diagnostics. Furthermore, existing datasets were reorganized into 4DR benchmark for DG scenario. Results demonstrate the effectiveness and the state-of-the-art (SOTA) performance of CauDR.","similar strategy, Li et.al designed a novel framework to match learned feature distributions across source domains by minimizing the Maximum Mean Discrepancy (MMD) and then aligned the matched representations to a prior distribution by adversarial learning to learn universal representations. Domain Generalization:
 In addition to the above-mentioned methods taht align features, several recent works have explored aligning gradients between domains to constrain the model's learning. For example, Shi et.al  and Rame et.al  incorporated new measures to align inter-domain gradients during the training process, encouraging the model to learn invariant features among domains. Domain Generalization:
 On the other hand, DG has attracted increasing attention in the field of medical image analysis, where domain shifts are often related to variations in clinical centers, imaging protocols, and imaging devices   . Domain Generalization:
 Li et al.  proposed to learn the invariant features by restricting the distribution of encoded features to follow a predefined Gaussian distribution, while Wang et al.  developed the DoFE framework for generalizable fundus image segmentation by enriching image features with domain prior knowledge learned from multiple source domains. Domain Generalization:
 Based on Fishr , Atwany et al.  recently adopted the stochastic weight averaging densely (SWAD) technique to find flat minima during DR grading training for better generalization on fundus images from unseen datasets. Causality-inspired DG:
 Causality  is a branch of research that explores the connections between various causes and their corresponding effects, with the primary goal to comprehend the underlying mechanisms and patterns behind the occurrence of events. Theoretically, the relationship of cause → ef f ect should be the same across domains. Causality-inspired DG:
 For example, an object is considered as a cat because it has the cat's characteristics, which are consistent in different domains. Therefore, the causality-inspired model is a feasible method to tackle DG problems. By considering causality, Arjovsky et. Causality-inspired DG:
 al  proposed a new regularization term to constrain the optimal classifier (learn X → Y ) in each domain to be the same by minimizing the invariant risk, called Invariant Risk Minimization (IRM). Causality-inspired DG:
 Similarly, Chevalley et.al  developed a framework CauIRL to minimize the distributional distance between intervened batches in latent space by using MMD or CORAL technique, encouraging the learning of invariant representations across domains. However, these works mainly focus on natural image processing instead of medical image analysis. Causality-inspired DG:
 More recently, Ouyang et.al  proposed to incorporate causality into data augmentation strategy to synthesize domain-shifted training examples in an organ segmentation task, which extends the distributions of training datasets to cover the potentially unseen data distributions. Causality-inspired DG:
 As for our task, the fundus image grading is relatively more challenging due to the complicated factors associated with DR severity. Methodology:
 This section provides a detailed description of our causality-inspired framework. We first formulate the DG problem, followed by a novel causality-based perspective on the image generation process. The proposed methodology is then elaborated. Problem Formulation:
 A training image set S train is composed of image pairs x ∈ X and its label y ∈ Y, which are sampled from a joint distribution P train XY . The test image pairs are sampled from P test XY to form the set S test ."
"Unsupervised domain adaptation for the detection of cardiomegaly in cross-domain chest X-ray images","https://scispace.com/paper/unsupervised-domain-adaptation-for-the-detection-of-7aqx8fuj","2023","Journal Article","Frontiers in artificial intelligence","Patrick Thiam
Ludwig Lausser
Christopher Kloth
Daniel Blaich
Andreas Liebold
Meinrad Beer
Hans A. Kestler","10.3389/frai.2023.1056422","https://scispace.com/pdf/unsupervised-domain-adaptation-for-the-detection-of-7aqx8fuj.pdf","In recent years, several deep learning approaches have been successfully applied in the field of medical image analysis. More specifically, different deep neural network architectures have been proposed and assessed for the detection of various pathologies based on chest X-ray images. While the performed assessments have shown very promising results, most of them consist in training and evaluating the performance of the proposed approaches on a single data set. However, the generalization of such models is quite limited in a cross-domain setting, since a significant performance degradation can be observed when these models are evaluated on data sets stemming from different medical centers or recorded under different protocols. The performance degradation is mostly caused by the domain shift between the training set and the evaluation set. To alleviate this problem, different unsupervised domain adaptation approaches are proposed and evaluated in the current work, for the detection of cardiomegaly based on chest X-ray images, in a cross-domain setting. The proposed approaches generate domain invariant feature representations by adapting the parameters of a model optimized on a large set of labeled samples, to a set of unlabeled images stemming from a different data set. The performed evaluation points to the effectiveness of the proposed approaches, since the adapted models outperform optimized models which are directly applied to the evaluation sets without any form of domain adaptation.","at a great extent. However, a significant performance degradation can be observed, when the optimized architectures are applied to data sets stemming from different clinical institutions or acquired with different protocols. This is mostly due to the domain shift observed in the new data sets. . Related works:
 The assumption behind the ability of a machine learning inference model (DNNs in this specific case) to generalize to unseen samples is that both the training set and evaluation set are independent and identically distributed. However, depending on several factors such as dissimilar data recording procedures, the data distribution of the evaluation set can significantly differ from the data distribution of the training set, thus causing the observed performance degradation when evaluating the optimized model on the evaluation set. Hence, various domain adaptation (DA) approaches  have been proposed in order to specifically deal with the domain shift between the training set and the evaluation set. In this setting, the training set is sampled from a specific source domain S and the evaluation set stems from a different but related target domain T . The goal of DA approaches is to optimize a model from the source domain in such a way that it generalizes in a target domain, by minimizing the difference between the data distribution of both domains.  define three specific categories of DA approaches: first, sample-based approaches which consist of weighting individual samples from the source domain during the optimization process of a model based on the relevance of these samples for the target domain. Such approaches as data importance-weighting  or class importance-weighting  belong to this category; second, feature-based approaches which consist of optimizing domain invariant feature representations in such a way that a model trained in the source domain can easily be applied to the target domain without any significant performance degradation. This category encompasses such approaches as the Domain-Adversarial Neural Network (DANN)  or the deep reconstruction-classification network (DRCN) ; third, inference-based approaches which incorporate the adaptation procedure into the parameter optimization process through the use of specific constraints during the optimization procedure. Such approaches as Cycle Self-Training (CST)  or Minimax Entropy (MME)  belong to this category. Domain adaptation approaches have been developed and applied to the analysis of chest X-ray images in a cross-domain setting such as in the work of , where the authors propose a task-oriented unsupervised adversarial network (TUNA-Net) for pneumonia recognition in cross-domain chest X-ray images, which is basically a cycle-consistent generative adversarial network.  propose an unsupervised domain adaptation approach for the cross-domain classification of thorax diseases, characterized by the application of three specific types of constraints [Domain-Invariance (DI), Instance-Invariance (II), and Pertubation-Invariance (PI)] for the optimization of domain invariant feature representations.  propose an unsupervised adversarial domain adaptation method for multi-label classification tasks. We refer the reader to the works presented by  and  for further insights into deep learning approaches as well as DA approaches applied in the area of medical image analysis."
"Revisiting Hidden Representations in Transfer Learning for Medical
  Imaging","https://scispace.com/paper/revisiting-hidden-representations-in-transfer-learning-for-1alztmz5","2023","Posted Content","","","10.48550/arxiv.2302.08272","https://scispace.com/pdf/revisiting-hidden-representations-in-transfer-learning-for-1alztmz5.pdf","While a key component to the success of deep learning is the availability of massive amounts of training data, medical image datasets are often limited in diversity and size. Transfer learning has the potential to bridge the gap between related yet different domains. For medical applications, however, it remains unclear whether it is more beneficial to pre-train on natural or medical images. We aim to shed light on this problem by comparing initialization on ImageNet and RadImageNet on seven medical classification tasks. We investigate their learned representations with Canonical Correlation Analysis (CCA) and compare the predictions of the different models. We find that overall the models pre-trained on ImageNet outperform those trained on RadImageNet. Our results show that, contrary to intuition, ImageNet and RadImageNet converge to distinct intermediate representations, and that these representations are even more dissimilar after fine-tuning. Despite these distinct representations, the predictions of the models remain similar. Our findings challenge the notion that transfer learning is effective due to the reuse of general features in the early layers of a convolutional neural network and show that weight similarity before and after fine-tuning is negatively related to performance gains. ","self-supervised pre-training. However, this approach still falls short of using real diverse data. Even with millions of unlabeled images, it cannot fully bridge the gap between fully-supervised and self-supervised pre-training for deeper layers of a CNN. Pre-training on different data:
 Although aforementioned work in general computer vision has demonstrated the potential of synthetic and augmented data, the importance of large-scale labeled source datasets remains strong, particularly ImageNet in medical imaging  despite its out-of-domain nature for medical targets. Pre-training on different data:
 Recently,  have demonstrated that RadImageNet, a large-scale dataset of radiology images similar in size to ImageNet, outperforms ImageNet on radiology target datasets. To further these findings, we investigate the potential of pre-training on the RadImageNet on a range of modalities that were not included in  experiments, such as X-rays, dermoscopic images, and histopathological scans. Effects of transfer learning:
 Transfer learning is a useful method for studying representations and generalization in deep neural networks.  defined the generality of features learned by a convolutional layer based on their transferability between tasks. They analyzed representations learned in ImageNet models and found that the early layers form general features resembling Gabor filters and color blobs, while deeper layers become more task-specific. More recently,  studied feature reuse in medical imaging using transfer learning and found that this reuse is limited to the lowest two convolutional layers. Besides feature reuse, they demonstrated that the scaling of pre-trained weights can result in significant improvement in convergence speed. Effects of transfer learning:
 Instead of investigating the transferability of weights at different layers,  investigated the distributions of convolution filters learned by computer vision models and found that they only exhibit minor variations across various tasks, image domains, and datasets. They noted that models based on the same architecture tend to learn similar distributions when compared to each other, but differ significantly when compared to other architectures. The authors also discovered that medical imaging models do not learn fundamentally different filter distributions compared to models for other image domains. Based on these findings, they concluded that medical imaging models can be pre-trained with diverse image data from any domain. Effects of transfer learning:
 Our work extends previous studies on the effects of pre-training by characterizing the representations learned from ImageNet and RadImageNet and investigating the implications of the source dataset on the learned representations. Our results provide additional evidence that the source domain may not be of high importance for pre-training medical imaging models, as we observe that even though ImageNet and RadImageNet pre-trained models converge to distinct hidden representations, their predictions are still similar. We outline our overall method in Figure . We fine-tune publicly available pre-trained ImageNet and RadImageNet weights on medical target datasets and quantify the model similarity by comparing the network activations over a sample of images from the target datasets using two similarity measures, Canonical Correlation Analysis (CCA, Section 3.1) and prediction similarity (Section 3.2). Canonical Correlation Analysis:
 CCA  which is a statistical method used to analyze the relationship between two sets of variables."
"Bridge Segmentation Performance Gap Via Evolving Shape Prior","https://scispace.com/paper/bridge-segmentation-performance-gap-via-evolving-shape-prior-2yif760woi","2020","Journal Article","IEEE Access","Chaoyu Chen
Xin Yang
Haoran Dou
Ruobing Huang
Xiaoqiong Huang
Xu Wang
Chong Duan
Shengli Li
Wufeng Xue
Pheng-Ann Heng
Dong Ni","10.1109/ACCESS.2020.3026073","https://scispace.com/pdf/bridge-segmentation-performance-gap-via-evolving-shape-prior-2yif760woi.pdf","Deep neural networks are very compelling for medical image segmentation. However, deep models often suffer from notable performance drops in real clinical settings due to the complex appearance shift in daily scannings. Domain adaptation partially addresses the problem between imaging domains. However, it heavily depends on the expensive re-collection and re-training for domain-specific datasets and thus is not applicable to domain-agnostic images. In this paper, we propose a case adaptation strategy aiming to bridge the segmentation performance gap on domain-agnostic images. Our contribution is three-fold. First, we design a general self-supervised learning framework for case adaptation , which exploits its predictions as supervision to drive the adaptation. Without extra annotations and any burden on model complexity, the framework enables trained deep models at-hand to directly segment domain-agnostic testing images. Second, we propose a novel Evolving Shape Prior (ESP) which recursively introduces strong shape knowledge into networks and evolves with the adaptation procedure to provide adaptive supervision. ESP can stabilize self-supervised learning and guide it to move towards model convergence. Third, we perform extensive experiments on 10 datasets with different levels of difficulty and typical appearance shifts blended, proving our framework is a promising solution in reducing segmentation performance degradation. Through this work, we investigate the feasibility of case adaptation as a general strategy in enhancing the robustness of deep segmentation networks, with comprehensive analyses proving its efficacy and efficiency.","Chen,Xin Yang,Haoran Dou,Ruobing Huang,Xiaoqiong Huan,X U Wang,Chong Duan,Sheng Li,Wufeng Xue,Ann Heng,Dong Ni (Corresponding Author) I. INTRODUCTION:
 The great resurgence of deep learning brings profound and lasting impact on medical image segmentation , . However, due to the data dependency and lack of generalization ability, deep segmentation models often lose their power in practical scenarios, especially in daily clinical settings , . As shown in the left of Fig. (a), high accuracies achieved by deep learning models are often reported within a pre-defined domain S, where training, validation and testing I. INTRODUCTION:
 The associate editor coordinating the review of this manuscript and approving it for publication was Orazio Gambino . I. INTRODUCTION:
 images share a coherent appearance distribution. However, what has been required in the real clinical settings is that, the model should be independent of its source domain S and work consistently on each domain-agnostic testing image ϕ (Fig. ) which presents unpredictable appearance shift (Fig. ). The segmentation performance gap between the model development phase on S and the deployment phase on ϕ can be large and has been recognized in some recent studies - . I. INTRODUCTION:
 Domain adaptation  is an alternative approach to deal with this gap (Fig.    the assistance from a labeled or unlabeled dataset T adap . Drozdzal et al.  proposed a lightweight Fully Convolutional Network to learn to normalize the medical image appearance. However, the learned normalization is only effective for small appearance variations. Generative adversarial network (GAN)  can generate realistic style translation between two distinctive medical image domains  and thus enable the appearance harmonization. To further enhance the boundary sharpness in CT-MR translation results, segmentation based shape consistency loss was proposed in cycled GAN - . By aligning features of different domains, Kamnitsas et al.  exploited an adversarial scheme to teach the network to learn source-invariant representations for brain lesion segmentation on images from different scanners. I. INTRODUCTION:
 A similar idea also appeared in  for appearance-invariant breast cancer classification in histopathology images. Despite the effectiveness of domain adaptation, it still has two critical drawbacks in clinical scenarios. First, it only extends models in S to a fixed and pre-defined domain T (Fig. ). As shown in Fig. , there are many imaging factors in daily scanning for the same examination, such as different scanners, operators, protocols, timepoints, etc. These blended factors make every testing image present a unique appearance shift and can be domain-blinded against segmentation models. Thus, it is infeasible to clearly define a bounded domain to run domain adaptation . Second, domain adaptation greatly depends on T adap . However, for each subject, only a few images (maybe just one) would be acquired for each task . Collecting a large amount of images and labels on-site to build a T adap and then modulating the pre-trained models are impractical. Therefore, as we propose, case adaptation (Fig. ) could be a better strategy to fulfill the requirements of clinical settings. This new strategy discards T adap and focuses on directly deploying the trained deep models on each domain-agnostic image."
"From Denoising Training to Test-Time Adaptation: Enhancing Domain Generalization for Medical Image Segmentation","https://scispace.com/paper/from-denoising-training-to-test-time-adaptation-enhancing-4iuldh8bni","2023","Journal Article","arXiv.org","Ruxue Wen
Hangjie Yuan
D. Ni
Wenbo Xiao
Yaoyao Wu","10.48550/arxiv.2310.20271","https://scispace.compdf/from-denoising-training-to-test-time-adaptation-enhancing-4iuldh8bni.pdf","In medical image segmentation, domain generalization poses a significant challenge due to domain shifts caused by variations in data acquisition devices and other factors. These shifts are particularly pronounced in the most common scenario, which involves only single-source domain data due to privacy concerns. To address this, we draw inspiration from the self-supervised learning paradigm that effectively discourages overfitting to the source domain. We propose the Denoising Y-Net (DeY-Net), a novel approach incorporating an auxiliary denoising decoder into the basic U-Net architecture. The auxiliary decoder aims to perform denoising training, augmenting the domain-invariant representation that facilitates domain generalization. Furthermore, this paradigm provides the potential to utilize unlabeled data. Building upon denoising training, we propose Denoising Test Time Adaptation (DeTTA) that further: (i) adapts the model to the target domain in a sample-wise manner, and (ii) adapts to the noise-corrupted input. Extensive experiments conducted on widely-adopted liver segmentation benchmarks demonstrate significant domain generalization improvements over our baseline and state-of-the-art results compared to other methods. Code is available at https://github.com/WenRuxue/DeTTA.","Title: From Denoising Training to Test-Time Adaptation: Enhancing Domain Generalization for Medical Image Segmentation Authors: Ruxue Wen,Hangjie Yuan,Dong Ni,Wenbo Xiao,Yaoyao Wu Introduction:
 In the last decade, deep learning has been extensively studied for assisting medical image analysis, aiming to reduce doctors' workload. Medical image segmentation, a critical prerequisite for various clinical analyses, has received significant attention. Introduction:
 Many deep neural networks , represented by U-Net , demonstrate remarkable performance in various medical image segmentation tasks. However, in clinical practice, medical images of-* Corresponding author. Introduction:
 ten display distributional discrepancies due to factors such as different equipment, diverse imaging parameters, and fluctuations in signal-to-noise ratio over time. While crossmodality datasets exhibit greater domain shift (e.g., from MRI to CT), it is important to note that scenarios involving the same modality are more prevalent in clinical practice. Introduction:
 Therefore, we restrict our focus solely to discrepancies in cases involving the same modality. Such discrepancies challenge deep neural networks to generalize to unseen domains, leading to performance degradation. Introduction:
 To address this problem, domain generalization (DG) has emerged to enhance the generalization ability of deep neural networks to unseen domains. Most existing domain generalization methods  aim to achieve generalization performance in unseen domains by extracting domaininvariant features from multiple domains . Introduction:
 These methods prove ineffective when dealing with the problem of medical image segmentation due to the scarcity of available data and available data annotations . Introduction:
 In medical image segmentation, a more realistic yet challenging setting is single domain generalization (SDG), where only one single domain is available for training. For the challenging SDG problem, an intuitive solution is to increase the diversity of training data through adversarial data augmentation  or data generation . Introduction:
 However, synthesizing high-quality medical images with intricate details is challenging, and these methods often struggle to perform well in domains that differ significantly from the source domain due to the challenge in anticipating the distribution of test data . Introduction:
 In addition to data manipulation, SDG is also studied in general machine learning paradigms , such as dictionary learning , contrastive learning . Introduction:
 Furthermore, there are also several studies embarking on the exploration of leveraging selfsupervised learning, such as predicting the shuffling order of patch-shuffled images  or rotation degrees , to enhance domain generalization performance. Introduction:
 An intuitive explanation is that the self-supervised learning paradigm allows a model to learn generic features and reduces the like-lihood of overfitting to the source domain . Introduction:
 Inspired by the success of self-supervised learning for generalization, we aim to address the SDG problem in a novel way for medical image segmentation. Introduction:
 We observe that medical images often suffer from various types of noise due to limitations in imaging technology or variations in imaging protocols, which is one of the key factors contributing to domain shift . Introduction:
 Hence, properly leveraging selfsupervised denoising can disregard the noise in medical images from different domains, allowing the network to focus more on clean images. Secondly, self-supervised denoising benefits from all available raw images, thereby enhancing the feature extraction capabilities of the encoder . Introduction:
 Thirdly, the single given test data hints at its distribution, enabling us to adapt the model to each unlabeled test data at test time only. This test-time adaptation approach is compatible with solving the SDG problem ."
"DRG-Net: Interactive Joint Learning of Multi-lesion Segmentation and
  Classification for Diabetic Retinopathy Grading","https://scispace.com/paper/drg-net-interactive-joint-learning-of-multi-lesion-2403wtqr","2022","Posted Content","","","10.48550/arxiv.2212.14615","https://scispace.com/pdf/drg-net-interactive-joint-learning-of-multi-lesion-2403wtqr.pdf","Diabetic Retinopathy (DR) is a leading cause of vision loss in the world, and early DR detection is necessary to prevent vision loss and support an appropriate treatment. In this work, we leverage interactive machine learning and introduce a joint learning framework, termed DRG-Net, to effectively learn both disease grading and multi-lesion segmentation. Our DRG-Net consists of two modules: (i) DRG-AI-System to classify DR Grading, localize lesion areas, and provide visual explanations; (ii) DRG-Expert-Interaction to receive feedback from user-expert and improve the DRG-AI-System. To deal with sparse data, we utilize transfer learning mechanisms to extract invariant feature representations by using Wasserstein distance and adversarial learning-based entropy minimization. Besides, we propose a novel attention strategy at both low- and high-level features to automatically select the most significant lesion information and provide explainable properties. In terms of human interaction, we further develop DRG-Net as a tool that enables expert users to correct the system's predictions, which may then be used to update the system as a whole. Moreover, thanks to the attention mechanism and loss functions constraint between lesion features and classification features, our approach can be robust given a certain level of noise in the feedback of users. We have benchmarked DRG-Net on the two largest DR datasets, i.e., IDRID and FGADR, and compared it to various state-of-the-art deep learning networks. In addition to outperforming other SOTA approaches, DRG-Net is effectively updated using user feedback, even in a weakly-supervised manner. ","selecting major lesion parts used in specific disease grading tasks and making the model to be robust. This sets us apart with other works ; . In the experiments, we demonstrate that this tactic improves prediction performance and provide helpful explanations to the experts (Figures ). C. Transfer learning and Domain Adaptation: Domain adaptation can be regarded as a special case kind of transfer learning, thus, we first review the definitions of each to provide the differences. In the standard transfer learning setting, there are two main concepts: domain and task. While domain means the feature space of a particular dataset and the marginal probability distribution of features, task means the label space of a dataset and an specific objective function. In general, transfer learning aims to transfer the knowledge learned from a task T a on domain A (i.e. source) to another task T B on domain B (i.e target). Domain adaptation, as a special case of transfer learning, it assumes that both source and target share the same domain feature spaces and tasks and whereas only marginal distributions between the source and target domains are different. In medical image analysis (MIA), domain adaptation and transfer learning have been promisingly used for dealing with limited labeled data issue. Transfer Learning in MIA: Rather than training a network with limited training data from a target task, the network is first trained for a task with potentially larger source datasets, creating a more robust model. This pre-trained network is then down-streamed on target task. However, a large-scale analysis by Cheplygina (2019); suggests that transfer learning might be not better than random initialization for most medical tasks. Medical images are significantly different from the natural image (e.g. ImageNet dataset); therefore, features learned from the external data maybe not effective helpful for the target medical domain task. Furthermore, medical data often face to the problem of imbalanced data at both instance level and pixel level . To address those limitations, propose a novel transfer learning, termed Task Agnostic Transfer Learning (TATL), motivated by dermatologists' behavior in the skincare context. TATL is learnt by a two-stage learning step i.e., first, an attribute-agnostic network is trained, which detects all the lesion regions irrespective of their labels; then, the knowledge from this network is transferred to a set of attributespecific classifiers to label each particular region. However, the performance of DNN trained on a particular source domain and transferred to a different target domain (e.g., different vendor, acquisition parameters), can drop unexpectedly due to domain shift. . Domain Adaptation in MIA: In this settings, the networks are trained with domain adaptation constraints to address the domain-shift problem. proposed a Deep Domain Confusion (DDC) method to reduce the divergence between two distributions by minimizing the maximum mean discrepancy (MMD) loss . MMD is a nonparametric metric and can be used to compute distances between distributions as distances between mean embeddings of features. In their method, a network is trained with data from multiple distributions using a loss function that consists of both task-specific loss and MMD loss. Some studies apply adversarial optimization to remove the domain discrepancy by incorporating generative adversarial networks (GANs). GANs consists of two networks, i.e., a generator to generate a new data from a distribution and a discriminator to evaluate the new data . The two networks trained using game theory theorem, i.e. minimax. combined standard adversarial loss with the task-specific classification loss to minimize domain distances. At first, a discriminative model (e.g. CNNs) is trained using labeled data from the source domain. Then, a target encoder is learned using GANs adaptation, where discriminator aims to distinguish between encoded source and target"
"Generalizing to Unseen Domains in Diabetic Retinopathy Classification","https://scispace.com/paper/generalizing-to-unseen-domains-in-diabetic-retinopathy-xc36ym2zy2","2023","Journal Article","arXiv.org","Chamuditha Jayanga Galappaththige
Gayal Kuruppu
Muhammad Haris Khan","10.48550/arxiv.2310.17255","https://scispace.compdf/generalizing-to-unseen-domains-in-diabetic-retinopathy-xc36ym2zy2.pdf","Diabetic retinopathy (DR) is caused by long-standing diabetes and is among the fifth leading cause for visual impairment. The prospects of early diagnosis and treatment could be helpful in curing the disease, however, the detection procedure is rather challenging and mostly tedious. Therefore, automated diabetic retinopathy classification using deep learning techniques has gained interest in the medical imaging community. Akin to several other real-world applications of deep learning, the typical assumption of i.i.d data is also violated in DR classification that relies on deep learning. Therefore, developing DR classification methods robust to unseen distributions is of great value. In this paper, we study the problem of generalizing a model to unseen distributions or domains (a.k.a domain generalization) in DR classification. To this end, we propose a simple and effective domain generalization (DG) approach that achieves self-distillation in vision transformers (ViT) via a novel prediction softening mechanism. This prediction softening is an adaptive convex combination of one-hot labels with the model’s own knowledge. We perform extensive experiments on challenging open-source DR classification datasets under both multi-source and more challenging single-source DG settings with three different ViT backbones to establish the efficacy and applicability of our approach against competing methods. For the first time, we report the performance of several state-of-the-art domain generalization (DG) methods on open-source DR classification datasets after conducting thorough experiments. Finally, our method is also capable of delivering improved calibration performance than other methods, showing its suitability for safety-critical applications, including health-care. We hope that our contributions would instigate more DG research across the medical imaging community. Code is available at github.com/Chumsy0725/SPSD-ViT.","propose a new prediction softening mechanism, featuring an adaptive convex combination of zero-entropy labels with the model's own knowledge. We empirically show that it is more effective in improving the model's generalizability to unseen domains in DR classification. DG in medical image analysis::
 The distribution of data originating from different hospitals or even different sensors could be sparingly different and hence it is important that a model should generalize to a different data distribution than it is trained on. Despite carrying significant importance, DG for medical imaging analysis remains largely unexplored. DG in medical image analysis::
 Among a few DG methods,  developed a meta-leaning approach based on episodic training with task augmentation for medical image classification, and  leveraged variational encoding to realize a characteristic feature space through linear-dependency regularization. DG in medical imaging has also been explored in the context of Federated Learning (FL). DG in medical image analysis::
 To allow privacy-protected distribution of information among clients,  presented episodic learning in Continuous Frequency Space (ELCFS) approach. We note that there is very little work on studying domain generalization for DR classification. Recently,  proposed the very first approach for robustifying the model under data from unseen domains in DR classification. DG in medical image analysis::
 It achieves flatness during the training of convolutional neural network (CNN) and also employs domain-level gradient variance regularization. We also propose a new DG approach for DR categorization which transfers the model's (ViTs) full knowledge to its intermediate feature routes by a new prediction softening scheme. Preliminaries:
 DG problem settings: In the typical domain generalization (DG) setting, as outlined in , we assume access to data from a set of training (source) domains, denoted as D = {D} N n=1 . Preliminaries:
 Each domain D n represents a distribution over the input space X , and there are a total of N training domains. Preliminaries:
 From each domain D n , we sample K training images consisting of pairs of inputs x k n ∈ X and labels y k n ∈ Y, where k ranges from 1 to K. Preliminaries:
 Moreover, we assume the existence of a set of target domains, denoted as {T } T t=1 , where T is the total number of target domains and is typically 1. Preliminaries:
 The core objective in DG is to learn a function F θ : X → Y, parameterized by θ which is capable of predicting accurate labels for input data from an unseen target domain T t . Preliminaries:
 ViT-based ERM for DG: We first briefly revisit empirical risk minimization (ERM) in the context of DG and then describe the ViT-based ERM in DG for DR classification task. Preliminaries:
 We assume the availability of a loss function L that can measure the discrepancy between the predicted label and the desired label. The ERM for DG accumulates data from all training (source) domains and trains a classifier that finds a predictor by minimizing : Preliminaries:
 Where M = N × K denotes the total number of images from all training (source) domains. The work of  established that this simple ERM-based DG baseline reveals competitive performance against many preceding state-ofthe-art DG methods under a fair evaluation protocol."
"Domain Generalization for Medical Image Analysis: A Survey","https://scispace.com/paper/domain-generalization-for-medical-image-analysis-a-survey-5elgcrf06b","2023","Journal Article","arXiv.org","Jee Seok Yoon
Kwanseok Oh
Yooseung Shin
Maciej A Mazurowski
Heung-Il Suk","10.48550/arxiv.2310.08598","https://scispace.compdf/domain-generalization-for-medical-image-analysis-a-survey-5elgcrf06b.pdf","Medical Image Analysis (MedIA) has become an essential tool in medicine and healthcare, aiding in disease diagnosis, prognosis, and treatment planning, and recent successes in deep learning (DL) have made significant contributions to its advances. However, DL models for MedIA remain challenging to deploy in real-world situations, failing for generalization under the distributional gap between training and testing samples, known as a distribution shift problem. Researchers have dedicated their efforts to developing various DL methods to adapt and perform robustly on unknown and out-of-distribution data distributions. This paper comprehensively reviews domain generalization studies specifically tailored for MedIA. We provide a holistic view of how domain generalization techniques interact within the broader MedIA system, going beyond methodologies to consider the operational implications on the entire MedIA workflow. Specifically, we categorize domain generalization methods into data-level, feature-level, model-level, and analysis-level methods. We show how those methods can be used in various stages of the MedIA workflow with DL equipped from data acquisition to model prediction and analysis. Furthermore, we include benchmark datasets and applications used to evaluate these approaches and analyze the strengths and weaknesses of various methods, unveiling future research opportunities.","generalization error and using CAM to visualize the model's decision. Similarly, Wang et al. proposed a novel focal domain generalization loss and used Grad-CAM++ to visualize the pathological activity from stereo-electroencephalogram (sEEG). Transferability:
 Methods that provide interpretability in one domain may not necessarily transfer well to other domains. Since different medical imaging data and tasks may require different interpretability approaches, ensuring that interpretability methods can be effectively applied across diverse domains is challenging. Gao et al.  proposed BayeSeg for interpretable medical image segmentation. Transferability:
 One of the key advantages of BayeSeg is its ability to control the performance and interpretability tradeoff. By approximating the posterior distributions of the shape, appearance, and segmentation, BayeSeg captures the statistical relationships between these variables. Transferability:
 This statistical modeling allows users to adjust the weights of the variational loss terms in BayeSeg to prioritize different aspects of the segmentation process, allowing them to control the tradeoff between interpretability and performance. Transferability:
 Yuan et al.  proposed a method for augmenting histopathology images using text prompts (e.g., ""synthesize image of a lymph node in the style of S*""). To tackle the challenge of transferability, authors proposed to leverage text-toimage (T2I) generators as a means of enabling interpretable interventions for robust representations. Transferability:
 The authors argue that T2I generators offer unprecedented capability and flexibility in approximating image interventions conditioned on natural language prompts. By using T2I generators, the proposed method can provide a more interpretable and domain-agnostic approach that can be effectively applied across diverse domains. Causalility:
 Causality refers to the relationship between variables in a causal system, where one variable (the cause) directly affects or influences another variable (the effect). In domain generalization, causality focuses on understanding the underlying causal mechanisms that lead to the differences between source and target domains. Causalility:
 It aims to identify the causal factors invariant across different domains and responsible for the targeted MedIA task. By understanding and leveraging causality, domain generalization methods can effectively generalize the learned knowledge from a source domain to target domains with different distributions. Causalility:
 Readers are referred to a survey by Seth et al.  for a deeper insight into the causal perspective of domain generalization for general tasks. In the following paragraph, we explore several approaches of causal learning specifically designed for domain generalization for MedIA tasks. Causalility:
 Mahajan et al.  proposed a causality-aware domain generalization method for pneumonia detection using chest X-ray images. They used a causal Bayesian network to model the relationships among the domain, the image features, and the class label. Causalility:
 By explicitly modeling the causal relationships, they were able to identify the common causal features that are invariant across domains and are important for predicting the presence of pneumonia. Wang et al.  used a causal graph-based approach for Alzheimer's disease diagnosis using MRI. Causalility:
 They modeled the causal relationships among imaging sites, gender, age, and imaging features using a Structural Causal Model (SCM). By performing counterfactual inference on the model, they could generate harmonized data that simulate the imaging data as if it came from the same site."
"Unsupervised Local Discrimination for Medical Images","https://scispace.com/paper/unsupervised-local-discrimination-for-medical-images-1z13pcpll7","2023","Journal Article","IEEE Transactions on Pattern Analysis and Machine Intelligence","Chen Huai
Renzhen Wang
Xiuying Wang
Jieyu Li
Fang Qu
Hui Li
Jianhao Bai
Qicong Peng
Deyu Meng
Lisheng Wang","10.1109/tpami.2023.3299038","https://scispace.compdf/unsupervised-local-discrimination-for-medical-images-1z13pcpll7.pdf","Contrastive learning, which aims to capture general representation from unlabeled images to initialize the medical analysis models, has been proven effective in alleviating the high demand for expensive annotations. Current methods mainly focus on instance-wise comparisons to learn the global discriminative features, however, pretermitting the local details to distinguish tiny anatomical structures, lesions, and tissues. To address this challenge, in this paper, we propose a general unsupervised representation learning framework, named local discrimination (LD), to learn local discriminative features for medical images by closely embedding semantically similar pixels and identifying regions of similar structures across different images. Specifically, this model is equipped with an embedding module for pixel-wise embedding and a clustering module for generating segmentation. And these two modules are unified by optimizing our novel region discrimination loss function in a mutually beneficial mechanism, which enables our model to reflect structure information as well as measure pixel-wise and region-wise similarity. Furthermore, based on LD, we propose a center-sensitive one-shot landmark localization algorithm and a shape-guided cross-modality segmentation model to foster the generalizability of our model. When transferred to downstream tasks, the learned representation by our method shows a better generalization, outperforming representation from 18 state-of-the-art (SOTA) methods and winning 9 out of all 12 downstream tasks. Especially for the challenging lesion segmentation tasks, the proposed method achieves significantly better performance. ","mechanisms, recent work can be mainly classified into specific-propertiesbased methods , , and application-driven methods , , . Contrastive Learning in Medical Images: Specific-properties-based methods aim to fuse the characteristics specific to the input modality into contrastive learning. Contrastive Learning in Medical Images:
 For instance, considering the domain-specific knowledge that different dyes can enhance different types of tissue components, Yang et al.  separated the stain results of hematoxylin and eosin, and fused them into the design of contrastive learning. Application-driven methods take account of the properties of the processing tasks when designing models. Contrastive Learning in Medical Images:
 For instance, based on the observation that the structure information in retinal images is sensitive to the orientations and is beneficial for the recognition of retinal diseases, Li et al.  fused the rotation prediction task with contrastive learning to improve the classification of pathologic myopia. Contrastive Learning in Medical Images:
 Most of the recent medical contrastive learning methods focus on using instance discrimination to learn global features, while neglecting local details , , , . And further, the special requirements for the training data hinder the current contrastive learning from broad medical applications , , . Contrastive Learning in Medical Images:
 Comparatively, our method learns local discriminative representation with the ability to measure pixel-wise and region-wise similarity. And by incorporating the common property of intra-modality structure similarity, our model is capable of processing and analyzing images for multiple medical modalities and downstream tasks. Cross-domain Knowledge Transfer:
 The purpose of cross-domain knowledge transfer is to improve the performance of tasks in the target domain by utilizing the knowledge from related domains. According to differences in the working mechanisms, it can be divided into domain adaptation and synthetic segmentation. Cross-domain Knowledge Transfer:
 Domain adaptation aims to adjust the trained model in the source domain to be generalized in the target domain , , which effectively alleviates the degeneration of the model caused by the changing imaging conditions. Cross-domain Knowledge Transfer:
 Synthetic segmentation is a unique cross-domain knowledge transfer framework for medical images to share knowledge across modalities , , , . Cross-domain Knowledge Transfer:
 It is based on the hypothesis that images of medical modalities focusing on the same body part, such as the head CT and the head MRI, share similar organs and tissues. Based on this, synthetic segmentation usually utilizes image translation to get synthetic target modality images from source labeled images. Cross-domain Knowledge Transfer:
 After that, the synthetic images are accompanied by original real labels to train a segmentation network and this model can be directly applied to the target modality. Synthetic segmentation has been widely investigated in cross-modality pairs, such as CT-MR , , and CBCT-MR . Cross-domain Knowledge Transfer:
 Recently, CycleGANbased synthetic segmentation, which is an unsupervised framework based on unpaired images, has obtained more attention than supervised ones , , , . Cross-domain Knowledge Transfer:
 Synthetic segmentation is a cost-effective method to transfer knowledge across modalities. It demonstrates that the deep network owns a similar ability as humans to memory and transfer knowledge from familiar medical scenarios to unknown ones . Cross-domain Knowledge Transfer:
 However, it can only share knowledge among modalities with almost the same content, i.e., modalities of the same body part. For radiologists, the ability to transfer knowledge is more powerful, by which, the knowledge can still be shared across modalities with big content differences."
"Federated and Transfer Learning for Cancer Detection Based on Image
  Analysis","https://scispace.com/paper/federated-and-transfer-learning-for-cancer-detection-based-349bk0t0is","2024","Preprint","","Avital Bechar
Youssef Elmir
Yassine Himeur
Rabah Medjoudj
Abbes Amira","10.48550/arxiv.2405.20126","https://scispace.compdf/federated-and-transfer-learning-for-cancer-detection-based-349bk0t0is.pdf","This review article discusses the roles of federated learning (FL) and transfer learning (TL) in cancer detection based on image analysis. These two strategies powered by machine learning have drawn a lot of attention due to their potential to increase the precision and effectiveness of cancer diagnosis in light of the growing importance of machine learning techniques in cancer detection. FL enables the training of machine learning models on data distributed across multiple sites without the need for centralized data sharing, while TL allows for the transfer of knowledge from one task to another. A comprehensive assessment of the two methods, including their strengths, and weaknesses is presented. Moving on, their applications in cancer detection are discussed, including potential directions for the future. Finally, this article offers a thorough description of the functions of TL and FL in image-based cancer detection. The authors also make insightful suggestions for additional study in this rapidly developing area. ","approach is very useful if there is difficulty in collecting labeled medical data, which is a problematic in healthcare department. Definition and concept of transfer learning: TL aims to improve learning in a new target task through the transfer of knowledge from a related source task. Definition and concept of transfer learning:
 It involves two domains: the source domain D S with data distribution P (X S , Y S ) and the target domain D T with data distribution P (X T , Y T ). Definition and concept of transfer learning:
 The goal is to use knowledge from D S to improve the learning of the target predictive function f T (•) in D T . Definition and concept of transfer learning:
 Typically, TL uses observations from multiple source domains and tasks to learn a decision function that can be applied to the target domain and task, with the aim of increasing diversity and improving the performance of the target class . Definition and concept of transfer learning:
 This is doable by minimizing the loss on the target domain, L T , using knowledge from the source domain. Fig.  provides a visual representation of the types and the subtypes of TL methods. Fine-tuning:
 Fine-tuning adjusts a pre-trained model to perform a new target task, involving minor modifications to the model architecture and re-training the model on the target task data. Fine-tuning:
 where L T is the target domain loss, R(θ) is a regularization term, and λ is a regularization parameter. Domain Adaptation:
 Domain adaptation is a technique in machine learning that aims to enable a model trained on a source domain to perform well on a different but related target domain, especially when there is abundant labeled data in the source domain and limited or no labeled data in the target domain . Domain Adaptation:
 Domain adaptation seeks to transfer knowledge from a well-labeled source domain to a less-labeled or unlabeled target domain by minimizing both the domain discrepancy and the predictive loss on the target domain, using techniques such as discrepancy measures, adversarial training, and covariate shift adjustment. Domain Adaptation:
 Domain adaptation aims at minimizing the domain discrepancy between D S and D T and the predictive loss on the target domain D T . -Discrepancy Measures: A common approach is to use discrepancy measures such as the maximum mean discrepancy (MMD) defined as : Domain Adaptation:
 where F is a class of functions, and E denotes the expectation. -Adversarial Training: Adversarial training can be formulated as a min-max problem : Domain Adaptation:
 where D is the domain classifier, θ f are the parameters of the feature extractor, and θ d are the parameters of the domain classifier. -Covariate Shift Adjustment: Covariate shift adjustment involves re-weighting the loss function on the source domain samples as : Domain Adaptation:
 and modifying the source domain loss function accordingly. Domain Adaptation:
 The optimization problem typically involves minimizing a combined loss function that includes the predictive loss on the source domain, the domain discrepancy measure, and the predictive loss on the target domain (if labeled data is available). Knowledge Distillation:
 Knowledge distillation transfers knowledge from a complex model (teacher) to a simpler model (student), aiming to retain much of the teacher's performance."
"DeepAD: A Robust Deep Learning Model of Alzheimer's Disease Progression
  for Real-World Clinical Applications","https://scispace.com/paper/deepad-a-robust-deep-learning-model-of-alzheimer-s-disease-2p7ehvg8","2022","Posted Content","","","10.48550/arxiv.2203.09096","https://scispace.com/pdf/deepad-a-robust-deep-learning-model-of-alzheimer-s-disease-2p7ehvg8.pdf","The ability to predict the future trajectory of a patient is a key step toward the development of therapeutics for complex diseases such as Alzheimer's disease (AD). However, most machine learning approaches developed for prediction of disease progression are either single-task or single-modality models, which can not be directly adopted to our setting involving multi-task learning with high dimensional images. Moreover, most of those approaches are trained on a single dataset (i.e. cohort), which can not be generalized to other cohorts. We propose a novel multimodal multi-task deep learning model to predict AD progression by analyzing longitudinal clinical and neuroimaging data from multiple cohorts. Our proposed model integrates high dimensional MRI features from a 3D convolutional neural network with other data modalities, including clinical and demographic information, to predict the future trajectory of patients. Our model employs an adversarial loss to alleviate the study-specific imaging bias, in particular the inter-study domain shifts. In addition, a Sharpness-Aware Minimization (SAM) optimization technique is applied to further improve model generalization. The proposed model is trained and tested on various datasets in order to evaluate and validate the results. Our results showed that 1) our model yields significant improvement over the baseline models, and 2) models using extracted neuroimaging features from 3D convolutional neural network outperform the same models when applied to MRI-derived volumetric features. ","a single dense block with 16 dense layers, a leaky ReLU layer, a 3D average pooling layer, and a linear layer. This branch serves as a domain adaptation network, enabling the model to learn domain invariant imaging feature representations. Training:
 The input patient p with clinical features Clin p and a 3D magnetic resonance imaging M RI p is fed into the deep model, where Clin p enters the encoder a and M RI p enters the dense network f to respectively learn clinical embeddings a(Clin p ) and image embeddings f (M RI p ) of the patient. Subsequently, f (M RI p ) is fed into the domain adaptation network h while both extracted features, a(Clin p ) and f (M RI p ), are fed forward through the endpoint prediction network g. The parameters of each network are defined as θ a , θ f , θ g , θ h with the subscripts indicating their specific network. Training:
 The objective of our model is to train a network that performs robustly on test data from an unseen domain, even though the network is trained with a mixture of data sources. To this end, we add mutual information based loss to the objective function for training networks as proposed in . Therefore, the training procedure is to optimize the following problem: Training:
 where loss(., .) and I(., .) respectively indicate the loss function and the mutual information loss, and λ is a hyperparameter to balance the terms. Replacing the mutual information, the above equation becomes the following equation: Training:
 where L endpoint (.), L bias (.), and H(h • f (.)) respectively represent the loss of the endpoint prediction, the loss of the bias prediction, and the entropy of the bias which acts as a regularizer. The set of networks, a, f , g, and h are trained end to end, with the adversarial strategy  and gradient reversal technique  updating the image only network weights θ f , θ h . Early in learning, g • f is rapidly trained to predict the endpoints by using the bias information. Then h learns to predict the bias, and f begin to learn how to extract feature embedding invariant to the imaging domain. Inter-study biases:
 There are two common forms of domain shifts (biases) in medical image analysis among different studies: population shift and acquisition shift. The population shift occurs when cohorts of subjects exhibit varying demographic or clinical characteristics, while the acquisition shift is observed due to differences in imaging protocols, modalities or scanners. To alleviate possible inter-study biases, our model regulates an additional network to minimize the mutual information shared between the extracted feature and inter-study distribution shift we want to unlearn (see equation 2). The additional network is trained adversarially against the feature embedding network and predicts the bias distribution. At the end of learning, the domain prediction network is not able to predict the domain the image came from not because it is poorly trained, but because the feature embedding network successfully unlearns the bias information."
"Domain Adaptation and Generalization on Functional Medical Images: A
  Systematic Survey","https://scispace.com/paper/domain-adaptation-and-generalization-on-functional-medical-1ui4boam","2022","Posted Content","","","10.48550/arxiv.2212.03176","https://scispace.com/pdf/domain-adaptation-and-generalization-on-functional-medical-1ui4boam.pdf","Machine learning algorithms have revolutionized different fields, including natural language processing, computer vision, signal processing, and medical data processing. Despite the excellent capabilities of machine learning algorithms in various tasks and areas, the performance of these models mainly deteriorates when there is a shift in the test and training data distributions. This gap occurs due to the violation of the fundamental assumption that the training and test data are independent and identically distributed (i.i.d). In real-world scenarios where collecting data from all possible domains for training is costly and even impossible, the i.i.d assumption can hardly be satisfied. The problem is even more severe in the case of medical images and signals because it requires either expensive equipment or a meticulous experimentation setup to collect data, even for a single domain. Additionally, the decrease in performance may have severe consequences in the analysis of medical records. As a result of such problems, the ability to generalize and adapt under distribution shifts (domain generalization (DG) and domain adaptation (DA)) is essential for the analysis of medical data. This paper provides the first systematic review of DG and DA on functional brain signals to fill the gap of the absence of a comprehensive study in this era. We provide detailed explanations and categorizations of datasets, approaches, and architectures used in DG and DA on functional brain images. We further address the attention-worthy future tracks in this field. ","to local data. Furthermore, each source can train and prepare its model using a cheap computer system, update its data and model according to its schedule, and use a high-performing model trained based on information from multiple sources at virtually no extra cost. Federated Learning of Medical Data:
 Using a federated learning approach to analyze medical data is also motivated by the diversity of signals and images generated by different physical and biological measurement processes and devices. Brand-new measurement technologies and more accurate devices are continuously being developed in order to improve the usable information extracted from medical data. Consequently, it's challenging to design a centralized system that handles all types and variations of medical data. Federated Learning of Medical Data:
 Federated learning architectures, with flexible source updates and inter-source information flow, can significantly help with efficient and high-performance medical data analysis. Cross-Modality Generalization and Adaptation:
 As previously explained in detail, numerous definitions of domains, identified by subjects, sessions, datasets, and many more, are studied in generalization and adaptation. However, presuming domains as modalities and investigating cross-modality cases have not yet been explored in this era. In this case, it is worth noting that the data recorded via different neuroimaging techniques may differ from each other since they reflect the behavior and functionality of different sections of the brain. The incomplete knowledge of the actual brain mechanisms implies that sometimes, one type of data might be a better choice than the other to be used in a fixed manner for a specific task; for example, a model trained on EEG data might need to be applied to fMRI data for the same task. Additionally, it should be noted that operating imaging equipment is typically timeconsuming and costly, and that the data collected from multiple centers might inevitably have varying data types. It might not be affordable for them to change the equipment to meet the requirements of the data type specified for the model being used. Consequently, domain shift within different modalities is a worthy and useful problem that has to be explored deeply in medical data analysis. Of course, serious challenges in aligning data representations from different modalities need to be tackled for cross-modality studies. For this purpose to be achieved in the future, it will be necessary to develop a more precise understanding of the meaning behind captured signals, along with more interpretable and disentangled representation learning techniques. Conclusion:
 Several methods have been developed in the literature to allow medical image analysis models trained on one or more source domains to adapt to unidentified Out-Of-Distribution (OOD) target data and train generalizable models. There have been many methods developed covering various applications. This area of research has recently received considerable attention and is an essential component of the deployment of many machine-learning algorithms. We presented a systematic and comprehensive review of domain adaptation and generalization methods for functional medical images, particularly functional brain images. In addition to the fundamentals of OOD generalization and adaptation, we presented the major approaches, architectures, and essential datasets used in the field of medical image analysis. Our analysis addresses a number of potential outstanding issues and promising directions for future research to advance the field, which was discussed in the section 7."
"Few-shot learning for COVID-19 chest X-ray classification with imbalanced data: an inter vs. intra domain study","https://scispace.com/paper/few-shot-learning-for-covid-19-chest-x-ray-classification-4p6tbyygbb","2024","Journal Article","Pattern Analysis and Applications","Alejandro Galán-Cuenca
Antonio‐Javier Gallego
Marcelo Saval-Calvo
Antonio Pertusa","10.1007/s10044-024-01285-w","https://scispace.compdf/few-shot-learning-for-covid-19-chest-x-ray-classification-4p6tbyygbb.pdf","Abstract Medical image datasets are essential for training models used in computer-aided diagnosis, treatment planning, and medical research. However, some challenges are associated with these datasets, including variability in data distribution, data scarcity, and transfer learning issues when using models pre-trained from generic images. This work studies the effect of these challenges at the intra- and inter-domain level in few-shot learning scenarios with severe data imbalance. For this, we propose a methodology based on Siamese neural networks in which a series of techniques are integrated to mitigate the effects of data scarcity and distribution imbalance. Specifically, different initialization and data augmentation methods are analyzed, and four adaptations to Siamese networks of solutions to deal with imbalanced data are introduced, including data balancing and weighted loss, both separately and combined, and with a different balance of pairing ratios. Moreover, we also assess the inference process considering four classifiers, namely Histogram, k NN, SVM, and Random Forest. Evaluation is performed on three chest X-ray datasets with annotated cases of both positive and negative COVID-19 diagnoses. The accuracy of each technique proposed for the Siamese architecture is analyzed separately. The results are compared to those obtained using equivalent methods on a state-of-the-art CNN, achieving an average F1 improvement of up to 3.6%, and up to 5.6% of F1 for intra-domain cases. We conclude that the introduced techniques offer promising improvements over the baseline in almost all cases and that the technique selection may vary depending on the amount of data available and the level of imbalance. ","Alejandro Galán-Cuenca,Antonio Javier Gallego,Marcelo Saval-Calvo,Antonio Pertusa Keywords: Medical imaging, Few-shot learning, Siamese convolutional neural networks, Imbalanced classification, Transfer learning Introduction:
 Deep learning algorithms exhibit remarkable capabilities in computer-aided detection and diagnosis (CAD) across diverse applications , including disease classification , segmentation , or medical object detection such as pulmonary nodules  or lymphocytes , among others. Introduction:
 In particular, the emergence of annotated X-ray imaging datasets  has made the research of many applications based on deep neural networks possible, greatly benefiting pathology diagnosis and prognosis. Introduction:
 Nevertheless, the performance of models trained on medical images highly depends on several factors that can notably worsen the results. Key challenges include the scarcity of annotated data and the substantial cost associated with expert labeling . Introduction:
 Compared to regular datasets in computer vision, a medical image dataset usually contains relatively few images, and in some cases, only a small percentage of them are annotated by experts . In addition, there is commonly a considerable imbalance between negative (healthy) and positive (pathological) samples. Introduction:
 Moreover, generated models strongly rely on the specific domain of data for which they were trained. Introduction:
 All these challenges collectively hinder the development of effective, robust, and generalizable methods for processing medical images , and only a few approaches based on deep learning techniques are eventually certified for clinical usage . Introduction:
 A standard solution to deal with the scarcity of annotated medical imaging data due to its associated high costs is data augmentation . This technique generates synthetic samples from existing images, expanding the training dataset. Introduction:
 However, the distinctive characteristics of medical images, such as their high dimensionality, intricate structures, and substantial inter-and intra-class variability, present challenges when applying traditional data augmentation techniques . Introduction:
 Therefore, designing effective augmentation strategies for medical imaging often requires domain expertise involving radiologists or medical professionals who can provide guidance and validation. Introduction:
 Another widely adopted solution for addressing the limited availability of annotated data is using transfer learning . This technique involves leveraging knowledge acquired from a domain with sufficient labeled data and applying it to another domain by fine-tuning the model. Introduction:
 In this process, the weights of a pre-trained model are used as an initialization for a new model. Transfer learning has gained significant interest for medical imaging . Notably, it helps reduce the amount of labeled data required for training, accelerates convergence, and yields models with better generalization capabilities. Introduction:
 These generalized models can be effectively transferred to other domains, enabling inter-domain use. Introduction:
 The issue of highly imbalanced data is another common challenge in medical imaging, where the number of positive samples is often significantly lower than that of negative ones . Introduction:
 Machine learning models trained on imbalanced data tend to exhibit bias towards the majority class, not paying attention to the samples from the minority class . Introduction:
 Consequently, this leads to suboptimal performance for the underrepresented samples, which can have severe consequences in detecting specific pathologies and could represent a risk for the patients in critical scenarios. Introduction:
 A small dataset becomes even more prone to overfitting, making the model lose generalization capabilities when the training data is not large enough. Few-shot learning (FSL) algorithms address this issue. These methods can be categorized  into metric-based, optimization-based, and transfer learning-based approaches."
"Learning Generalized Medical Image Segmentation from Decoupled Feature Queries","https://scispace.com/paper/learning-generalized-medical-image-segmentation-from-61aww3gwiq","2024","Proceedings Article","Proceedings of the ... AAAI Conference on Artificial Intelligence","Qi Bi
Jingjun Yi
Hao Zheng
Wei Ji
Yawen Huang
Yuexiang Li
Yefeng Zheng","10.1609/aaai.v38i2.27839","https://scispace.compdf/learning-generalized-medical-image-segmentation-from-61aww3gwiq.pdf","Domain generalized medical image segmentation requires models to learn from multiple source domains and generalize well to arbitrary unseen target domain. Such a task is both technically challenging and clinically practical, due to the domain shift problem (i.e., images are collected from different hospitals and scanners). Existing methods focused on either learning shape-invariant representation or reaching consensus among the source domains. An ideal generalized representation is supposed to show similar pattern responses within the same channel for cross-domain images. However, to deal with the significant distribution discrepancy, the network tends to capture similar patterns by multiple channels, while different cross-domain patterns are also allowed to rest in the same channel. To address this issue, we propose to leverage channel-wise decoupled deep features as queries. With the aid of cross-attention mechanism, the long-range dependency between deep and shallow features can be fully mined via self-attention and then guides the learning of generalized representation. Besides, a relaxed deep whitening transformation is proposed to learn channel-wise decoupled features in a feasible way. The proposed decoupled fea- ture query (DFQ) scheme can be seamlessly integrate into the Transformer segmentation model in an end-to-end manner. Extensive experiments show its state-of-the-art performance, notably outperforming the runner-up by 1.31% and 1.98% with DSC metric on generalized fundus and prostate benchmarks, respectively. Source code is available at https://github.com/BiQiWHU/DFQ. ","Title: Learning Generalized Medical Image Segmentation from Decoupled Feature Queries Authors: Qi Bi,Jingjun Yi,Hao Zheng,Wei Ji,Yawen Huang,Yuexiang Li,Yefeng Zheng Introduction:
 Despite the rapid development of deep learning techniques, most existing medical image segmentation approaches assume that the training and testing samples follow the same statistical distribution. Unfortunately, this assumption may not be fulfilled in many practical medical scenarios. Introduction:
 In practice, it is notoriously taxing and expertise-demanding to annotate large amount of segmentation ground truth . In this regard, medical images are usually collected from a variety of hospitals  and annotated by different annotators with different levels of expertise . Introduction:
 Consequently, the domain shift inevitably exists among these data sources, and thus leads to the high requirement on the generalization ability for medical image segmentation models. Introduction:
 In the past few years, the area of domain adaptation for medical image segmentation has been extensively studied. Its pre-requisite is that samples from the target domain are involved in training , and can only generalize to the target domain seen in training . Introduction:
 In contrast, the domain generalization paradigm allows the learnt representation to be generalized to any unseen target domains, which significantly alleviates the aforementioned dilemma of annotated data . Introduction:
 In general, learning domain generalized medical image segmentation is both technically and clinically significant, as it predicts reliable segmentation results from a variety of scanners, annotators and hospitals. Introduction:
 Existing domain generalized medical image segmentation methods can be summarized into two categories. The one is to learn shape-invariant features from multiple source domains , and the other one is to explicitly learn the inter-domain shift among multiple source domains . Introduction:
 Unfortunately, these methods may not be able to handle the feature distribution variation on arbitrary unseen domains under different imaging conditions (e.g., illumination, image contrast, and scanning). Introduction:
 Due to the aforementioned domain shift problem, medical images from different domains may have dramatically different activation patterns among the same channel of a deep learning model (shown in red boxes of Fig. ). Introduction:
 The feature misalignment is particularly obvious for shallower features, which are more sensitive to the variation of imaging conditions. Introduction:
 To capture the required pattern in each domain, the network tends to learn a similar pattern in multiple channels, which further leads to varying degrees of feature redundancy across images from different domains (shown in blue boxes of Fig. ). Introduction:
 Feature redundancy helps the model to perform well on the training data from various domains even in the presence of single-channel mismatches, while negatively affecting the generalization ability to unseen domains. Introduction:
 In this paper, we are motivated to address the feature misalignment, which helps the models to build a more expressive cross-domain medical image representation and improves the their generalization on unseen target domains. Introduction:
 First, we propose to minimize the channel-wise correlation among cross-domain medical images, which helps remove the feature redundancy and maximize the per-channel representation ability. A more expressive per-channel and less redundant representation in the feature encoding stage in turn benefits the generalization on arbitrary unseen target domains. Introduction:
 To this end, we propose a relaxed deep whitening transformation, which can be integrated into existing deep segmentation models in a feasible and learnable fashion."
"Generalizing to Unseen Domains in Diabetic Retinopathy with Disentangled
  Representations","https://scispace.com/paper/generalizing-to-unseen-domains-in-diabetic-retinopathy-with-2217i8pp6n","2024","Preprint","","Pengcheng Xia
Hui Cai
Feilong Tang
W. Li
Wenhua Zheng
Ju Li
Peng Duan
Huaxiu Yao
Zongyuan Ge","10.48550/arxiv.2406.06384","https://scispace.compdf/generalizing-to-unseen-domains-in-diabetic-retinopathy-with-2217i8pp6n.pdf","Diabetic Retinopathy (DR), induced by diabetes, poses a significant risk of visual impairment. Accurate and effective grading of DR aids in the treatment of this condition. Yet existing models experience notable performance degradation on unseen domains due to domain shifts. Previous methods address this issue by simulating domain style through simple visual transformation and mitigating domain noise via learning robust representations. However, domain shifts encompass more than image styles. They overlook biases caused by implicit factors such as ethnicity, age, and diagnostic criteria. In our work, we propose a novel framework where representations of paired data from different domains are decoupled into semantic features and domain noise. The resulting augmented representation comprises original retinal semantics and domain noise from other domains, aiming to generate enhanced representations aligned with real-world clinical needs, incorporating rich information from diverse domains. Subsequently, to improve the robustness of the decoupled representations, class and domain prototypes are employed to interpolate the disentangled representations while data-aware weights are designed to focus on rare classes and domains. Finally, we devise a robust pixel-level semantic alignment loss to align retinal semantics decoupled from features, maintaining a balance between intra-class diversity and dense class features. Experimental results on multiple benchmarks demonstrate the effectiveness of our method on unseen domains. The code implementations are accessible on https://github.com/richard-peng-xia/DECO. ","Tang,Wenxue Li,Wenhao Zheng,Lie Ju,Peibo Duan,Huaxiu Yao,Zongyuan Ge Keywords: Diabetic Retinopathy, Domain Generalization, Disentangled Representations. ⋆ Equal Contribution Introduction:
 Diabetic Retinopathy (DR) is a diabetes-induced ocular disorder that affects the retina, which epitomizes one of the foremost causes of blindness . Typically, the diagnosis of DR is based on the presence of several key lesions, namely  The motivations of our approach. Introduction:
 Firstly, while the augmentation methods are simple visual transformations, we consider more feature-level class-agnostic latent noise, such as macular degeneration caused by age. Additionally, existing pixellevel alignment may act on features containing domain bias, replacing original features with decoupled semantic features to alleviate domain noise. Introduction:
 microaneurysms, hemorrhages, soft or hard exudates, hemorrhages, and cotton wool spots. Therefore, the grading of DR usually includes five categories: no DR, mild DR, moderate DR, severe DR, and proliferative DR . Introduction:
 Although deep learning methods have achieved promising results in grading DR , which simplifies the diagnostic process and reduces the demand for trained ophthalmologists, one major challenge they face in practical clinical applications is domain shifts , meaning there are some visual biases between training and testing data caused by factors such as imaging conditions or the ethnic of the population, as shown in Figure . Introduction:
 This leads to a decline in models' performance when these models are applied to new data from unseen domains, which is known as Domain Generalization (DG) . Introduction:
 Previous efforts have sought to learn robust domain-invariant representations, such as through flatness and regularization , and variational autoencoders . There are works from the perspective of image augmentation , using visual transformation or image degradation to simulate domain shifts. Introduction:
 However, generalized features are likely affected by domain biases unrelated to DR categories, such as macular degeneration in elderly retinas and ethnic variations in retinal structure. This renders current data augmentation schemes and direct feature learning ineffective when confronted with new unseen domains. Introduction:
 Additionally, there are works proposing pixel-level supervised losses to learn diverse intra-class features , but the targets themselves contain domain noise, which makes the representations less robust. Introduction:
 Considering these drawbacks, we propose to Decouple the rEpresentations of semantiC features from dOmain features to reduce domain bias, which we call DECO. Introduction:
 Specifically, features are decoupled into representations of semantic features (DR-related retinal semantics, e.g., microaneurysms, hemorrhages, hard/soft exudates) and domain features (domain noise, e.g., explicit noise in image style and implicit noise stemming from age, gender, and ethnicity, etc.). Introduction:
 To mitigate the impact of domain shifts, we average the domain information over examples of the same class and construct class prototype representations. Then we linearly interpolate each semantic representation using the corresponding class prototype. Introduction:
 Similarly, we interpolate domain representations with class-agnostic domain factors to improve training stability and remove noise. During the insertion process, we design data-aware weights to focus on rare classes and domains. Introduction:
 By utilizing a set of data from different domains, new representations containing the semantic features of one example and domain features from another can be reassembled for data augmentation."
"Image Registration: Fundamentals and Recent Advances Based on Deep Learning","https://scispace.com/paper/image-registration-fundamentals-and-recent-advances-based-on-4lzrcz6p03","2023","Book Chapter","Neuromethods","Min Chen
Nick Tustison
Rohit Jena
James C. Gee","10.1007/978-1-0716-3195-9_14","https://scispace.compdf/image-registration-fundamentals-and-recent-advances-based-on-4lzrcz6p03.pdf","Abstract Registration is the process of establishing spatial correspondences between images. It allows for the alignment and transfer of key information across subjects and atlases. Registration is thus a central technique in many medical imaging applications. This chapter first introduces the fundamental concepts underlying image registration. It then presents recent developments based on machine learning, specifically deep learning, which have advanced the three core components of traditional image registration methods—the similarity functions, transformation models, and cost optimization. Finally, it describes the key application of these techniques to brain disorders. ","features from both modalities. Domain Adaptation: When applied to image registration, these synthesized modalities can then be used to convert multimodal registration problems into mono-modal problems that can be solved by leveraging the efficiency and accuracy of mono-modal registration techniques. Domain Adaptation:
 Of particular note in this area are methods developed around generative adversarial networks (GANs), first introduced by Goodfellow and colleagues , which have increasingly found traction in addressing many types of deep learning problems in the medical imaging domain [41] including image registration. Domain Adaptation:
 GANs are a special type of network composed of two adversarial subnetworks known as the generator (usually characterized by deconvolutional layers) and the discriminator (usually a CNN). These work in a minimax fashion to learn data distributions in the absence of extensive sample data. Domain Adaptation:
 Seeded with a random noise image (e.g., sampled from a uniform or Gaussian distribution), the generator produces synthetic images which are then evaluated by the discriminator as belonging either to the true or synthetic data distributions in terms of some probability scalar value. Domain Adaptation:
 This back-and-forth results in a generator network which continually improves its ability to produce data that more closely resembles the true distribution while simultaneously enhancing the discriminator's ability to judge between true and synthetic data sets. Domain Adaptation:
 Since the original ""vanilla"" GAN paper, the number of proposed GAN extensions has exploded in the literature. Initial extensions included architectural modifications for improved stability in training which have since become standard (e.g., deep convolutional GANs ). Please refer to Chap. 5 for a more extensive coverage of GANs. Domain Adaptation:
 In order to constrain the mapping between moving and fixed images, the GAN-based approach outlined in [43] combines a content loss term (which includes subterms for normalized mutual information, structural similarity [44], and a VGG-based filter feature ℓ 2 -norm between the two images) with a ""cyclical"" adversarial loss. Domain Adaptation:
 This is constructed in the style of  who proposed this GAN extension, CycleGAN, to ensure that the normally underconstrained forward intensity mapping is consistent with a similarly generated inverse mapping for ""image-to-image translation"" (e.g., converting a Monet painting to a realistic photo or rendering a winter nature scene as its summer analog). Domain Adaptation:
 However, in this case, the cyclical aspect is to ensure a regularized field through forward and inverse displacement consistency. Domain Adaptation:
 The work of  employs discriminator training between finite-element modeling and generated displacements for the prostate and surrounding tissues to regularize the predicted displacement fields. Domain Adaptation:
 The generator loss employs the weakly supervised learning method proposed by the same authors in [47] whereby anatomical labels are used to drive registration during training only. The generator is constructed from an encoder/decoder architecture based on ResNet blocks . Domain Adaptation:
 The prediction framework includes both localized tissue deformation and the linear coordinate system changes associated with the ultrasound imaging acquisition. Domain Adaptation:
 In , the discriminator loss is based on quantification of how well two images are aligned where the negative cases derive from the registration generator and the positive cases consist of identical images (plus small perturbations)."
"Camera Adaptation for Fundus-Image-Based CVD Risk Estimation","https://scispace.com/paper/camera-adaptation-for-fundus-image-based-cvd-risk-estimation-2fwa5shi","2022","Book Chapter","International Conference on Medical Image Computing and Computer-Assisted Intervention","Zhihong Lin
Danli Shi
Donghao Zhang
Xuedong Shang
Mingguang He
Zongyuan Ge","10.1007/978-3-031-16434-7_57","https://scispace.com/pdf/camera-adaptation-for-fundus-image-based-cvd-risk-estimation-2fwa5shi.pdf","Recent studies have validated the association between cardiovascular disease (CVD) risk and retinal fundus images. Combining deep learning (DL) and portable fundus cameras will enable CVD risk estimation in various scenarios and improve healthcare democratization. However, there are still significant issues to be solved. One of the top priority issues is the different camera differences between the databases for research material and the samples in the production environment. Most high-quality retinography databases ready for research are collected from high-end fundus cameras, and there is a significant domain discrepancy between different cameras. To fully explore the domain discrepancy issue, we first collect a Fundus Camera Paired (FCP) dataset containing pair-wise fundus images captured by the high-end Topcon retinal camera and the low-end Mediwork portable fundus camera of the same patients. Then, we propose a cross-laterality feature alignment pre-training scheme and a self-attention camera adaptor module to improve the model robustness. The cross-laterality feature alignment training encourages the model to learn common knowledge from the same patient’s left and right fundus images and improve model generalization. Meanwhile, the device adaptation module learns feature transformation from the target domain to the source domain. We conduct comprehensive experiments on both the UK Biobank database and our FCP data. The experimental results show that the CVD risk regression accuracy and the result consistency over two cameras are improved with our proposed method. The code is available here: https://github.com/linzhlalala/CVD-risk-based-on-retinal-fundus-images ","CVD. The Cycle GAN learns some image style transformation but fails to improve the predcition outcome. The advantage of our CLFA pre-training plus camera adaptor method is that both generalization and adaptation are considered to improve the model's adaptability. Conclusion:
 This paper researches the domain discrepancy problem in developing a fundusimage-based CVD risk predicting algorithm. We observe that the deep learning model trained conventionally will have a variant representation on photos from different fundus cameras. Therefore, we propose a cross-laterality feature learning training method and a camera adaptor module. The experiments show that our design has improved on the prediction result consistency. Also, we find that the feature-space distribution discrepancy between pre-training and target domain data may be the key factor of model transportability. Future research will explore the data augmentation in adaption to overcome the data lacking and utilize the FCP data in the backbone model pre-training."
"Semi-supervised learning methods for unsupervised domain adaptation in medical imaging segmentation","https://scispace.com/paper/semi-supervised-learning-methods-for-unsupervised-domain-35yfo2bmhv","2019","Dissertation","","Pedro Ballester","","https://scispace.com/pdf/semi-supervised-learning-methods-for-unsupervised-domain-35yfo2bmhv.pdf","Machine learning applications make several assumptions regarding the scenario where they are employed. One common assumption is that data distribution in the test environment follows the same distribution of the training set. This assumption is systematically broken in most real-world scenarios; the difference between these distributions is commonly known as domain shift. Unsupervised domain adaptation aims at suppressing this problem by leveraging knowledge with unlabeled data from the test environment. One of the most sensitive fields for domain shift is medical imaging. Due to the heterogeneity in data distributions from scanners, models tend to vary in predictive performance when dealing with images from scanners with no examples in the training set. We propose two contributions in this work. First, we introduce the use of self-ensembling domain adaptation in the field of medical imaging segmentation in a spinal cord grey matter segmentation task. Next, based on the success of self-ensembling, we adapt two other recent work from the semi-supervised learning literature to the same task, namely, unsupervised data augmentation and MixMatch. We conduct ablation studies and other experiments in order to understand the behavior of each method and compare their best results. The results show improvements over training models in a supervised learning fashion and demonstrate that recent semi-supervised learning methods are promising for domain adaptation in medical imaging segmentation.","presented with their respective samples. This assumes that in order to predict properly for the target domain, high-level feature space should be similar when predicting on source and target images. This is a common premise, frequently seen in seminal papers of deep domain adaptation . Domain adaptation in medical imaging segmentation:
 There are not many popular methods designed specifically for the purpose of domain adaptation in medical imaging. Existing work focus mostly on using adversarial-based methods in a similar fashion as they were originally proposed. We discuss some of the papers and present their current efforts to solve the task. Domain adaptation in medical imaging segmentation:
 Chen et al.  developed a domain adaptation method based on CycleGANs . Their work resembles CyCADA , with the difference of also introducing a discriminator for the network output, creating what they called semantic-aware generative adversarial networks. Javanmardi and Tasdizen  use a framework very similar to domain-adversarial neural networks  that use a domain classification network with a gradient reversal layer (similar effect of a discriminator) to model a joint feature space. Some work focus on domain adaptation for cross-modality segmentation. Dou et al.  designed an adversarial-based method that learns both domain-specific feature extraction layers and a joint high-level representation space that can segment data from both MRI or CT data. Domain adaptation in medical imaging segmentation:
 Gholami et al.  proposes a biophysics-based data augmentation method to produce synthetic tumor-bearing images for the training set. The authors argue that this augmentation procedure improves model generalization. Mahmood et al.  presents a generative model that translates images from a simulated endoscopic images domain to a realistic-looking domain as data augmentation. The authors also introduce a l1-regularization loss between the translated and the original image to minimize distortions. Madani et al.  introduced a generative adversarial network with a discriminator capable of detecting cardiac disease alongside the adversarial objective. They employ their network in a semi-supervised scenario and also evaluate their model for domain adaptation with promising results. Domain adaptation in medical imaging segmentation:
 We include these studies in the related work for completeness but do not compare them in our experiment section with the methods we employ. Some methods from this section are very domain-specific and demand biology and radiology specialists to translate them for different machines and body regions, which falls out of our scope of more easily generalizable approaches. We focus only on evaluating recent work from the semi-supervised learning literature, since they were shown to produce some of the best results in domain adaptation while not relying on domain-specific knowledge nor on, what tend to be, unstable procedures, such as adversarial learning."
"Deploying deep learning models on unseen medical imaging using adversarial domain adaptation","https://scispace.com/paper/deploying-deep-learning-models-on-unseen-medical-imaging-ryjewwwl","2022","Journal Article","PLOS ONE","Aly A. Valliani
F. Gulamali
Young-Joon Kwon
Michael L Martini
Chi-Tei Wang
Douglas Kondziolka
Viola Chen
Weichung Wang
Anthony Costa
Eric K. Oermann","10.1371/journal.pone.0273262","https://scispace.com/pdf/deploying-deep-learning-models-on-unseen-medical-imaging-ryjewwwl.pdf","The fundamental challenge in machine learning is ensuring that trained models generalize well to unseen data. We developed a general technique for ameliorating the effect of dataset shift using generative adversarial networks (GANs) on a dataset of 149,298 handwritten digits and dataset of 868,549 chest radiographs obtained from four academic medical centers. Efficacy was assessed by comparing area under the curve (AUC) pre- and post-adaptation. On the digit recognition task, the baseline CNN achieved an average internal test AUC of 99.87% (95% CI, 99.87-99.87%), which decreased to an average external test AUC of 91.85% (95% CI, 91.82-91.88%), with an average salvage of 35% from baseline upon adaptation. On the lung pathology classification task, the baseline CNN achieved an average internal test AUC of 78.07% (95% CI, 77.97-78.17%) and an average external test AUC of 71.43% (95% CI, 71.32-71.60%), with a salvage of 25% from baseline upon adaptation. Adversarial domain adaptation leads to improved model performance on radiographic data derived from multiple out-of-sample healthcare populations. This work can be applied to other medical imaging domains to help shape the deployment toolkit of machine learning in medicine.","confirming that adaptation is acting upon the underlying distribution of features rather than simply recalibrating the models (Fig 3C Fig in ). For the CXR models there was a median salvage of 20.98% of AUC after adaptation. Domain spread:
 The present work displays encouraging and practically useful results across both non-medical and medical datasets for mitigating dataset shift in the challenging case of not having access to labeled data in the target domain. When dealing with easily transported data, this problem can be somewhat obviated by simply localizing the data and training models on a union of the data or utilizing other techniques from transfer learning. Importantly, however, we observe that the performance of global models on pooled data from multiple data sources does not reflect efficacy on individual data domains (Fig 4, S4 and S5 Tables in S1 File). Instead, stratifying assessment by domain allows for examination of the domain spread, or inter-domain variance, as an a priori measure of expected model performance upon deployment. The dramatic reduction in domain spread with increasing amounts of handwritten digits data (0.1% domain spread = 112.06 and 100% domain spread = 0.01) relative to that of CXR data (0.1% domain spread = 26.12 and 100% domain spread = 23.83) suggests that added radiographs may not be sufficient to overcome data shift across hospital sites. a technical framework for domain adaptation. A generator translates real data from one domain into fake data that resembles that of a different domain while the discriminator aims to distinguish between the two, which enables the generator to generate realistic-looking data in the target domain. (C) Schematic of the proposed algorithm. a) Real data from a source domain is translated by the generator to resemble data from a specified target domain while maintaining underlying semantic qualities of the input image. b) Translated data is reconstructed by the generator to resemble data from the source domain to maintain domain-agnostic image characteristics with a semantic consistency constraint ensuring that reconstructed images maintain the semantic characteristics of the source data. c) The discriminator aims to distinguish between real and synthetic images and identify the domain of input images to constrain the generator to produce realistic-looking synthetic images from a specified domain. d) A target discriminator is fine-tuned on synthetic images to better identify opacity in the target domain."
"Domain Generalization for Medical Imaging Classification with Linear-Dependency Regularization","https://scispace.com/paper/domain-generalization-for-medical-imaging-classification-57gy509bsh","2020","Proceedings Article","Neural Information Processing Systems","Haoliang Li
Yufei Wang
Renjie Wan
Shiqi Wang
Tie-Qiang Li
Alex C. Kot","","https://scispace.com/pdf/domain-generalization-for-medical-imaging-classification-57gy509bsh.pdf","Recently, we have witnessed great progress in the field of medical imaging classification by adopting deep neural networks. However, the recent advanced models still require accessing sufficiently large and representative datasets for training, which is often unfeasible in clinically realistic environments. When trained on limited datasets, the deep neural network is lack of generalization capability, as the trained deep neural network on data within a certain distribution (e.g. the data captured by a certain device vendor or patient population) may not be able to generalize to the data with another distribution. In this paper, we introduce a simple but effective approach to improve the generalization capability of deep neural networks in the field of medical imaging classification. Motivated by the observation that the domain variability of the medical images is to some extent compact, we propose to learn a representative feature space through variational encoding with a novel linear-dependency regularization term to capture the shareable information among medical data collected from different domains. As a result, the trained neural network is expected to equip with better generalization capability to the ""unseen"" medical data. Experimental results on two challenging medical imaging classification tasks indicate that our method can achieve better cross-domain generalization capability compared with state-of-the-art baselines.","either distribution alignment (e.g. Maximum Mean Discrepancy) or adversarial learning through feature level or pixel level . Recently, it has been shown that by considering pixel level adaptation and feature level adaptation together, better adaptation performance can be achieved . Related Works:
 Compared with domain adaptation, domain generalization is much more challenging, as we assume that we have no access to the target domain. Instead, we aim to train a model that is expected to be generalized to the ""unseen"" target by assuming that only multiple source domains are available. For example, Yang and Gao  proposed to leverage Canonical Correlation Analysis (CCA) to extract shareable information among domains. Muandet et al.  proposed a Domain Invariant Component Analysis (DICA) algorithm to learn an empirical mapping based on multiple sourcedomain data where the distribution mismatch across domains was minimized. This idea was further extended by  in an autoencoder framework with distribution regularization on latent space. In , the low-rank regularization based on classifier and model parameters were explored to extract universal feature representation. Ghifary et al.  proposed a multi-task autoencoder to learn domain invariant features by reconstructing the latent representation of a given sample from one domain to another domain. Motiian et al.  proposed to minimize the semantic alignment loss as well as the separation loss based on deep learning models. Carlucci et al.  proposed to shuffle the image patch to learn generalized feature representation. Recently, Wang et al.  proposed to extend MixUp  to the settings of multiple domains for heterogeneous domain generalization task. As for meta-learning based techniques, Li et al.  proposed to transfer the idea in  to the ""unseen"" target domain setting by randomly constructing meta-train and meta-test set, which was further extended by Balaji et al.  with a scheme to learn a regularization network to improve the scalability of domain generalization. Related Works:
 Cross-Domain Medical Imaging Classification. Due to the various imaging protocols, device vendors and patient populations, we may also encounter the problem of distribution shift in clinical practice. To tackle such domain shift problem, image synthesis can be adopted through Generative Adversarial Networks  to mitigate the domain shift problem. For example, Zhang et al.  proposed to leverage CycleGAN for medical imaging problem to transfer the knowledge from CT images to X-ray images. Chen et al.  conducted domain translation from MR to CT domain for heart segmentation problem. With few label information available in target domain, Zhang et al.  proposed to conduct segmentation and data synthesis jointly to segment heart chambers in both CT and MR domain. Dou et al.  proposed a two parallel domain-specific encoders and a decoder where the weights are shared between domains to boost the performance of training on both single domain and cross-domain scenario. When target domain data are not available, Zhang et al.  proposed to conduct data augmentation on source domain to achieve better generalization capability for medical imaging classification task. Yoon et al.  proposed to learn generalized feature representation through classification and contrastive semantic alignment technique . More recently, Dou et al.  proposed to conduct meta-learning with global class alignment as well as local sample clustering regularization for medical imaging classification task. Methodology:
 Preliminary. We denote the training samples from multiple source domains on a joint space X × Y as"
"Continual Learning in Medical Imaging Analysis: A Comprehensive Review of Recent Advancements and Future Prospects","https://scispace.com/paper/continual-learning-in-medical-imaging-analysis-a-gqtfkl7qks","2023","Journal Article","arXiv.org","Pratibha Kumari
Joohi Chauhan
Afshin Bozorgpour
Reza Azad
Dorit Merhof","10.48550/arxiv.2312.17004","https://scispace.compdf/continual-learning-in-medical-imaging-analysis-a-gqtfkl7qks.pdf","Medical imaging analysis has witnessed remarkable advancements even surpassing human-level performance in recent years, driven by the rapid development of advanced deep-learning algorithms. However, when the inference dataset slightly differs from what the model has seen during one-time training, the model performance is greatly compromised. The situation requires restarting the training process using both the old and the new data which is computationally costly, does not align with the human learning process, and imposes storage constraints and privacy concerns. Alternatively, continual learning has emerged as a crucial approach for developing unified and sustainable deep models to deal with new classes, tasks, and the drifting nature of data in non-stationary environments for various application areas. Continual learning techniques enable models to adapt and accumulate knowledge over time, which is essential for maintaining performance on evolving datasets and novel tasks. This systematic review paper provides a comprehensive overview of the state-of-the-art in continual learning techniques applied to medical imaging analysis. We present an extensive survey of existing research, covering topics including catastrophic forgetting, data drifts, stability, and plasticity requirements. Further, an in-depth discussion of key components of a continual learning framework such as continual learning scenarios, techniques, evaluation schemes, and metrics is provided. Continual learning techniques encompass various categories, including rehearsal, regularization, architectural, and hybrid strategies. We assess the popularity and applicability of continual learning categories in various medical sub-fields like radiology and histopathology...","Title: CONTINUAL LEARNING IN MEDICAL IMAGE ANALYSIS: A COMPREHENSIVE REVIEW OF RECENT ADVANCEMENTS AND FUTURE PROSPECTS Authors: Pratibha Kumari,Joohi Chauhan,Afshin Bozorgpour,Boqiang Huang,Reza Azad,Dorit Merhof (Corresponding Author) Keywords: Continual Learning, Medical Data Drift, Domain Shift, Concept Drift, Medical Image Analysis, Histopathology, Radiology Introduction:
 In the evolving field of medical image analysis, the dynamic nature of healthcare data poses a critical challenge for the generalizability of the machine learning/deep learning models to new data/domains ). Introduction:
 The data-driven approaches have challenges due to the limited availability and accessibility of sufficiently large and diverse medical data for training . Additionally, the source variability due to different scanner manufacturers, staining and imaging protocols, slice thickness, different patient cohorts, etc., makes the medical data heterogeneous. Introduction:
 This introduces bias and discrepancies between the training and test datasets if they originate from different data sources, thus leading to performance degradation . Introduction:
 In order to handle the model generalizability issues, domain adaptation methods become popular and aim to transfer knowledge from one domain to other unseen data sources or domains . However, domain adaptation poses unique challenges due to the sensitive and complex nature of healthcare data . Introduction:
 The most common associated issues are the limited availability of labeled medical data for training, heterogeneity in data sources contributing to significant domain shifts, clinical disparities and population variances, and inter-rater variabilities. Introduction:
 Also, biases present in the source domain data can propagate to the target domain, so ensuring fair and unbiased predictions across diverse patient populations becomes a critical concern. Introduction:
 Moreover, medical data encompasses various modalities, including imaging, electronic health records, and genomic data, making the task of adapting models to handle multimodal data and ensuring interoperability exceptionally complex. Introduction:
 Further, the accessibility of source data may be limited to a short period of time or may be prohibited altogether due to strict privacy regulations in the medical domain ). Thus, domain adaptation approaches that require simultaneous availability of source and target data may not be feasible. Introduction:
 Another related learning paradigm, transfer learning, has been widely adopted in the medical domain to address challenges related to limited data availability . It transfers knowledge gained from the source task to the target task to improve its learning or performance. Introduction:
 Unlike domain adaptation, where only the data distribution changes, transfer learning covers changes in the feature space, label space, as well as in the data distribution of the source and target domain . Introduction:
 In a small-scale medical disease classification dataset, it can be beneficial to include knowledge gained from a model trained on a large-scale labeled natural image dataset (ImageNet). Introduction:
 The model performance on the medical disease dataset may be better as compared to training the same model solely on the medical disease dataset from scratch. Introduction:
 However, at the same time the performance on the ImageNet dataset cannot be guaranteed by this model (which is also not intended in transfer learning). In transfer learning, the focus is on leveraging prior knowledge rather than retaining it, hence performance on the source data may be compromised."
"What Makes Transfer Learning Work For Medical Images: Feature Reuse &
  Other Factors","https://scispace.com/paper/what-makes-transfer-learning-work-for-medical-images-feature-2ly62719","2022","Posted Content","","","10.48550/arxiv.2203.01825","https://scispace.com/pdf/what-makes-transfer-learning-work-for-medical-images-feature-2ly62719.pdf","Transfer learning is a standard technique to transfer knowledge from one domain to another. For applications in medical imaging, transfer from ImageNet has become the de-facto approach, despite differences in the tasks and image characteristics between the domains. However, it is unclear what factors determine whether - and to what extent - transfer learning to the medical domain is useful. The long-standing assumption that features from the source domain get reused has recently been called into question. Through a series of experiments on several medical image benchmark datasets, we explore the relationship between transfer learning, data size, the capacity and inductive bias of the model, as well as the distance between the source and target domain. Our findings suggest that transfer learning is beneficial in most cases, and we characterize the important role feature reuse plays in its success. ","reuse is difficult to measure precisely, we examine it from multiple perspectives through a series of experiments. We find that when transfer learning works well: (1) weight statistics cannot account the majority of the gains (2) evidence for feature reuse is strongest. Introduction:
 Our findings do not contradict those of , rather, we show that they uncovered an isolated case (* in Figure ) where feature reuse is less important: a large dataset, distant from IMAGENET. In this scenario, transfer learning yields only marginal benefits which can largely be attributed to the weight statistics. Our work paints a more complete picture, considering datasets with more variety in size and distance to the source domain, and concludes that feature reuse plays an important role in nearly all cases. Introduction:
 We add to this picture with the finding that vision transformers (ViTs), a rising class of models with fewer inductive biases , show a strong dependence on feature reuse in all the datasets we tested. We select four families of CNNs and ViTs with progressively stronger inductive biases and find that models with less inductive bias rely more heavily on feature reuse. Moreover, the pattern of feature reuse changes in models with less inductive bias. Specifically, feature reuse in ViTs is concentrated in early layers, whereas CNNs reuse features more consistently throughout the network. Introduction:
 We share the code to reproduce our experiments, available at github.com/ChrisMats/feature-reuse. Introduction:
 A limitation of  was that they only considered CNNs applied to CHEXPERT, one of the largest publicly available medical imaging datasets (and a similarly large private retinal image dataset in ). Problem Formulation and Methodology:
 The aim of this work is to examine transfer learning from the natural to the medical image domain. To investigate these questions, we conduct a series of experiments considering a variety of medical image datasets, initialization strategies, and architectures with different levels of inductive bias. We also perform several ablation studies to characterize feature reuse at different depths throughout each network. The details of our methodology are described below. Problem Formulation and Methodology:
 Datasets. We select datasets that help us characterize how the efficacy of transfer learning varies with properties of the data. For the source domain, we use IMAGENET throughout this work. For the target domain, we select a representative set of five standard medical image classification datasets. They cover a variety of imaging modalities and tasks, ranging from a few thousand examples to the largest public medical imaging datasets. Problem Formulation and Methodology:
 • APTOS2019 (N = 3, 662) High-resolution diabetic retinopathy images where the task is classification into 5 categories of disease severity . Problem Formulation and Methodology:
 • CBIS-DDSM (N = 10, 239) A mammography dataset in which the task is to detect the presence of masses . Problem Formulation and Methodology:
 • ISIC 2019 (N = 25, 331) Dermoscopic images -the task is to classify among 9 different diagnostic categories of skin lesions . Problem Formulation and Methodology:
 • CHEXPERT (N = 224, 316) Chest X-rays with labels over 14 categories of diagnostic observations . Problem Formulation and Methodology:
 • PATCHCAMELYON (N = 327, 680) Patches of H&E stained WSIs of lymph node sections. The task is to classify each patch as cancerous or normal ."
"A Novel Distant Domain Transfer Learning Framework for Thyroid Image Classification","https://scispace.com/paper/a-novel-distant-domain-transfer-learning-framework-for-u6da16u7","2022","Journal Article","Neural Processing Letters","Fenghe Tang
Jianrui Ding
Lingtao Wang
Chun-ping Ning","10.1007/s11063-022-10940-4","https://scispace.com/pdf/a-novel-distant-domain-transfer-learning-framework-for-u6da16u7.pdf","Medical ultrasound imaging technology is currently the preferred method for early diagnosis of thyroid nodules. Radiologists' analysis of ultrasound images is highly dependent on their clinical experience and is susceptible to intra- and inter-observer variability. Although end-to-end deep learning technique can address these limitations, the difficulty of acquiring annotated medical image makes it very challenging. Transfer learning can alleviate the problems, but the large gap between source and target domain will lead to negative transfer. In this paper, a novel transfer learning method with distant domain high-level feature fusion (DHFF) model is proposed. It reduces the distribution distance between the source domain and the target domain while maintaining the characteristics of respective domains, which can avoid excessive feature fusion while enabling the model to learn more valuable transfer knowledge. The DHFF is validated by multiple public source and private target datasets in experiments. The results show that the classification accuracy of DHFF is up to 88.92% with thyroid ultrasound auxiliary source domains, which is up to 8% higher than existing transfer and distant transfer algorithms. ","Title: A Novel Distant Domain Transfer Learning Framework for Thyroid Image Classification Authors: Fenghe Tang,Jianrui Ding,Lingtao Wang,Chunping Ning Keywords: Transfer learning, Thyroid image classification, Distant domain, Feature Fusion Introduction:
 Ultrasound imaging technology is a non-invasive, non-radiation, low cost and real time detection method. It has widely used in the detection of thyroid, fetal, mammary gland and gonadal tissue . However, due to its low contrast, Manual image analysis is time-consuming and laborious. At the same time, it will be affected by subjective factors such as radiologists' experience and mental state, which is prone to misdiagnosis. Automatic medical image analysis techniques can effectively overcome the above limitations. Generally, it can be roughly classified into two main categories: hand-crafted feature-based methods and the data-driven methods. The pipeline of hand-craft feature methods frequently involves feature extraction and classification. Despite their rapid development in recent years, handcrafted features are highly dependent on expert knowledge. Moreover, handcrafted features in some sense ignore high-level abstract information that is not visible to the human eye in ultrasound images and it can only exploit the low-level information, such as image texture , geometry morphology , and statistical distributions . Such methods usually require further employ classifiers to conduct classification. Therefore, only given the highly discriminative features, this method can solve the recognition problem well. On the contrary, data-driven methods, without the need of hand-crafted feature description, can greatly improve the classification performance of medical images by using the convolution neural networks (CNNs) . At present, deep learning technology in data-driven approaches has become the mainstream method of image analysis and understanding . It depends on large-scale labeled training data. However, compared with natural scene images, the collection and annotation of medical images is difficult and expensive, which brings great challenges to the application of deep learning technology in medical domain. Transfer learning  is one of the effective methods to solve small data learning, which has been widely used in medical image analysis . Theoretically, it attempts to build a robust model by transferring the knowledge learned from the source domain with large-scale training data to the target domain with a small amount of data. However, low or even irrelevance between the source and target domains may lead to negative transfer , which causes the knowledge generated by the source domain negatively affects the target domain. How to transfer the knowledge beneficial to medical image analysis from the source domain is a challenging problem. In this paper, we propose a method to transfer valuable knowledge from distant domains which have low correlation or even seemingly unrelated with target domain, and apply it to thyroid ultrasound image classification."
"Using Out-of-the-Box Frameworks for Contrastive Unpaired Image Translation for Vestibular Schwannoma and Cochlea Segmentation: An Approach for the CrossMoDA Challenge","https://scispace.com/paper/using-out-of-the-box-frameworks-for-contrastive-unpaired-26deoc56","2022","Book Chapter","","Peter N. Van Buren","10.1007/978-3-031-09002-8_44","https://scispace.com/pdf/using-out-of-the-box-frameworks-for-contrastive-unpaired-26deoc56.pdf","The purpose of this study is to apply and evaluate out-of-the-box deep learning frameworks for the crossMoDA challenge. We use the CUT model, a model for unpaired image-to-image translation based on patchwise contrastive learning and adversarial learning, for domain adaptation from contrast-enhanced T1 MR to high-resolution T2 MR. As data augmentation, we generate additional images with vestibular schwannomas with lower signal intensity. For the segmentation task, we use the nnU-Net framework. Our final submission achieved mean Dice scores of 0.8299 in the validation phase and 0.8253 in the test phase. Our method ranked 3rd in the crossMoDA challenge. ","adversarial learning, to adapt ceT1 domain hrT2 domain. For the segmentation task in the hrT2 domain, we utilize nnU-Net § , a framework that showed state-of-the-art performance in multiple medical image segmentation challenges . Related Work:
 Over the past few years, deep learning has been widely used for medical image segmentation, and many papers have shown great success in various tasks on different modalities . However, most of the recent high-performing deep learning models are based on supervised learning which often requires a large amount of carefully labeled data. The collection and annotation of data is especially challenging for medical image segmentation because of the high cost of pixel-level labeling by experts and heterogeneous nature of medical data. Thus, there has been many research efforts on learning with limited supervision although they have not been as successful as fully supervised learning . c Domain adaptation (DA) is a popular subcategory of transfer learning that tackles limited supervision by utilizing labeled data in source domains to execute tasks in a target domain. Unsupervised domain adaptation (UDA) refers to a domain adaptation task where only labeled data from the source domain and none from the target domain are available. Feature alignment is an approach for UDA that learns domain-invariant feature distributions across domains. One method of aligning feature spaces is through minimizing the discrepancy between the distributions based on measurements such as maximum mean discrepancy  and correlation alignment . Also, the features spaces can be aligned via adversarial learning commonly based on domain classifier . Another line of research in UDA is the alignment of input spaces instead of features that makes use of unsupervised image-to-image translation. The popular strategy in this field is the cycle-consistency constraint of CycleGAN  that inspired many networks such as UNIT  and U-GAT-IT , where bi-directional image translations are learned by two GANs. Some works address a one-sided image translation by utilizing some kind of content loss between domains. Benaim et al.  propose DistanceGAN that learns image translation by pairwise distances matching between images within domains. Fu et al.  propose GCGAN to preserve the predefined geometric transformation between the input images before and after translation. Recently, Park et al.  propose CUT to maximize the mutual information between the translations based on patchwise contrastive learning. Few studies have assessed application of CUT in medical image analysis. In this study, we use CUT to translate from ceT1 to hrT2 images. Methods:
 Since we focus on applications of the publicly available frameworks, there is no modification to the mathematical setting or algorithm of the original works. All implementations were performed with PyTorch  (version 1.7.1) on Nvidia RTX 3090 GPUs (single GPU training). Data:
 The official training set includes ceT1 images with segmentation labels from 105 patients and hrT2 images without labels from a separate set of 105 patients. The VS (label 1) and cochlea (label 2) were manually segmented in consensus by the treating neurosurgeon and physicist using both the ceT1 and hrT2 images . As stated in the official challenge rules, no additional data was included for training. The official validation set and test set include hrT2 MR images of 32 patients and 138 patients, respectively. The test phase is evaluated privately by the challenge organizers based on submissions using Docker containers during the evaluation phase of the challenge."
"SSiT: Saliency-guided Self-supervised Image Transformer for Diabetic
  Retinopathy Grading","https://scispace.com/paper/ssit-saliency-guided-self-supervised-image-transformer-for-3efsnh2a","2022","Posted Content","","","10.48550/arxiv.2210.10969","https://scispace.com/pdf/ssit-saliency-guided-self-supervised-image-transformer-for-3efsnh2a.pdf","Self-supervised learning (SSL) has been widely applied to learn image representations through exploiting unlabeled images. However, it has not been fully explored in the medical image analysis field. In this work, we propose Saliency-guided Self-Supervised image Transformer (SSiT) for diabetic retinopathy (DR) grading from fundus images. We novelly introduce saliency maps into SSL, with a goal of guiding self-supervised pre-training with domain-specific prior knowledge. Specifically, two saliency-guided learning tasks are employed in SSiT: (1) We conduct saliency-guided contrastive learning based on the momentum contrast, wherein we utilize fundus images' saliency maps to remove trivial patches from the input sequences of the momentum-updated key encoder. And thus, the key encoder is constrained to provide target representations focusing on salient regions, guiding the query encoder to capture salient features. (2) We train the query encoder to predict the saliency segmentation, encouraging preservation of fine-grained information in the learned representations. Extensive experiments are conducted on four publicly-accessible fundus image datasets. The proposed SSiT significantly outperforms other representative state-of-the-art SSL methods on all datasets and under various evaluation settings, establishing the effectiveness of the learned representations from SSiT. The source code is available at https://github.com/YijinHuang/SSiT. ","Yijin Huang,Junyan Lyu,Pujin Cheng,Roger Tam,Xiaoying Tang,) Yijin Huang Keywords: Diabetic retinopathy, Fundus image, Saliency map, Self-supervised learning, Vision transformer I. INTRODUCTION:
 Diabetic retinopathy (DR) is one of the microvascular complications of diabetes and the leading cause of blindness in the working-age population of developed countries . Delayed treatment may induce irreversible vision impairments and malfunctions. DR biomarkers can be identified from fundus images, including hemorrhages, exudates, microaneurysms, and retinal neovascularization. However, due to the nearly imperceptible early pathological signs and the rapid increase in the number of patients with diabetes, DR screening is time-consuming and labor-intensive, even for well trained clinicians. Therefore, automated DR detection methods are desired to reduce the number of untreated patients and the burden on clinicians, especially in regions with limited medical resources. I. INTRODUCTION:
 During the past decade, deep learning has achieved great success in the DR detection realm . Specifically, various convolutional neural networks (CNNs) have been proposed for automated DR grading - . Recently, vision transformers (ViTs)  have further boosted the performance of deep learning, exhibiting prominence on a variety of image benchmark datasets - . ViTs are generally more datahungry than common CNNs , , and thus large-scale datasets with high-quality annotations are required to train ViTs well. However, annotating medical images is extremely time-intensive and error-prone, exerting a heavy burden to clinical experts. I. INTRODUCTION:
 Self-supervised learning (SSL) has been explored to learn representations from images with no annotations. Contrastive learning -  is one of the promising SSL paradigms, wherein differently augmented views are created from the same images, and then representations are learned by maximizing the similarity between features from those different views. SSL has successfully established its effectiveness in computer vision, but the medical image analysis realm has not fully benefited from such advances yet, mainly because of the giant domain gap between natural images and medical images. In natural images, salient objects generally occupy a large portion and their characteristics are discriminative (e.g. shape and color). In contrast, medical images of the same modality have similar anatomy and intensity profiles, being inadequate for disease discrimination. In such context, contrastive SSL approaches that highly rely on global informatics are prone to learn feature representations that are useful for contrastive tasks but not sufficiently discriminative for downstream medical tasks. Furthermore, medical images (e.g. fundus images) may have various diagnostic features (e.g. lesions) dispersed throughout the entire image. Therefore, local fine-grained information is highly crucial for medical image based disease discrimination."
"Cardiac Magnetic Resonance Left Ventricle Segmentation and Function Evaluation Using a Trained Deep-Learning Model","https://scispace.com/paper/cardiac-magnetic-resonance-left-ventricle-segmentation-and-fsd9zxd0","2022","Journal Article","Applied Sciences","Fumin Guo
Matthew Ng
Idan Roifman
Graham A. Wright","10.3390/app12052627","https://scispace.com/pdf/cardiac-magnetic-resonance-left-ventricle-segmentation-and-fsd9zxd0.pdf","Cardiac MRI is the gold standard for evaluating left ventricular myocardial mass (LVMM), end-systolic volume (LVESV), end-diastolic volume (LVEDV), stroke volume (LVSV), and ejection fraction (LVEF). Deep convolutional neural networks (CNNs) can provide automatic segmentation of LV myocardium (LVF) and blood cavity (LVC) and quantification of LV function; however, the performance is typically degraded when applied to new datasets. A 2D U-net with Monte-Carlo dropout was trained on 45 cine MR images and the model was used to segment 10 subjects from the ACDC dataset. The initial segmentations were post-processed using a continuous kernel-cut method. The refined segmentations were employed to update the trained model. This procedure was iterated several times and the final updated U-net model was used to segment the remaining 90 ACDC subjects. Algorithm and manual segmentations were compared using Dice coefficient (DSC) and average surface distance in a symmetric manner (ASSD). The relationships between algorithm and manual LV indices were evaluated using Pearson correlation coefficient (r), Bland-Altman analyses, and paired t-tests. Direct application of the pre-trained model yielded DSC of 0.74 ± 0.12 for LVM and 0.87 ± 0.12 for LVC. After fine-tuning, DSC was 0.81 ± 0.09 for LVM and 0.90 ± 0.09 for LVC. Algorithm LV function measurements were strongly correlated with manual analyses (r = 0.86–0.99, p < 0.0001) with minimal biases of −8.8 g for LVMM, −0.9 mL for LVEDV, −0.2 mL for LVESV, −0.7 mL for LVSV, and −0.6% for LVEF. The procedure required ∼12 min for fine-tuning and approximately 1 s to contour a new image on a Linux (Ubuntu 14.02) desktop (Inter(R) CPU i7-7770, 4.2 GHz, 16 GB RAM) with a GPU (GeForce, GTX TITAN X, 12 GB Memory). This approach provides a way to incorporate a trained CNN to segment and quantify previously unseen cardiac MR datasets without needing manual annotation of the unseen datasets.","that deep-learning models trained on one domain (source) do not generalize well to a new domain (target) and direct application of a pre-trained model to a new dataset often yields degraded performance because of the well-known domain shift issue facing the community. Introduction:
 To facilitate translation of this important tool for widespread use in research and clinical care, it is urgently required to improve the generalizability of deep-learning methods to datasets collected using different imaging settings on different systems at various locations in patients with distinct diseases. Domain adaptation  aims to address this issue by fine-tuning a pre-trained model using a small amount of labeled data from a target domain, or by learning domain-invariant features or transforming data from the target domain to resemble the source domain. For example, previous studies  fine-tuned a pre-trained model using manually annotated datasets for new cardiac MRI segmentation tasks in a supervised manner. Other studies employed adversarial learning to transform data in a one domain (source) to resemble data in another domain (target) at the image level  or image-and-feature level  for unsupervised domain-adaptation-based cardiac image segmentation. Data augmentation  represents a very different approach to solving this problem by artificially enlarging the training datasets through extensive transformations to train a model that is robust to potential variations in new domains. Although commonly used classical data augmentation techniques (e.g., geometrical transformation, noise, contrast and blurring perturbation, histogram equalization and matching) have been widely used in various applications, other advanced and extensive augmentation techniques have also demonstrated effectiveness in addressing the domain shift issue . In particular, recent studies using advanced data augmentation techniques demonstrated higher performance than several adversarial learning-based domain-adaptation methods for several medical image segmentation tasks ."
"High-Precision Segmentation of Buildings with Small Sample Sizes Based on Transfer Learning and Multi-Scale Fusion","https://scispace.com/paper/high-precision-segmentation-of-buildings-with-small-sample-13ljgia3","2023","Journal Article","Remote sensing","Xiaobin Xu
Hao Zhang
Yingying Ran
Zhiying Tan","10.3390/rs15092436","https://scispace.com/pdf/high-precision-segmentation-of-buildings-with-small-sample-13ljgia3.pdf","In order to improve the accuracy of the segmentation of buildings with small sample sizes, this paper proposes a building-segmentation network, ResFAUnet, with transfer learning and multi-scale feature fusion. The network is based on AttentionUnet. The backbone of the encoder is replaced by the ResNeXt101 network for feature extraction, and the attention mechanism of the skip connection is preserved to fuse the shallow features of the encoding part and the deep features of the decoding part. In the decoder, the feature-pyramid structure is used to fuse the feature maps of different scales. More features can be extracted from limited image samples. The proposed network is compared with current classical semantic segmentation networks, Unet, SuUnet, FCN, and SegNet. The experimental results show that in the dataset selected in this paper, the precision indicators of ResFAUnet are improved by 4.77%, 2.3%, 2.11%, and 1.57%, respectively, compared with the four comparison networks.","extraction of information on buildings from images, large numbers of building samples are required in the network-training processes. However, in real-worldapplication scenarios, the sample data sizes are often limited, leading to unsatisfactory learning results when using semantic segmentation networks. Introduction:
 Obtaining strong network performances with limited data-set capacity is a common problem. At present, an intuitive solution is to use transfer learning, which involves the transfer of the pre-training weight of related fields and fine-tuning the network. In medical imaging, label scarcity is a common problem. Therefore, transfer learning has been widely applied in the field of medical imaging, in the form of by domain adaptation (DA). This is a transfer-learning method in which the source task is the same as the target task, but the data distribution of the source domain is different from that of the target domain. In recent years, scholars have proposed a series of DA methods applicable to the medical field. Khan et al.  pre-trained the VGG network on the ImageNet dataset and then fine-tuned it using small amounts of labeled magnetic resonance imaging (MRI) data for the classification of Alzheimer's disease (AD). Gu et al.  proposed a two-stage adaptation approach based on an intermediate domain for use in situations in which there are insufficient target samples with which to fine-tune the model directly. All these examples are domain-adaptive methods under supervision. Since data-set annotation is time-consuming, scholars have proposed a series of unsupervised domain-adaptive (UDA) methods. Wollmann et al.  proposed an UDA method based on style transfer learning. They first used CycleGAN to transform whole-slide images (WSIs) of lymph nodes from a source domain to the target domain. Next, DenseNet was used to classify breast cancers. However, due to the lack of class-level joint distribution in adversarial learning, the aligned distribution is not discriminative. In order to solve this problem, Liu et al.  proposed a novel margin-preserving self-paced contrast learning (MPSCL) model to facilitate the classification of the alignment of perceptual features through crossdomain contrast learning, which effectively improved the segmentation performance of the network. Yao et al.  proposed an unsupervised domain-adaptive framework. The framework achieves full image alignment to alleviate the domain-offset problem, and introduces 3D segmentation in domain-adaptive tasks to maintain semantic consistency at deep levels. In some areas of confidential medical imaging, source datasets are often unavailable. To solve this problem, Liu et al.  proposed a passive semantic segmentation adaptive framework, which can use knowledge migration to obtain knowledge in the source domain from existing source models, and then achieve domain adaptation. Stan et al.  developed an UDA algorithm that does not require access to source-domain data during target adaptation. This method encodes the source-domain information as an internal distribution, which is used to guide the adaptation in the absence of source samples. The successful application of transfer learning in the medical field also provides a new approach to the high-precision extraction of information on buildings with small sample sizes. Introduction:
 To address the challenge of achieving high-precision building segmentation with neural networks with small sampling sizes, this paper proposes a segmentation network, ResFAUnet, based on transfer learning and multi-scale fusion. The contributions of this paper are as follows:"
"Enhancing Unsupervised Domain Adaptation via Semantic Similarity Constraint for Medical Image Segmentation","https://scispace.com/paper/enhancing-unsupervised-domain-adaptation-via-semantic-15n55x08","2022","Proceedings Article","Proceedings of the Thirty-First International Joint Conference on Artificial Intelligence","T. Hu
Shiliang Sun
Jing Zhao
Dongyu Shi","10.24963/ijcai.2022/426","https://scispace.com/pdf/enhancing-unsupervised-domain-adaptation-via-semantic-15n55x08.pdf","This work proposes a novel unsupervised cross-modality adaptive segmentation method for medical images to tackle the performance degradation caused by the severe domain shift when neural networks are being deployed to unseen modalities. The proposed method is an end-2-end framework, which conducts appearance transformation via a domain-shared shallow content encoder and two domain-specific decoders. The feature extracted from the encoder is enhanced to be more domain-invariant by a similarity learning task using the proposed Semantic Similarity Mining (SSM) module which has a strong help of domain adaptation. The domain-invariant latent feature is then fused into the target domain segmentation sub-network, trained using the original target domain images and the translated target images from the source domain in the framework of adversarial training. The adversarial training is effective to narrow the remaining gap between domains in semantic space after appearance alignment. Experimental results on two challenging datasets demonstrate that our method outperforms the state-of-the-art approaches.","part of our method, and we also study its effectiveness. In comparison, we remove this loss term in the overall loss term and train again. We can see a reduction of 3.9% in the average Dice compared to the original model. Conclusion:
 We proposed a novel unsupervised domain adaptation medical image segmentation algorithm that combines both unpaired image transformation and domain invariant feature alignment learning. Domain invariant features are directly enhanced by the proposed SSM module. Extensive experiments with promising results on two challenging datasets show the effectiveness of our approach. At present, all of the experiments only use 2D volume slices, and it is also possible to extend our method to 3D volumes. From a practical perspective, our method has the potential to be extended to evaluate the quality of cross-domain image translation, e.g., via scoring the segmentation predictions of real and translated images by the discriminator. These could be explored in our future work. Acknowledgments:
 This work was supported by the NSFC Projects 62076096 and 62006078, the Shanghai Municipal Project 20511100900, Shanghai Knowledge Service Platform Project ZF1213, STCSM Project 22ZR1421700 and the Fundamental Research Funds for the Central Universities."
"A Review of Causality for Learning Algorithms in Medical Image Analysis","https://scispace.com/paper/a-review-of-causality-for-learning-algorithms-in-medical-11nwrqsl","2022","Journal Article","","","10.59275/j.melba.2022-4gf2","https://scispace.com/pdf/a-review-of-causality-for-learning-algorithms-in-medical-11nwrqsl.pdf","Medical image analysis is a vibrant research area that offers doctors and medical practitioners invaluable insight and the ability to accurately diagnose and monitor disease. Machine learning provides an additional boost for this area. However, machine learning for medical image analysis is particularly vulnerable to natural biases like domain shifts that affect algorithmic performance and robustness. In this paper we analyze machine learning for medical image analysis within the framework of Technology Readiness Levels and review how causal analysis methods can fill a gap when creating robust and adaptable medical image analysis algorithms.&lt;br&gt;We review methods using causality in medical imaging AI/ML and find that causal analysis has the potential to mitigate critical problems for clinical translation but that uptake and clinical downstream research has been limited so far. ","discovery of medical biomarkers in brain MRI volumes. Generative methods: Finally ) use deep diffusion model to ask counterfactual questions and generate hard to obtain medical scans. These, in turn, are used to augment existing datasets for other downstream tasks. Domain Generalization:
 One of the most promising areas where causal reasoning can be applied in the field of medical imaging is Domain Adaptation and Out-of-Distribution detection, directly associated with TRL 6.4. If we model the generative process that results in a medical image and include factors like the medical history, the disease, imaging domain, etc. we can then go on and interpret domain generalization and adaptation as a model that is able to perform well under different treatments in the imaging domain parameter, as argued by . In their paper Huang et al model domain adaptation as a non stationary change in the underlying causal graph and propose methods to identify and resolve these changes.  analyze the domain shifts experienced in clinical deployment of AIML algorithms from a causal perspective and then proceed to investigate and benchmark eight popular methods of domain generalization. They find that domain generalization methods fail to provide any improvement in performance over empirical risk minimization in situations where we find sampling bias. Similarly  model the causal relationships leading to the medical images and create synthetic datasets in order to evaluate the transportability of methods to external settings where interventions on factors like ages, sex and medical metrics have been performed.  apply a causal analysis on the problem of domain generalization in segmentation of medical images. They first simulate shifted domain images via a randomly weighted shallow network; then they intervene upon the images such that spurious correlations are removed and finally train their segmentation model while enforcing a domain invariance condition.  develop a method to reuse adversarial mask discriminators for test-time training to combat distribution shifts in medical image segmentation tasks. In their discussion of their method they explain the good performance of their method under a causal lens. Finally  build a causal Bayesian prior to aide MRI tissue segmentation to generalize across different medical centers."
"CCT-Net: Category-Invariant Cross-Domain Transfer for Medical Single-to-Multiple Disease Diagnosis","https://scispace.com/paper/cct-net-category-invariant-cross-domain-transfer-for-medical-3jf5lvrked","2021","Proceedings Article","International Conference on Computer Vision","Yi Zhou
Lei Huang
Tao Zhou
Ling Shao","","https://scispace.com/pdf/cct-net-category-invariant-cross-domain-transfer-for-medical-3jf5lvrked.pdf","","image-level disease categories are known). We propose a category-invariant crossdomain transfer method to learn the knowledge from the source domain task and improve both the classification and localization performance of the target domain task. The main contributions of this work are as follows: Diabetic Retinopathy Diagnosis:
 1. A domain-specific task learning module is designed to learn domain-invariant features while preserving the disease discrepancy between two domains. A CWP global pooling method is proposed to obtain better class activation maps (CAMs) than other global pooling operations. Diabetic Retinopathy Diagnosis:
 2. Conditioned on the coarse heatmaps of different categories, we propose CIFR blocks to construct the CCT-Net for localizing more discriminative regions of the corresponding diseases. The category-invariant characteristic enables the transferability from the source domain to target domain. Moreover, such refined features can contribute to the final classification performance as well. Diabetic Retinopathy Diagnosis:
 3. Experimental evaluations are conducted in two popular medical imaging tasks. First, we extend DR diagnosis on fundus images to the diagnosis of multiple ocular dis-eases, such as glaucoma and hypertension. Second, glioma segmentation on brain MRI scans is exploited to improve the segmentation and classification performance of other tumors, such as meningioma and pituitary tumors. Experimental results demonstrate the effectiveness of our method. Medical Disease Diagnosis Scenarios:
 Deep neural networks have achieved significant success in the diagnosis of numerous individual diseases from medical imaging data. For example, in fundus imaging for ocular diseases, DR and glaucoma have been widely explored, with tasks including DR grading , DR lesion semantic segmentation , and glaucoma detection . In chest X-rays for thoracic diseases, pneumonia  and tuberculosis  identification models have been developed and used in clinical applications. In brain MRI, researchers are most interested in glioma segmentation, and the BraTS  competition is organized annually to provide a platform for contributing to the community. Moreover, in lung CT, many well-developed lung nodule  and pneumonia  detection systems have achieved satisfactory performance and reached radiologist level. However, these single-disease diagnosis models have limited transferability to multiple diseases and usually require new annotations. Multi-disease diagnosis systems are more practical. Cross Domain Transfer Learning:
 Domain adaptation (DA)  is a way of transfer learning which deals with scenarios in which a model trained on a source distribution is used in the context of a different (but related) target distribution. DA methods aim to learn domain-invariant representations to address domain shifts. Adversarial networks  and diverse variants  based on the adversarial strategy have been widely explored for domain alignment. For example, the domainadversarial neural network (DANN)  introduces a confusion loss to match the distributions of the source and target domains in order to confuse the high-level classification layers. Meanwhile, maximum classifier discrepancy (MCD)  was presented to utilize task-specific decision boundaries to align distributions. Except for the adversarial approach, divergence-based DA methods  aim to minimize the divergence criterion between the source and target domains, while batch normalization  parameters have been used to model domain-specific information. DA has also been addressed by adopting auxiliary reconstruction tasks  to create a shared representation for each of the domains. Most of these methods define DA to be a problem in which the task space is similar to the source space, with the only difference being the input domain divergence. However, disease discrepancy exists in our task."
"Robust multi-view approaches for retinal layer segmentation in glaucoma patients via transfer learning","https://scispace.com/paper/robust-multi-view-approaches-for-retinal-layer-segmentation-20lohb1m","2023","Journal Article","Quantitative imaging in medicine and surgery","Mateo Gende
Joaquim de Moura
José Ignacio Fernández-Vigo
Jose M. Martinez-de-la-Casa
Julian Garcia-Feijoo
Jorge Novo
Marcos Ortega","10.21037/qims-22-959","https://scispace.com/pdf/robust-multi-view-approaches-for-retinal-layer-segmentation-20lohb1m.pdf","Background Glaucoma is the leading global cause of irreversible blindness. Glaucoma patients experience a progressive deterioration of the retinal nervous tissues that begins with a loss of peripheral vision. An early diagnosis is essential in order to prevent blindness. Ophthalmologists measure the deterioration caused by this disease by assessing the retinal layers in different regions of the eye, using different optical coherence tomography (OCT) scanning patterns to extract images, generating different views from multiple parts of the retina. These images are used to measure the thickness of the retinal layers in different regions. Methods We present two approaches for the multi-region segmentation of the retinal layers in OCT images of glaucoma patients. These approaches can extract the relevant anatomical structures for glaucoma assessment from three different OCT scan patterns: circumpapillary circle scans, macular cube scans and optic disc (OD) radial scans. By employing transfer learning to take advantage of the visual patterns present in a related domain, these approaches use state-of-the-art segmentation modules to achieve a robust, fully automatic segmentation of the retinal layers. The first approach exploits inter-view similarities by using a single module to segment all of the scan patterns, considering them as a single domain. The second approach uses view-specific modules for the segmentation of each scan pattern, automatically detecting the suitable module to analyse each image. Results The proposed approaches produced satisfactory results with the first approach achieving a dice coefficient of 0.85±0.06 and the second one 0.87±0.08 for all segmented layers. The first approach produced the best results for the radial scans. Concurrently, the view-specific second approach achieved the best results for the better represented circle and cube scan patterns. Conclusions To the extent of our knowledge, this is the first proposal in the literature for the multi-view segmentation of the retinal layers of glaucoma patients, demonstrating the applicability of machine learning-based systems for aiding in the diagnosis of this relevant pathology.","images that display the visual features that are not present in the original images. Furthermore, more specific approaches could be taken to this approach targeting the specific morphology of the OD, using the information contained in the A-scans to better delineate this region. Discussion:
 In order to validate the use of transfer learning that was used to leverage the data that was available, an experiment was performed in which the models were trained without an initialisation to the pre-trained weights of the baseline. A comparison between these models and the ones that were pre-trained can be found in Table . These results indicate that transfer learning can be especially useful for the least represented classes, when compared with the models that were not initialised to the baseline. These results show that after a pre-training on a source domain such as peripapillary radial scans belonging to patients suffering from other diseases, transfer learning can be used to increase the performance in the similar target domain of OCT images acquired with other scanning patterns such as circular or macular cube of patients suffering from glaucoma."
"Transductive Transfer Learning for Domain Adaptation in Brain Magnetic Resonance Image Segmentation.","https://scispace.com/paper/transductive-transfer-learning-for-domain-adaptation-in-aozn2xvt78","2021","Journal Article","Frontiers in Neuroscience","Kaisar Kushibar
Mostafa Salem
Mostafa Salem
Sergi Valverde
Alex Rovira
Joaquim Salvi
Arnau Oliver
Xavier Lladó","10.3389/FNINS.2021.608808","https://scispace.com/pdf/transductive-transfer-learning-for-domain-adaptation-in-aozn2xvt78.pdf","Segmentation of brain images from Magnetic Resonance Images (MRI) is an indispensable step in clinical practice. Morphological changes of sub-cortical brain structures and quantification of brain lesions are considered biomarkers of neurological and neurodegenerative disorders and used for diagnosis, treatment planning, and monitoring disease progression. In recent years, deep learning methods showed an outstanding performance in medical image segmentation. However, these methods suffer from generalisability problem due to inter-centre and inter-scanner variabilities of the MRI images. The main objective of the study is to develop an automated deep learning segmentation approach that is accurate and robust to the variabilities in scanner and acquisition protocols. In this paper, we propose a transductive transfer learning approach for domain adaptation to reduce the domain-shift effect in brain MRI segmentation. The transductive scenario assumes that there are sets of images from two different domains: (1) source-images with manually annotated labels; and (2) target-images without expert annotations. Then, the network is jointly optimised integrating both source and target images into the transductive training process to segment the regions of interest and to minimise the domain-shift effect. We proposed to use a histogram loss in the feature level to carry out the latter optimisation problem. In order to demonstrate the benefit of the proposed approach, the method has been tested in two different brain MRI image segmentation problems using multi-centre and multi-scanner databases for: (1) sub-cortical brain structure segmentation; and (2) white matter hyperintensities segmentation. The experiments showed that the segmentation performance of a pre-trained model could be significantly improved by up to 10%. For the first segmentation problem it was possible to achieve a maximum improvement from 0.680 to 0.799 in average Dice Similarity Coefficient (DSC) metric and for the second problem the average DSC improved from 0.504 to 0.602. Moreover, the improvements after domain adaptation were on par or showed better performance compared to the commonly used traditional unsupervised segmentation methods (FIRST and LST), also achieving faster execution time. Taking this into account, this work presents one more step toward the practical implementation of deep learning algorithms into the clinical routine.","artefacts that may appear during intensity transformations that reduce the image quality. Moreover, it was shown ) that approaches such as standardising images using the Nyúl histogram matching or mixing datasets from different domains during training cannot overcome the effect of the domain-shift. INTRODUCTION:
 More complex Generative Adversarial Networks (GAN)  based approaches have also been introduced for translating images into a new target domain. However, most of the works in the literature propose synthesising images from a different imaging modality. For example, , utilise CycleGAN framework to generate CT images from MRI to allow splenomegaly segmentation without using manual annotation on CT. Also,  proposed a modified CycleGAN approach for multi-organ segmentation on X-ray images using Digitally Reconstructed Radiographs by performing pixel-to-pixel style transfer from one modality to another. Although such approaches have shown promising results, there is still a lack of GAN based methods for singlemodality image harmonisation. INTRODUCTION:
 Some feature-level domain adaptation methods have also been proposed in recent years. Such methods employ a transductive learning strategy for domain adaptation. In the transductive scenario, the images without expert annotations from unseen domain are included in the training process with the aim to minimise the domain-shift effect. Adversarial training of the network is a well-known transductive learning method. Similarly to GAN architectures, the training strategy consists of two network paths: one for classifying the input patch, and another to force the network to learn domain-invariant features by discriminating source and target domains. Recent work of  utilises an adversarial training approach for unsupervised domain adaptation from Gradient Echo images to Susceptibility Weighted Images for brain lesion segmentation task. Moreover, an adversarial domain adaptation from Whole Slide pathology to Microscopy images has been studied in .  proposed simultaneous image to image translation and domain alignment between CT and MRI images using a modification of a CycleGAN for cardiac and abdominal multi-organ segmentation. However, more investigation is needed for the adversarial training for domain adaptation for a scenario where the domain difference is subtle-i.e., multi-site and single-modality images. INTRODUCTION:
 There are some drawbacks of GAN based and adversarial training strategies. These methods are usually formulated as a competition between two agents: discriminator and segmenter . In general, the objective for the latter can vary according to the task (e.g., it is called generator for image synthesis), but in most cases the objective of the former is to differentiate between two distributions. In this non-convex minmax formulation, the training of the network can be difficult and unstable, which requires a careful selection of architecture, weight initialisation, and hyper-parameter tuning . For example,  proposed an adversarial approach for single modality domain adaptation with flip-label technique where the labels of the discriminator model were partly inverted during training to minimise over-fitting. INTRODUCTION:
 Other feature-level transductive domain adaptation methods perform domain distribution discrepancy minimisation to learn domain-invariant features. Most of the advancements of such approaches are done for computer vision with natural images . However, only a few works have been proposed in medical imaging field for single-modality images. One of the recent domain adaptation approaches is the work of  for multi-site brain multiple sclerosis lesion segmentation. The authors adopted a joint distribution optimal transport framework proposed in  to compare the source and target distributions and bring them closer in a feature-level."
"SSiT: Saliency-guided Self-supervised Image Transformer for Diabetic Retinopathy Grading","https://scispace.com/paper/ssit-saliency-guided-self-supervised-image-transformer-for-zolo00v5","2022","Journal Article","arXiv.org","Yijin Huang
Junyan Lyu
Pujin Cheng
Roger Tam
Xiaoying Tang","10.48550/arXiv.2210.10969","https://scispace.compdf/ssit-saliency-guided-self-supervised-image-transformer-for-zolo00v5.pdf","Self-supervised learning (SSL) has been widely applied to learn image representations through exploiting unlabeled images. However, it has not been fully explored in the medical image analysis field. In this work, we propose Saliency-guided Self-Supervised image Transformer (SSiT) for diabetic retinopathy (DR) grading from fundus images. We novelly introduce saliency maps into SSL, with a goal of guiding self-supervised pre-training with domain-specific prior knowledge. Specifically, two saliency-guided learning tasks are employed in SSiT: (1) We conduct saliency-guided contrastive learning based on the momentum contrast, wherein we utilize fundus images' saliency maps to remove trivial patches from the input sequences of the momentum-updated key encoder. And thus, the key encoder is constrained to provide target representations focusing on salient regions, guiding the query encoder to capture salient features. (2) We train the query encoder to predict the saliency segmentation, encouraging preservation of fine-grained information in the learned representations. Extensive experiments are conducted on four publicly-accessible fundus image datasets. The proposed SSiT significantly outperforms other representative state-of-the-art SSL methods on all datasets and under various evaluation settings, establishing the effectiveness of the learned representations from SSiT. The source code is available at https://github.com/YijinHuang/SSiT.","characteristics. F. Generalizability on Other Retinal Tasks: SSiT demonstrates its ability to acquire semantic information of DR-related diagnostic regions. In this section, we show that SSiT can also learn general feature representations for diverse retinal tasks. F. Generalizability on Other Retinal Tasks:
 To analyze the generalizability of learned features, we evaluate our proposed method on two distinct downstream retinal tasks, i.e., age-related macular degeneration (AMD) and pathologic myopia (PM) diagnoses. Typical signs of AMD in fundus images include drusen, exudation and hemorrhage, while those of PM include atrophy and lacquer crack. F. Generalizability on Other Retinal Tasks:
 For evaluation purposes, we employ two publicly available datasets, namely Ichallenge-AMD  and Ichallenge-PM , which are specifically and respectively associated with AMD and PM. Both datasets contain 400 fundus images with normal and abnormal annotations, each of which is randomly split into 70%/10%/20% for training/validation/testing. F. Generalizability on Other Retinal Tasks:
 As tabulated in Table , SSiT consistently outperforms all other ViT-based SSL methods by at least 9.76% on Ichallenge-AMD and 0.99% on Ichallenge-PM. Please note that the task of Ichallenge-PM is relatively simple, so there is not much room for improvement. F. Generalizability on Other Retinal Tasks:
 These experimental results suggest that the saliency-guided feature representations learned from fundus images not only benifit DR grading but also facilitate the diagnoses of other retinal diseases. In essence, SSiT effectively learns generalized representations from fundus images, which can be applied to diverse retinal diagnostic tasks. V. DISCUSSION:
 Motivated by its huge success in computer vision, SSL recently has been actively explored in medical image analysis. However, unlike natural images in computer vision, medical images are much more expensive to collect, more complicated, and more informative. V. DISCUSSION:
 Therefore, the general SSL approaches in computer vision may be incompatible or perform unsatisfactorily for medical image analysis. Here, we emphasize the importance and effectiveness of incorporating prior knowledge into SSL for medical images, such as the saliency information that provides diagnostic characteristics for downstream DR grading in this work. V. DISCUSSION:
 Introducing prior knowledge into SSL may enhance disease-related features in the learned representations and alleviate the demanding requirement of a large number of samples in the pre-training step, thereby improving the feasibility and generalizability of self-supervised learning in the medical image analysis realm. V. DISCUSSION:
 Note that with an extra pre-training dataset, as tabulated in Table , our proposed SSiT achieves SOTA DR grading performance on the DDR dataset which has an official test split. V. DISCUSSION:
 Although we have only validated the effectiveness of SSiT in learning representations from fundus images, our method can be easily transferred to other types of medical images since fine-grained information is very important and is hold in almost all types of medical imaging data, which will be one of our future extensions."
"Federated learning for diagnosis of age-related macular degeneration","https://scispace.com/paper/federated-learning-for-diagnosis-of-age-related-macular-42pvvnof7k","2023","Journal Article","Frontiers in Medicine","Sina Gholami
Jennifer I. Lim
Theodore Leng
Sim Heng Ong
Atalie C. Thompson
Minhaj Nur Alam","10.3389/fmed.2023.1259017","https://scispace.compdf/federated-learning-for-diagnosis-of-age-related-macular-42pvvnof7k.pdf","This paper presents a federated learning (FL) approach to train deep learning models for classifying age-related macular degeneration (AMD) using optical coherence tomography image data. We employ the use of residual network and vision transformer encoders for the normal vs. AMD binary classification, integrating four unique domain adaptation techniques to address domain shift issues caused by heterogeneous data distribution in different institutions. Experimental results indicate that FL strategies can achieve competitive performance similar to centralized models even though each local model has access to a portion of the training data. Notably, the Adaptive Personalization FL strategy stood out in our FL evaluations, consistently delivering high performance across all tests due to its additional local model. Furthermore, the study provides valuable insights into the efficacy of simpler architectures in image classification tasks, particularly in scenarios where data privacy and decentralization are critical using both encoders. It suggests future exploration into deeper models and other FL strategies for a more nuanced understanding of these models' performance. Data and code are available at https://github.com/QIAIUNCC/FL_UNCC_QIAI. ","emphasizing the advantage of FL in multi-institutional learning, especially beneficial for smaller institutions with limited resources. . Introduction: Sadilek et al. highlighted the advancements in FL that ensure robust privacy protections while integrating differential privacy into clinical research. . Introduction:
 Another study focused on retinopathy of prematurity (ROP), where a DL model trained via FL with data from 5,245 patients across seven institutions identified diagnostic disparities and suggested standardization potential in clinical diagnoses . . Introduction:
 Furthermore, Lu et al.  demonstrated that FL-trained models for ROP diagnosis exhibited comparable performance to centralized models. Investigating diabetic retinopathy leveraged FL's potential to develop more generalized models by utilizing diverse datasets without compromising data privacy . . Introduction:
 Lastly, a comprehensive review by Nguyen et al.  emphasized the transformative potential of DL in ocular imaging, with FL providing an effective solution to data security concerns. . Introduction:
 Inconsistencies in optical coherence tomography (OCT) image acquisition parameters and scanning protocols can induce variations in image quality . . Introduction:
 Clinical and technical hurdles including differing standards and regulations among various Institutional Review Boards (IRBs), and limited training datasets for rare diseases can exacerbate the complexities of constructing and implementing DL techniques . Such variations can impact the competence and generalizability of DL models . . Introduction:
 The domain shift problem also poses a significant challenge in the context of FL . Domain shift arises when there is a substantial difference in data distributions across various local devices or nodes, also termed as clients, within the FL system. . Introduction:
 The non-identically distributed nature of decentralized data, a key characteristic of FL, can potentially compromise model learning performance . Rectifying this issue necessitates strategic and robust methodologies. . Introduction:
 In the research of Li et al. , domain adaptation (DA) techniques are outlined for optimizing learning algorithms irrespective of disparities in data distribution. Employing domaininvariant features or transfer learning methodologies, these techniques endeavor to lessen the impact of varied data distributions. . Introduction:
 Additionally, data augmentation can be leveraged to artificially enhance data representation, thereby diminishing the effects of domain shift . Other methods can also be utilized to counter this challenge, encompassing client selection and sampling strategies, model aggregation procedures, proactive domain exploration , and FL personalization . . Introduction:
 By effectively tackling domain shifts, FL can bolster the model's generalization capacity and augment performance across disparate domains. . Introduction:
 The purpose of this manuscript is to delineate the practicality of employing DA FL in the diagnosis of AMD. There is potential for domain shifts due to variations in protocols and OCT machines used for retinal imaging in the collaborative development of a classification model across institutions. . Introduction:
 Using data from three distinct datasets, this study examines various FL strategies to address this issue for AMD retinal OCT binary classification, utilizing an open-source FL Python library. . Introduction:
 The performance of these FL strategies was compared with a baseline centralized approach, emphasizing the potential benefits of employing multiple FL techniques to counteract the domain shift. . Introduction:
 However, this research did not delve into the security aspects of the FL framework, and all the involved entities, including the server and the FL node, were reliable and did not distribute distorted data or behave maliciously."
"A Fully Unsupervised Deep Learning Framework for Non-Rigid Fundus Image Registration","https://scispace.com/paper/a-fully-unsupervised-deep-learning-framework-for-non-rigid-19yismlz","2022","Journal Article","Bioengineering","G. A. Benvenuto
Marilaine Colnago
Maurício Araújo Dias
Rogério Galante Negri
Erivaldo A. Silva
Wallace Casaca","10.3390/bioengineering9080369","https://scispace.com/pdf/a-fully-unsupervised-deep-learning-framework-for-non-rigid-19yismlz.pdf","In ophthalmology, the registration problem consists of finding a geometric transformation that aligns a pair of images, supporting eye-care specialists who need to record and compare images of the same patient. Considering the registration methods for handling eye fundus images, the literature offers only a limited number of proposals based on deep learning (DL), whose implementations use the supervised learning paradigm to train a model. Additionally, ensuring high-quality registrations while still being flexible enough to tackle a broad range of fundus images is another drawback faced by most existing methods in the literature. Therefore, in this paper, we address the above-mentioned issues by introducing a new DL-based framework for eye fundus registration. Our methodology combines a U-shaped fully convolutional neural network with a spatial transformation learning scheme, where a reference-free similarity metric allows the registration without assuming any pre-annotated or artificially created data. Once trained, the model is able to accurately align pairs of images captured under several conditions, which include the presence of anatomical differences and low-quality photographs. Compared to other registration methods, our approach achieves better registration outcomes by just passing as input the desired pair of fundus images.","anatomical differences). •: The combination of multiple DL networks with image analysis techniques, such as isotropic undecimated wavelet transform and connected component analysis, allow-ing for the registration of fundus photographs even with low-quality segments and abrupt changes. Related Work:
 The literature covers a large number of DL-driven applications for clinical diagnosis in ophthalmology. Recently, several studies have been conducted on deep learning for the early detection of diseases and eye disorders, which include diabetic retinopathy detection , glaucoma diagnosis , and the automated identification of myopia using eye fundus images . All these DL-based applications have high clinical relevance and may prove effective in supporting the design of suitable protocols in ophthalmology. Going deeper into DL-based applications, the image translation problem has also appeared in different ophthalmology image domains, such as image super resolution , denoising of retinal optical coherence tomography (OCT) , and OCT segmentation . For instance, Mahapatra et al.  introduced a generative adversarial network (GAN) to increase the resolution of fundus images in order to enable more precise image analysis. Aiming at solving the issue of image denoising in high-and low-noise domains for OCT images, Manakov et al.  developed a model on the basis of the cycleGAN network to learn a mapping between these domains. Still on image translation, Sanchez et al.  combined two CNNs, the Pix2Pix and a modified deep retinal understanding network, to achieve the segmentation of intraretinal and subretinal fluids, and hyper-reflective foci in OCT images. For a comprehensive survey of image translation applications, see . Related Work:
 We now focus on discussing particular approaches for solving the image registration task. We split the registration methods into two groups: those that do not use DL (traditional methods), and those that do. Since our work seeks to advance the DL literature, we focus our discussion on this particular branch. Related Work:
 Considering the general application of image registration in the medical field, the literature has recently explored DL as a key resolution paradigm, including new approaches to obtain highly accurate results for various medical image categories, as discussed by Litjens et al. , Haskings et al. , and Fu et al. . Most of these approaches rely on supervised learning, requiring annotated data to train a model. For example, Yang et al.  introduced an encoder-decoder architecture to carry out the supervised registration of magnetic resonance images (MRI) of the brain.  covered the same class of images, but they employed a guided learning strategy instead. Eppenhof and Pluim  also applied a supervised approach, but for registering chest computed tomography (CT) images through a U-shaped encoder-decoder network . Still concerning supervised learning, several works attempted to compensate for the lack of labeled data by integrating new metrics into an imaging network. Fan et al.  induced the generation of ground-truth information used to perform the registration of brain images. Hering et al.  utilized a weakly supervised approach to align cardiac MRI images, and Hu et al.  took two networks: the former applied an affine transformation, while the latter gave the final registration of patients with prostate cancer."
"Unsupervised Domain Adaptation Network With Category-Centric Prototype Aligner for Biomedical Image Segmentation","https://scispace.com/paper/unsupervised-domain-adaptation-network-with-category-centric-2wj9z8e2c1","2021","Journal Article","IEEE Access","Ping Gong
Wenwen Yu
Qiuwen Sun
Ruohan Zhao
Junfeng Hu","10.1109/ACCESS.2021.3063634","https://scispace.compdf/unsupervised-domain-adaptation-network-with-category-centric-2wj9z8e2c1.pdf","With the widespread success of deep learning in biomedical image segmentation, domain shift becomes a critical and challenging problem, as the gap between two domains can severely affect model performance when deployed to unseen data with heterogeneous features. To alleviate this problem, we present a novel unsupervised domain adaptation network, for generalizing models learned from the labeled source domain to the unlabeled target domain for cross-modality biomedical image segmentation. Specifically, our approach consists of two key modules, a conditional domain discriminator (CDD) and a category-centric prototype aligner (CCPA). The CDD, extended from conditional domain adversarial networks in classifier tasks, is effective and robust in handling complex cross-modality biomedical images. The CCPA, improved from the graph-induced prototype alignment mechanism in cross-domain object detection, can exploit precise instance-level features through an elaborate prototype representation. In addition, it can address the negative effect of class imbalance via entropy-based loss. Extensive experiments on a public benchmark for the cardiac substructure segmentation task demonstrate that our method significantly improves performance on the target domain.","Wasserstein distance guided representation learning. Long et al. presented the multikernel MMD distance based on MMD to reduce the domain discrepancy. Yan et al. further extended the work and employed weighted MMD with a task-specific loss for domain adaptation. II. RELATED WORK:
 Ding et al.  proposed an adaptive exploration method to maximize the distances of all target images, minimizes distances of similar target images, and address the domain-shift problem for person re-identification (re-ID) in an unsupervised manner. II. RELATED WORK:
 Fan et al.  proposed a progressive unsupervised learning method to transfer pretrained deep representations to unseen domains based on clustering to improve the re-ID accuracy and produced CNN models with high discriminative ability. Nevertheless, both ,  are aimed at the topic of re-ID. II. RELATED WORK:
 In addition, the difference between ,  and our method is that  minimizes distances between one image and its neighbors and maximizes distances between one image and other images at the feature level, and  performs clustering at the feature level. II. RELATED WORK:
 However, our method is not only performs conditional domain discriminator at the feature level and instance level, but also aligns category-centric prototype at the instance-level. II. RELATED WORK:
 More recently, with the advent of generative adversarial networks (GAN) , another line of research is based on adversarial training. For instance, Ganin et al.  aligned the distributions of features across the two domains accomplished through standard backpropagation training via a simple new gradient reversal layer. II. RELATED WORK:
 Tzeng et al.  introduced a more flexible adversarial learning framework with united weight sharing to address the problem of domain shift. II. RELATED WORK:
 With the widespread success of CycleGAN  in unpaired image-toimage transformations, many previous image adaptation efforts were based on modified CycleGAN with applications in both natural datasets ,  and medical image segmentation - . Overall, our framework belongs to the latter category based on adversarial learning. II. RELATED WORK:
 For biomedical image segmentation applications, as the common cross-modality, interscanner, and different imaging protocols vary, domain shift has become a critical and challenging problem in biomedical image segmentation. Ghafoorian et al.  reduced the required number of labels in the target domain via transfer learning for brain MRI lesion segmentation tasks. II. RELATED WORK:
 Opbroek et al.   presented a novel patch-based output space adversarial learning framework to jointly and robustly segment the optic disc and optic cup from different fundus image datasets and achieved effective feature alignment. However, these works did not aim at the topic of unsupervised domain adaptation for cross-modality biomedical images. II. RELATED WORK:
 In the field of cross-modality biomedical segmentation, Dou et al. ,  employed adversarial learning to adapt the early-layer feature distributions while the higherlayer features are fixed. However, their method needs comprehensive empirical studies and design to determine the optimal adaptation depth. II. RELATED WORK:
 Chen et al. ,  used deeply synergistic image and feature alignment for unsupervised bidirectional cross-modality adaptation segmentation and achieved better domain performance, but it has a complex training process due to the essence of GAN is used."
"Physical imaging parameter variation drives domain shift","https://scispace.com/paper/physical-imaging-parameter-variation-drives-domain-shift-1dxf1zkx","2022","Journal Article","Dental science reports","Oz Kilim
Alexandru Olar
Tamás Joó
Tamás Palicz
Péter Pollner
István Csabai","10.1038/s41598-022-23990-4","https://scispace.com/pdf/physical-imaging-parameter-variation-drives-domain-shift-1dxf1zkx.pdf","Statistical learning algorithms strongly rely on an oversimplified assumption for optimal performance, that is, source (training) and target (testing) data are independent and identically distributed. Variation in human tissue, physician labeling and physical imaging parameters (PIPs) in the generative process, yield medical image datasets with statistics that render this central assumption false. When deploying models, new examples are often out of distribution with respect to training data, thus, training robust dependable and predictive models is still a challenge in medical imaging with significant accuracy drops common for deployed models. This statistical variation between training and testing data is referred to as domain shift (DS).To the best of our knowledge we provide the first empirical evidence that variation in PIPs between test and train medical image datasets is a significant driver of DS and model generalization error is correlated with this variance. We show significant covariate shift occurs due to a selection bias in sampling from a small area of PIP space for both inter and intra-hospital regimes. In order to show this, we control for population shift, prevalence shift, data selection biases and annotation biases to investigate the sole effect of the physical generation process on model generalization for a proxy task of age group estimation on a combined 44 k image mammogram dataset collected from five hospitals.We hypothesize that training data should be sampled evenly from PIP space to produce the most robust models and hope this study provides motivation to retain medical image generation metadata that is almost always discarded or redacted in open source datasets. This metadata measured with standard international units can provide a universal regularizing anchor between distributions generated across the world for all current and future imaging modalities. ","features between our task and medical tasks is fundamentally the same so there is no reason to think PIP variation has no part to play in the domain shift observed in medically relevant tasks. Currently further investigation into this is limited by the issue of data availability. Discussion:
 We explored leveraging these PIPs with a Domain Adversarial Neural Network (DANN)  architecture with the domain prediction changed to a regression task of PIP prediction however, we did not see any improvements over state of the art methods described in the related work section. As G is non-invertable, images generated cannot be fully separated from their PIPs with any mathematical transformations or learnt approximation. Concretely, one cannot gain information present in an MRI image with a CycleGAN transformed CT scan or vice versa  . Despite this knowledge, integrating PIPs into conditional CycleGANs  or Pseudo-physical augmentations  may improve model generalization despite not being fully rigorous and physically aware methods but rather ""physically inspired"" solutions. Discussion:
 Despite further work required to separate and fully categorise the contributions to domain shift  that effect model generalization in medical imaging tasks the results we present in 3 could be a useful tool for predicting the worst case generalization scenario when we have full knowledge of PIPs in the training set as well as full knowledge of PIPs from the hospitals or vendors where the model is deployed. Concretely, if we trained our model on data from hospital A we could go to hospital B and C and give an approximation for the reduction in accuracy of our model with knowledge of the distance in PCA space between hospitals generated data and the training data. This type of worst case scenario would be possible to deploy today if PIP data is preserved at sites. This could be paired with other tools such as visualisations of variations of concepts that change model predictions for a given image  or any other interpretability aids. Discussion:
 Our results lead us to believe that a more homogeneous sampling process of images in PIP space may provide better generalization to unseen images as these may lie inside this ""evenly tiled"" PIP space if the training set is from a diverse enough set of vendors. In the case of models trained on data from one vendor, hospitals may chose which models to use based on their mean distance from the training sets used for different models, for example if hospital C was presented with two models, one trained on A and one trained on B it would be advisable to pick the model trained on data from B see Fig. . To extend and transfer this knowledge to the area of federated learning; if we train on data from multiple hospitals in a federated regime there may be large unbalanced clusters in the PIP space and we may end up with sub-optimal generalization power as models are bias to learn well in these PIP manifolds where there are many examples but not outside. Further work could be done to explore this hypothesis and may aid the advancement of federated learning as a regime to enhance generalization. It may well be that ""more data"" does not necessarily mean better model generalization for medical imaging tasks, however, this is speculative."
"Cross-Domain Deep Face Matching for Real Banking Security Systems","https://scispace.com/paper/cross-domain-deep-face-matching-for-real-banking-security-305sen3auy","2020","Proceedings Article","International Conference on eDemocracy & eGovernment","Johnatan S. Oliveira
Gustavo Botelho de Souza
Anderson Rocha
Flavio E. Deus
Aparecido Nilceu Marana","10.1109/ICEDEG48599.2020.9096783","https://scispace.com/pdf/cross-domain-deep-face-matching-for-real-banking-security-305sen3auy.pdf","Ensuring the security of transactions is currently one of the major challenges that banking systems deal with. The usage of face for biometric authentication of users is attracting large investments from banks worldwide due to its convenience and acceptability by people, especially in cross-domain scenarios, in which facial images from ID documents are compared with digital self-portraits (selfies) for the automated opening of new checking accounts, e.g, or financial transactions authorization. Actually, the comparison of selfies and IDs has also been applied in another wide variety of tasks nowadays, such as automated immigration control. The major difficulty in such process consists in attenuating the differences between the facial images compared given their different domains. In this work, in addition to collecting a large cross-domain face dataset, with 27,002 real facial images of selfies and ID documents (13,501 subjects) captured from the databases of the major public Brazilian bank, we propose a novel architecture for such cross-domain matching problem based on deep features extracted by two well-referenced Convolutional Neural Networks (CNN). Results obtained on the dataset collected, called FaceBank, with accuracy rates higher than 93%, demonstrate the robustness of the proposed approach to the cross-domain face matching problem and its feasible application in real banking security systems.","approach that transfers the supervision knowledge from a labeled source domain (training data) to the unlabeled target domain (testing data). Cross-Domain issues and Domain Adaptation: Their basic idea is to convert the source domain images to the target domain. Cross-Domain issues and Domain Adaptation:
 The problem of cross-domain image retrieval is addressed in  considering the following practical application: given a user photo depicting a clothing image, our goal is to retrieve the same or attribute-similar clothing items from online shopping stores. In this case, it is a challenging problem due to the large discrepancy between online shopping images, usually taken in ideal lighting/pose/background conditions, and user photos captured in uncontrolled conditions. They address this problem by proposing a Dual Attribute-aware Ranking Network (DARN) for retrieval feature learning, consisting of two sub-networks, one for each domain, whose retrieval feature representations are driven by semantic attribute learning. With a large-scale real dataset, collected from real-world consumer websites, the results were considered important. Cross-Domain issues and Domain Adaptation:
 In a work with the same principles but with different approaches (SHI; JAIN, 2018) uses two private datasets and by employing the transfer learning technique, they propose a new method called DocFace, to train a domain-specific network for ID document photo matching without a large dataset. Compared with the baseline of applying existing methods for general recognition of this problem, their method achieves considerable improvement. In their study, they explore tests with and without transfer learning. Cross-Domain issues and Domain Adaptation:
 Despite the similar approach to the same type of problem, we consider our tests more complex and with better results. Cross-Domain issues and Domain Adaptation:
 Despite the approaches presented, to the best of our knowledge, no evaluation regarding face authentication on considerably large cross-domain dataset such as ours was reported in the literature, especially for banking scenarios, the target application of our analysis. Proposed methodology:
 In (imposter pair). Figure .1 shows these steps, which are described in more detail in the following sections. Collected Dataset -FaceBank:
 In , only a small data set was used, but the results were promising and indicated that the path chosen would be worth more research. In this way, the best approach would logically be to increase the amount of training data to improve learning. The importance of the amount of data used should not be overlooked when dealing with machine learning, especially Deep Learning Architectures. As already mentioned, ML's algorithms learn through training, initially receiving examples whose outputs are known, note the difference between its predictions and the correct outputs, and tune the weightings of the inputs to improve the accuracy of its predictions until they are optimized. Somehow, the quality of their predictions improves with experience and the more data we provide, the better the prediction engines we can create."
"Truly Generalizable Radiograph Segmentation With Conditional Domain Adaptation","https://scispace.com/paper/truly-generalizable-radiograph-segmentation-with-conditional-4nvr5gbfut","2020","Journal Article","IEEE Access","Hugo N. Oliveira
Edemir Ferreira
Jefersson A. dos Santos","10.1109/ACCESS.2020.2991688","https://scispace.com/pdf/truly-generalizable-radiograph-segmentation-with-conditional-4nvr5gbfut.pdf","Digitization techniques for biomedical images yield disparate visual patterns in radiological exams. These pattern differences, which can be viewed as a domain-shift problem, may hamper the use of data-driven approaches for inference over these images, such as Deep Neural Networks. Another noticeable difficulty in this field is the lack of labeled data, even though in many cases there is an abundance of unlabeled data available. Therefore, an important step in improving the generalization capabilities of these methods and mitigate domain-shift effects is to perform unsupervised or semi-supervised adaptation between different domains of biomedical images. In this work, we propose a novel approach for segmentation of biomedical images based on Generative Adversarial Networks. The proposed method, named Conditional Domain Adaptation Generative Adversarial Network (CoDAGAN), merges unsupervised networks with supervised deep semantic segmentation architectures in order to create a semi-supervised method capable of learning from both unlabeled and labeled data, whenever labeling is available. We conducted experiments to compare our method with traditional and state-of-the-art baselines by using several domains, datasets, and segmentation tasks. The proposed method yielded consistently better results than the baselines in scarce labeled data scenarios, achieving Jaccard values greater than 0.9 and good segmentation quality in most tasks. Unsupervised Domain Adaptation results were observed to be close to the Fully Supervised Domain Adaptation used in the traditional procedure of fine-tuning pretrained networks.","Title: Truly Generalizable Radiograph Segmentation With Conditional Domain Adaptation Authors: Hugo N Oliveira (Corresponding Author),Edemir Ferreira,Jefersson A Dos Santos Keywords: INDEX TERMS Deep learning, domain adaptation, biomedical images, semantic segmentation, image translation, semi-supervised learning"
"Visual Domain Adaptation in the Deep Learning Era","https://scispace.com/paper/visual-domain-adaptation-in-the-deep-learning-era-3uktcbnh","2022","Journal Article","Synthesis lectures on computer vision","Gabriela Csurka
Timothy M. Hospedales
Mathieu Salzmann
Tatiana Tommasi","10.2200/s01169ed1v01y202202cov020","https://scispace.com/pdf/visual-domain-adaptation-in-the-deep-learning-era-3uktcbnh.pdf","Solving problems with deep neural networks typically relies on massive amounts of labeled training data to achieve high performance. While in many situations huge volumes of unlabeled data can be and ","Keywords: domain adaptation, domain generalization, computer vision, deep learning"
"Noise Transfer for Unsupervised Domain Adaptation of Retinal OCT Images","https://scispace.com/paper/noise-transfer-for-unsupervised-domain-adaptation-of-retinal-1s2f170k","2022","Book Chapter","International Conference on Medical Image Computing and Computer-Assisted Intervention","Valentin Koch
Olle Holmberg
Hannah Spitzer
Johannes Schiefelbein
Ben Asani
Michael Hafner
Fabian J. Theis","10.1007/978-3-031-16434-7_67","https://scispace.com/pdf/noise-transfer-for-unsupervised-domain-adaptation-of-retinal-1s2f170k.pdf","Optical coherence tomography (OCT) imaging from different camera devices causes challenging domain shifts and can cause a severe drop in accuracy for machine learning models. In this work, we introduce a minimal noise adaptation method based on a singular value decomposition (SVDNA) to overcome the domain gap between target domains from three different device manufacturers in retinal OCT imaging. Our method utilizes the difference in noise structure to successfully bridge the domain gap between different OCT devices and transfer the style from unlabeled target domain images to source images for which manual annotations are available. We demonstrate how this method, despite its simplicity, compares or even outperforms state-of-the-art unsupervised domain adaptation methods for semantic segmentation on a public OCT dataset. SVDNA can be integrated with just a few lines of code into the augmentation pipeline of any network which is in contrast to many state-of-the-art domain adaptation methods which often need to change the underlying model architecture or train a separate style transfer model. The full code implementation for SVDNA is available at https://github.com/ValentinKoch/SVDNA. ","Holmberg,Hannah Spitzer,Johannes Schiefelbein,Ben Asani,Michael Hafner,Fabian J Theis (Corresponding Author) Keywords: Style-transfer, Unsupervised Domain Adaptation, Semantic Segmentation Introduction:
 Diseases in the Human retina are among the leading reasons for reduced vision and blindness globally. Estimates are that currently roughly 170 million people are affected by Age-related Macular Degeneration , while Diabetic Retinopathy is recognized as a global epidemic with the numbers increasing at ever higher rates . Optical coherence tomography (OCT) is a powerful technique used in many medical applications to generate real time and non-invasive cross-sectional images of live biological tissues . In the field of Ophthalmology, OCT images help doctors to make therapy decisions and monitor the treatment outcome. As eye diseases are becoming more and more prevalent due to the increased age of populations , the need for research in automating diagnosis and aiding doctors is increasing. Introduction:
 Recently, deep learning methods are showing promising results in areas such as disease prediction , semantic segmentation  or improving quality  of retinal OCT images. Although a lot of progress has been made, challenges remain in applying artificial intelligence methods on real-world OCT data, where image characteristics such as signal-to-noise ratio, brightness, and contrast can vary and cause changes in data distribution, so-called domain shifts. For OCT images, a particular domain shift is introduced from different OCT imaging devices being used, which have different image-quality properties. While for a medical doctor those differences are only a mild annoyance, machine learning models can quickly fail when faced with only small disturbances in the underlying data distribution. One solution is to label images from all possible devices, but as manually labeling images is very costly and needs highly skilled specialists, other methods need to be developed. Domain adaptation methods offer a solution to the reduced performance of AI algorithms that are caused by the difference in data distribution . Domain adaptation has also been used for shifting domains between different device manufacturers for OCT imaging devices. Yang et al.  detect lesions in OCT images from different camera devices using an adversarial approach, several recent works use CycleGAN approaches to transfer style between domains . In particular, Romo-Bucheli et al. train a CycleGAN and measure the improved performance on a segmentation task, which makes it the most comparable work to ours . While GAN architectures are performant when dealing with domain shifts where the structural difference between the domains is much larger than between OCT camera devices , we argue that for domain adaptation between retinal OCT devices these models are unnecessarily complex and can through small changes to image content even be decremental to the performance."
"Embracing the disharmony in medical imaging: A Simple and effective framework for domain adaptation","https://scispace.com/paper/embracing-the-disharmony-in-medical-imaging-a-simple-and-197lcqv7","2022","Journal Article","Medical Image Analysis","","10.1016/j.media.2021.102309","https://scispace.com/pdf/embracing-the-disharmony-in-medical-imaging-a-simple-and-197lcqv7.pdf","","and Effective Framework for Domain Adaptation Authors: Rongguang Wang,Pratik Chaudhari (Corresponding Author),Christos Davatzikos Keywords: Heterogeneity, Distribution shift, Domain adaptation, Domain generalization, MRI Introduction:
 Deep learning models have shown great promise in several fields related to medicine, including medical imaging diagnostics  and predictive modeling . Applications of medical imaging range from relatively common segmentation tasks , to more complex and high level decision-support functions, such as estimating different patterns of brain diseases  and producing personalized prognosis . However, despite their promise, complex deep learning models tend to have poor reproducibility across hospitals, scanners, and patient cohorts, since these high-dimensional models can overfit specific studies, and hence achieve modest generalization performance . While a potential solution to this weakness is to train on very large databases of diverse studies, this approach is limited in several ways. Firstly, the characteristics of imaging devices change constantly, and hence even amply trained models are bound to face the same generalization challenges for new studies. Secondly, training labels, such as clinical or molecular classifications, or treatment measurements, are often scarce and hard to obtain. It is therefore impractical to expect that such ample training is possible in many problems. Finally, even if it were possible to train a model on large and diverse databases that would cover all possible variations across images, such a model would almost certainly sacrifice accuracy in favor of generalization under diverse conditions, i.e. it would have to rely on coarse imaging features that are stable across imaging devices and patient populations, and might fail to capture subtle and highly informative detail. Introduction:
 Herein, we propose a domain adaptation framework, which overcomes these limitations by allowing trained models to adapt to new imaging conditions in two paradigms: intra-study adaptation and inter-study generalization. To improve the prediction accuracy of a model on heterogeneous images within each single study, intra-study adaptation strategy fast adapts a model which is pre-trained on the entire study to each sub-groups, e.g. age range, race, scanner type, by fine-tuning. We use label information in the re-training process in this situation. For adaptation between different studies, our inter-study generalization method can avoid using ground truth from the unseen study in view of the scarcity of labels in medical imaging. Fundamental in our approach is the utilization of ""auxiliary tasks"", i.e., learning tasks that can be performed on readily available data from a new imaging condition (scanner, site, or population), and which can be used to adapt the primary trained model (e.g. disease classification) to these new conditions. An example of auxiliary tasks are estimation of readily available demographic characteristics, since such data is amply available in most practical settings. Essentially, the auxiliary tasks help an already trained model adapt to new imaging conditions, by adapting the features extracted by networks that are shared between the primary learning task and the auxiliary tasks. We conducted extensive experiments on clinical large-scale studies of 2,614 3D T1-MRI scans to evaluate the effectiveness of the proposed framework for both Alzheimer's disease and schizophrenia classification tasks. Experimental results indicate that our proposed framework substantially improves the performance in both intra-study adaptation and inter-study generalization paradigms. Introduction:
 Contributions Our main contributions are as follows."
"Zero-Shot Medical Image Translation via Frequency-Guided Diffusion Models","https://scispace.com/paper/zero-shot-medical-image-translation-via-frequency-guided-2uplbon8z9","2024","Journal Article","IEEE Transactions on Medical Imaging","Yunxiang Li
Hua-Chieh Shao
Xiao Liang
Liyuan Chen
Ruiqi Li
Steve Jiang
Jing Wang
You Zhang","10.1109/tmi.2023.3325703","https://scispace.compdf/zero-shot-medical-image-translation-via-frequency-guided-2uplbon8z9.pdf","Recently, the diffusion model has emerged as a superior generative model that can produce high quality and realistic images. However, for medical image translation, the existing diffusion models are deficient in accurately retaining structural information since the structure details of source domain images are lost during the forward diffusion process and cannot be fully recovered through learned reverse diffusion, while the integrity of anatomical structures is extremely important in medical images. For instance, errors in image translation may distort, shift, or even remove structures and tumors, leading to incorrect diagnosis and inadequate treatments. Training and conditioning diffusion models using paired source and target images with matching anatomy can help. However, such paired data are very difficult and costly to obtain, and may also reduce the robustness of the developed model to out-of-distribution testing data. We propose a frequency-guided diffusion model (FGDM) that employs frequency-domain filters to guide the diffusion model for structure-preserving image translation. Based on its design, FGDM allows zero-shot learning, as it can be trained solely on the data from the target domain, and used directly for source-to-target domain translation without any exposure to the source-domain data during training. We evaluated it on three cone-beam CT (CBCT)-to-CT translation tasks for different anatomical sites, and a cross-institutional MR imaging translation task. FGDM outperformed the state-of-the-art methods (GAN-based, VAE-based, and diffusion-based) in metrics of Fréchet Inception Distance (FID), Peak Signal-to-Noise Ratio (PSNR), and Structural Similarity Index Measure (SSIM), showing its significant advantages in zero-shot medical image translation. ","information should be similar. V. LIMITATION: Therefore, our model is currently only applicable for translation tasks between similar modalities such as CBCT and CT, or medical images with the same modality but different acquisition centers, machines, or protocols. V. LIMITATION:
 For tasks such as MR-to-CT translation, where the differences between lowfrequency and high-frequency information are substantial, our model may not perform well. V. LIMITATION:
 In future work, we will develop methods to convert the low-frequency and high-frequency information between modalities like CT and MR first, before filling in the intermediate frequencies using the developed FGDM framework. V. LIMITATION:
 In addition, we chose diffusion GAN as our backbone network as we want to achieve a balance between computational speed and image quality, as the CBCTto-CT conversion efficiency is of critical importance for online adaptive radiotherapy. V. LIMITATION:
 In future work, the frequency guidance using the original DDPM backbone can also be evaluated for its potential in generating more accurate images when the inference speed is not a critical factor. V. LIMITATION:
 Another aspect is that our study only focused on the design of the overall framework and did not work on optimizing the model structure or designing more tailored loss functions for the image translation task, which can help to further improve the translation results. V. LIMITATION:
 Future works can focus on improving loss functions to emphasize important anatomical structures for each specific translation task and optimizing the model structure to include potential attention mechanisms to enhance the accuracy and efficiency of image translation. VI. CONCLUSION:
 In this paper, we propose a frequency-guided diffusion model for medical image translation. Based on the observations in the frequency domain, we extract domain invariant information by high-pass and low-pass filters. VI. CONCLUSION:
 The addition of high-frequency information greatly improves the retention capability of anatomical structures of the diffusion model in medical image translation tasks. Moreover, the thresholds of our two frequency filters can be freely adjusted during testtime to cope with different types of medical image translation tasks without re-training the model. VI. CONCLUSION:
 FGDM not only outperforms other models in all metrics, but can also be trained only on the target-domain images and directly applied to the source-domain images to achieve zero-shot image translation. VI. CONCLUSION:
 We applied the FGDM model trained on the head and neck image dataset to translate the lung image dataset and the OARs image dataset without any fine-tuning, further demonstrating the robustness of our model for zero-shot image translation and out-of-distribution data."
"CDDSA: Contrastive domain disentanglement and style augmentation for generalizable medical image segmentation","https://scispace.com/paper/cddsa-contrastive-domain-disentanglement-and-style-efonvs7fin","2023","Journal Article","Medical Image Analysis","Ran Gu
Guotai Wang
Jiangshan Lu
Jingyang Zhang
Wenhui Lei
Yinan Chen
Wenjun Liao
Shichuan Zhang
Kang Li
Dimitris Metaxas
Shaoting Zhang","10.1016/j.media.2023.102904","https://scispace.compdf/cddsa-contrastive-domain-disentanglement-and-style-efonvs7fin.pdf","Generalization to previously unseen images with potential domain shifts is essential for clinically applicable medical image segmentation. Disentangling domain-specific and domain-invariant features is key for Domain Generalization (DG). However, existing DG methods struggle to achieve effective disentanglement. To address this problem, we propose an efficient framework called Contrastive Domain Disentanglement and Style Augmentation (CDDSA) for generalizable medical image segmentation. First, a disentangle network decomposes the image into domain-invariant anatomical representation and domain-specific style code, where the former is sent for further segmentation that is not affected by domain shift, and the disentanglement is regularized by a decoder that combines the anatomical representation and style code to reconstruct the original image. Second, to achieve better disentanglement, a contrastive loss is proposed to encourage the style codes from the same domain and different domains to be compact and divergent, respectively. Finally, to further improve generalizability, we propose a style augmentation strategy to synthesize images with various unseen styles in real time while maintaining anatomical information. Comprehensive experiments on a public multi-site fundus image dataset and an in-house multi-site Nasopharyngeal Carcinoma Magnetic Resonance Image (NPC-MRI) dataset show that the proposed CDDSA achieved remarkable generalizability across different domains, and it outperformed several state-of-the-art methods in generalizable segmentation. Code is available at https://github.com/HiLab-git/DAG4MIA. ","propose a domain style contrastive learning loss to encourage the style codes in different domains to be discriminative from each other, which improves the model's ability to recognize domain-invariant anatomical representations that is sent to a segmentor to obtain segmentation results. Introduction:
 As the segmentor is not affected by domain-specific features, it has a high generalizability across different domains. Introduction:
 In addition, based on the extracted style codes in training domains, we can generate a new random style code and combine it with an existing anatomical representation to simulate images in an unseen domain with a new styles using the decoder, i.e., style augmentation, which further improves the generalizability of our framework. Introduction:
 To the best of our knowledge, this is the first work in the literature to propose feature disentanglement learning for domaingeneralizable medical image segmentation. The contributions of our method are summarised as follows: Introduction:
 1) We introduce a novel framework CDDSA using GANfree disentanglement for domain generalization in medical image segmentation. It achieves generalizability by segmentation from decomposed domain-invariant representations extracted by a single anatomy Encoder that is shared across domains and more efficient and scalable than GANbased disentanglement. Introduction:
 2) To make the disentangled domain-specific style codes more representative and distinguishable, we propose domain style contrasitve learning, which forces the style codes from the same domain and different domains to be similar and dissimilar, respectively. Introduction:
 3) We propose style augmentation based on the disentangled anatomical representations and style codes to simulate images from unseen domains with different styles, which further improves the generalizability of the disentanglement and segmentation models. Introduction:
 4) Comprehensive experimental results on multi-domain fundus images and multi-domain nasopharyngeal carcinoma magnetic resonance images (NPC-MRI) showed that our proposed CDDSA achieved high generalization on unseen domains, and it outperformed several state-of-the-art domain generalization methods. Domain Generalization for Medical Image Analysis:
 Recently, domain generalization has attracted increasing attentions to avoid dramatic performance degradation when inferring with images from unseen domains . It aims to learn a model from a single or multiple source domains to make it directly applicable for unseen target domains without extra training . Domain Generalization for Medical Image Analysis:
 Existing DG methods mainly include metalearning methods, data-based methods and feature-based methods. Meta-learning  splits a set of source domains into meta-train and meta-test subsets, and adopts meta-optimization that iteratively updates model parameters to improve performance on the meta-test subset to simulate the situation when inferring on unseen domains. Domain Generalization for Medical Image Analysis:
 combined meta-learning with federated learning to achieve privacy-preserving generalizable segmentation through continuous frequency space interpolation across clients. However, meta-optimization process is highly time-consuming since all potential splitting results of metatrain and meta-test should be considered during training . Domain Generalization for Medical Image Analysis:
 Data-based approaches usually use different data augmentation strategies for improving the model's generalizability. a deep stacked transformation assuming that the shift between different domains can be simulated by extensive data augmentation on a single domain. utilized Cycle-GAN  to transform images from one certain domain to other domains for augmentation."
"Adversarial Consistency for Single Domain Generalization in Medical Image Segmentation","https://scispace.com/paper/adversarial-consistency-for-single-domain-generalization-in-bbytjxns","2022","Proceedings Article","International Conference on Medical Image Computing and Computer-Assisted Intervention","Yanwu Xu
Shaoan Xie
Maxwell Reynolds1
Matthew Ragoza1
Mingming Gong
Kayhan Batmanghelich","10.48550/arXiv.2206.13737","https://scispace.compdf/adversarial-consistency-for-single-domain-generalization-in-bbytjxns.pdf",". An organ segmentation method that can generalize to unseen contrasts and scanner settings can significantly reduce the need for retraining of deep learning models. Domain Generalization (DG) aims to achieve this goal. However, most DG methods for segmentation require training data from multiple domains during training. We propose a novel adversarial domain generalization method for organ segmentation trained on data from a single domain. We synthesize the new domains via learning an adversarial domain synthesizer (ADS) and presume that the synthetic domains cover a large enough area of plausible distributions so that unseen domains can be interpolated from synthetic domains. We propose a mutual information regularizer to enforce the semantic consistency between images from the synthetic domains, which can be estimated by patch-level contrastive learning. We evaluate our method for various organ segmentation for unseen modalities, scanning protocols, and scanner sites.","Yanwu Xu,Shaoan Xie,Maxwell Reynolds,Matthew Ragoza,Mingming Gong,Kayhan Batmanghelich Keywords: Medical Image Segmentation, Single Domain Generalization, Adversarial Training, Mutual Information Introduction:
 Deep Learning-based methods for the segmentation of medical images hold stateof-the-art performance across various organs and anatomies . Introduction:
 The independent and identically distributed (i.i.d.) is the underlying assumption of most of those methods.However, the difference in the image acquisition, such as scanning protocol and image modality, introduces domain shifts, rendering the assumption impractical.Domain Adaptation  (DA) and Multi-source Domain generalization  (MDG) aim to alleviate the domain shift issue.However, those approaches are not data-efficient as they either require access to test distribution (i.e., DA) or need multiple labeled source domains during training (i.e., MDG). Introduction:
 In this paper, we focus on single-source domain generalization (SDG), aiming to train a generalizable deep model on only one source domain. Introduction:
 The SDG does not require access to the test distribution or labeled data from multiple sources during the training. As a result, it reduces annotation costs and avoids repetitive adaptation for each new domain. Introduction:
 In the literature, various SDG methods have been proposed that are based on augmentation of input image  and meta learning . The meta-learning techniques tend to be extremely slow during inference time. The augmentation methods synthesize new images using random initialization of convolution filter . Introduction:
 However, those methods cannot avoid over-fitting to a regular pattern of synthetic data. Thus, we propose synthesizing the new domains via learning an adversarial framework. Introduction:
 We propose synthesizing the new domains via learning an adversarial domain synthesizer (ADS). The intuition is that the synthetic domains cover a large enough area of plausible distributions so that unseen domains can be interpolated from synthetic domains. Introduction:
 Specifically, we design the synthesizer with a random style module, enabling ADS to synthesize random textures during adversarial training. Without a constraint, adversarial training may change the image semantics, making the synthetic domains irrelevant. Introduction:
 To remedy this problem, we propose to keep the underlying semantic information between the source image and the synthetic image via a mutual information regularizer. Introduction:
 As estimating mutual information is hard for high dimensional data, we utilize the patch-level contrastive loss  as a surrogate to maximize the mutual information between the original and synthesized images. Introduction:
 The main contributions of this work can be summarized as follows: 1) We propose an adversarial framework for single domain generalization of medical image segmentation. 2) We redesign the network structure of synthesizing new domains. 3) To constrain the adversarial training, we propose a regularization method for synthetic images. Introduction:
 To evaluate our model, we conduct experiments of single domain generalization of medical image segmentation on cross-modality image segmentation (CT → MRI), -imaging protocol, and -organizations. Related Works:
 Unsupervised Domain Adaptation and Domain Generalization Unsupervised Domain Adaptation (UDA) is a proposed strategy for addressing domain shift between the training data and testing data of deployed applications."
"Information extraction from German radiological reports for general clinical text and language understanding","https://scispace.com/paper/information-extraction-from-german-radiological-reports-for-302jgvun","2023","Journal Article","Dental science reports","Michael Jantscher
Felix Gunzer
R. Kern
Eva Hassler
Sebastian Tschauner
Gernot Reishofer","10.1038/s41598-023-29323-3","https://scispace.com/pdf/information-extraction-from-german-radiological-reports-for-302jgvun.pdf","Abstract Recent advances in deep learning and natural language processing (NLP) have opened many new opportunities for automatic text understanding and text processing in the medical field. This is of great benefit as many clinical downstream tasks rely on information from unstructured clinical documents. However, for low-resource languages like German, the use of modern text processing applications that require a large amount of training data proves to be difficult, as only few data sets are available mainly due to legal restrictions. In this study, we present an information extraction framework that was initially pre-trained on real-world computed tomographic (CT) reports of head examinations, followed by domain adaptive fine-tuning on reports from different imaging examinations. We show that in the pre-training phase, the semantic and contextual meaning of one clinical reporting domain can be captured and effectively transferred to foreign clinical imaging examinations. Moreover, we introduce an active learning approach with an intrinsic strategic sampling method to generate highly informative training data with low human annotation cost. We see that the model performance can be significantly improved by an appropriate selection of the data to be annotated, without the need to train the model on a specific downstream task. With a general annotation scheme that can be used not only in the radiology field but also in a broader clinical setting, we contribute to a more consistent labeling and annotation process that also facilitates the verification and evaluation of language models in the German clinical setting. ","of Ramponi and Plank . Since we already use active learning and strategic sampling for model training in the first experiment, and large unlabeled datasets are available in the foreign domains, we continue with this data-centric data selection approach for domain adaptation. Transfer to another clinical domain (domain adaptation). Medical entities as well as the writing:
 Results for MRI Head reports. In a second experiment, we validate a fine-tuned NER and RE model adapted to a foreign imaging domain (Magnetic Resonance Imaging). Hence, we compare the models that were (a) previously trained on the CTH dataset and now fine-tuned on the MRI Head (MRIH) training set ( BERT ct,mr and RBERT ct,mr ) with those that were (b) trained on the MRIH dataset only ( BERT mr and RBERT mr ). After several active learning iterations with strategic sampling on the MRIH training set, we see that the BERT ct,mr model outperforms the BERT mr model (Fig. ). The same applies to the RE task. This is especially true for the very early stages where the amount of training data is quite small. Transfer to another clinical domain (domain adaptation). Medical entities as well as the writing:
 Results for Xray pediatric reports. In the third experiment, we do not only switch to another imaging modality but also to a different anatomical region where the examination is performed. Like in the experiment before we compare the models previously trained on the CTH reports and fine-tuned on the Xray pediatric (XPED) dataset and the models trained on the XPED reports set only. It is shown that although the performance of www.nature.com/scientificreports/ the BERT xray and RBERT xray increases with growing training set, the multi-domain models BERT ct,xray and RBERT ct,xray are dominating (Fig. )."