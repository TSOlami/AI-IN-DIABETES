\section{Model Limitations: Bias, Generalization, and Interpretability}

Despite impressive performance on benchmark datasets, AI/ML models for diabetes care exhibit critical limitations that impede clinical translation and risk exacerbating health inequalities. This section examines three interrelated challenges—bias and fairness, generalization and robustness, and interpretability—that represent fundamental barriers to equitable, reliable AI-driven healthcare.

\subsection{Algorithmic Bias and Fairness}

Algorithmic bias in diabetes AI systems manifests through systematic performance disparities across demographic subgroups defined by sex, race, ethnicity, age, socioeconomic status, and clinical characteristics \cite{wang2024ai, prioleau2025deep}. Recent reproducibility studies reveal pervasive gender bias in glucose prediction models, with all evaluated deep learning architectures exhibiting significantly higher prediction errors for female patients compared to males \cite{prioleau2025deep}. These disparities persist across multiple datasets and model architectures, suggesting fundamental issues in data collection, feature engineering, or model optimization rather than isolated implementation errors \cite{prioleau2025deep}.

Racial and ethnic biases are particularly pronounced in retinal imaging systems for diabetic retinopathy screening. Models trained predominantly on European and North American populations exhibit elevated false negative rates when applied to African, Asian, and Hispanic patients, potentially delaying diagnosis and treatment of vision-threatening complications \cite{wang2024ai, fahmy2025exploring}. The underrepresentation of diverse populations in training datasets, coupled with physiological differences in retinal pigmentation and disease presentation, contributes to these disparities \cite{bai2024federated}. However, the lack of standardized reporting of demographic characteristics and subgroup performance in published literature obscures the true extent of these biases \cite{wang2024ai}.

Performance disparities based on glycemic control represent another critical fairness concern. Models systematically underperform for individuals with poor glycemic control—precisely the population most likely to benefit from AI-assisted interventions \cite{prioleau2025deep}. This pattern suggests that models may be optimizing for average performance on well-controlled patients rather than robust performance across the full spectrum of disease severity \cite{prioleau2025deep}. The clinical implications are profound: AI systems that fail for high-risk patients may widen rather than narrow existing disparities in diabetes outcomes.

The sources of algorithmic bias are multifaceted, encompassing data collection practices, label definitions, feature selection, model architecture choices, and optimization objectives \cite{wang2024ai, fahmy2025exploring}. Historical biases in healthcare access and documentation propagate through EHR data, with underserved populations having sparser, lower-quality records that degrade model performance \cite{oikonomou2023machine}. Imbalanced training datasets that overrepresent majority populations lead to models that prioritize their performance at the expense of minorities \cite{fahmy2025exploring}. Evaluation metrics that emphasize overall accuracy without considering fairness constraints enable models to achieve high aggregate performance while exhibiting substantial subgroup disparities \cite{wang2024ai}.

Addressing algorithmic bias requires interventions at multiple stages of the model development pipeline. Data collection efforts must prioritize diversity and representativeness, with explicit targets for inclusion of underrepresented populations \cite{fahmy2025exploring, bai2024federated}. Fairness-aware learning algorithms that incorporate demographic parity, equalized odds, or other fairness constraints into optimization objectives show promise but face trade-offs between fairness and overall accuracy \cite{fahmy2025exploring}. Post-hoc calibration and threshold adjustment can mitigate some disparities but do not address underlying model limitations \cite{wang2024ai}. Critically, fairness audits and mandatory reporting of subgroup performance must become standard practice in AI research and regulatory approval processes \cite{fahmy2025exploring, wang2024ai}.

\subsection{Generalization and Dataset Shift}

The ability of AI models to generalize beyond their training distribution represents a fundamental challenge in medical applications, where deployment settings often differ substantially from development environments \cite{prioleau2025deep, zhu2021deep}. Dataset shift—encompassing covariate shift, label shift, and concept drift—arises from differences in patient populations, clinical protocols, device characteristics, and temporal trends \cite{prioleau2025deep}.

Recent reproducibility studies provide compelling evidence of poor generalization in glucose prediction models \cite{prioleau2025deep}. When six representative deep learning architectures were evaluated across three public datasets with different diabetes management practices, conceptual reproducibility was poor despite good reproducibility when using identical code and evaluation datasets \cite{prioleau2025deep}. Performance degradation of 20-40\% in terms of root mean squared error was observed when models trained on one dataset were applied to others, even though all datasets comprised individuals with type 1 diabetes using similar CGM devices \cite{prioleau2025deep}.

The sources of generalization failure are diverse. Device heterogeneity introduces systematic differences in measurement characteristics, with CGM sensors from different manufacturers exhibiting distinct noise profiles, calibration algorithms, and temporal resolutions \cite{alhaddad2022sense, prioleau2025deep}. Clinical protocol variations—including insulin regimens, meal patterns, and physical activity levels—create distribution shifts that models fail to accommodate \cite{prioleau2025deep}. Temporal drift in patient behavior, disease progression, and treatment practices over time degrades model performance even within the same population \cite{jacobs2023artificial}.

Retinal imaging systems face analogous generalization challenges. Models trained on high-quality images from research-grade fundus cameras exhibit substantial performance degradation when applied to images from portable, low-cost devices used in community screening programs \cite{bai2024federated}. Image quality variations related to lighting conditions, pupil dilation, operator expertise, and patient cooperation introduce distribution shifts that models struggle to handle \cite{scheideman2025machine}. Geographic and demographic differences in disease prevalence and presentation further complicate generalization \cite{wang2024ai}.

The limited availability of external validation studies represents a critical gap in current literature. Most published models are evaluated only on held-out test sets from the same distribution as training data, providing optimistic estimates of real-world performance \cite{prioleau2025deep, jacobs2023artificial}. The few studies that conduct external validation on independent datasets consistently report substantial performance degradation, highlighting the fragility of current approaches \cite{prioleau2025deep, wang2024ai}. The overreliance on a small number of public datasets (e.g., OhioT1DM for glucose prediction, EyePACS for diabetic retinopathy) further limits the diversity of evaluation scenarios and may lead to overfitting to dataset-specific characteristics \cite{prioleau2025deep}.

Improving generalization requires fundamental shifts in model development and evaluation practices. Domain adaptation and transfer learning techniques that explicitly account for distribution shift show promise but remain underutilized in diabetes applications \cite{contreras2018artificial, fahmy2025exploring}. Multi-site training on diverse datasets can improve robustness but requires addressing data sharing barriers and harmonization challenges \cite{bai2024federated}. Federated learning enables collaborative model development without centralizing data, offering a pathway for training on heterogeneous populations while preserving privacy \cite{fahmy2025exploring, bai2024federated}. Critically, external validation on multiple independent datasets from diverse clinical settings must become a prerequisite for publication and regulatory approval \cite{prioleau2025deep, wang2024ai}.

\subsection{Interpretability and Explainability}

The opacity of deep learning models represents a fundamental barrier to clinical adoption, as healthcare providers require transparent reasoning to trust AI-generated recommendations and integrate them into decision-making processes \cite{jacobs2023artificial, mackenzie2023diabetes}. The "black box" nature of neural networks, particularly deep architectures with millions of parameters, makes it difficult to understand why specific predictions are made or to identify potential failure modes \cite{zhu2021deep, alam2024machine}.

Interpretability challenges are particularly acute in safety-critical applications such as insulin dosing recommendations, where erroneous predictions can lead to life-threatening hypoglycemia or hyperglycemia \cite{contreras2018artificial, mackenzie2023diabetes}. Clinicians need to understand not only what the model predicts but also the confidence of predictions, the key factors driving recommendations, and how predictions align with or diverge from clinical guidelines \cite{jacobs2023artificial}. The lack of interpretability impedes error analysis, limits opportunities for model improvement, and raises liability concerns when adverse events occur \cite{mackenzie2023diabetes}.

Various approaches to post-hoc interpretability have been developed, including feature importance scores, saliency maps, attention visualization, and counterfactual explanations \cite{zhu2021deep, alam2024machine}. Feature importance methods such as SHAP (SHapley Additive exPlanations) quantify the contribution of individual input features to model predictions, providing insights into which clinical variables or sensor measurements drive decisions \cite{oikonomou2023machine}. Attention mechanisms in Transformer architectures enable visualization of which time points or image regions the model focuses on, offering partial transparency into the reasoning process \cite{alam2024machine}.

However, these interpretability methods have significant limitations. Feature importance scores may be unstable across similar inputs or misleading when features are correlated \cite{jacobs2023artificial}. Saliency maps for image models often highlight spurious patterns or artifacts rather than clinically meaningful features \cite{zhu2021deep}. Attention weights do not necessarily correspond to causal relationships or clinical reasoning \cite{alam2024machine}. Counterfactual explanations that describe how inputs would need to change to alter predictions may suggest clinically infeasible or harmful interventions \cite{mackenzie2023diabetes}.

Intrinsically interpretable models offer an alternative to post-hoc explanation methods. Decision trees, rule-based systems, and linear models provide transparent decision logic but sacrifice predictive performance on complex tasks \cite{contreras2018artificial, alam2024machine}. Hybrid approaches that combine interpretable models with deep learning components aim to balance transparency and accuracy but face challenges in maintaining coherent explanations across model components \cite{zhu2021deep}.

The tension between interpretability and performance represents a fundamental trade-off in AI system design. Highly accurate deep learning models may be opaque, while transparent models may lack the capacity to capture complex patterns necessary for clinical utility \cite{jacobs2023artificial, zhu2021deep}. The appropriate balance depends on the specific application, with safety-critical decisions requiring greater transparency than screening or risk stratification tasks \cite{mackenzie2023diabetes}.

Regulatory frameworks increasingly emphasize the importance of interpretability and explainability for medical AI systems. The European Union's AI Act and FDA guidance on clinical decision support software highlight transparency, validation, and human oversight as key requirements \cite{mackenzie2023diabetes}, \cite{khalifa2024artificial}. However, the lack of standardized metrics for interpretability and consensus on what constitutes adequate explanation complicates regulatory evaluation \cite{jacobs2023artificial}.

\subsection{Reproducibility and Reporting Standards}

The reproducibility crisis in AI research extends to diabetes applications, with substantial heterogeneity in methodological rigor, evaluation protocols, and reporting practices \cite{prioleau2025deep, jacobs2023artificial}. A recent systematic assessment of 60 deep learning papers for glucose prediction found that code availability, overreliance on single public datasets, and limited use of multiple datasets for evaluation are among the top challenges to reproducibility \cite{prioleau2025deep}.

Inconsistent reporting of data preprocessing, feature engineering, hyperparameter selection, and model architecture details impedes replication and comparison across studies \cite{jacobs2023artificial, prioleau2025deep}. The lack of standardized evaluation metrics and protocols leads to incomparable results, with studies using different prediction horizons, error metrics, and statistical tests \cite{jacobs2023artificial}. Demographic characteristics of study populations are often unreported or incompletely described, obscuring potential biases and limiting generalizability assessment \cite{wang2024ai, prioleau2025deep}.

Addressing reproducibility challenges requires adoption of reporting standards such as TRIPOD (Transparent Reporting of a multivariable prediction model for Individual Prognosis Or Diagnosis) and CONSORT-AI (Consolidated Standards of Reporting Trials—Artificial Intelligence) \cite{wang2024ai}. Mandatory code and data sharing, where feasible given privacy constraints, enables independent validation and accelerates scientific progress \cite{prioleau2025deep}. Standardized benchmark datasets with well-defined train-test splits and evaluation protocols facilitate fair comparison across methods \cite{prioleau2025deep, jacobs2023artificial}.
