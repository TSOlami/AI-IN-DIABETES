\section{Model Limitations: Bias, Generalization, and Interpretability}

Benchmark results tell only part of the story. When diabetes AI systems move from research papers into clinical practice, they encounter problems that standard evaluations often miss. This section examines three persistent challenges: systematic biases that disadvantage certain patient groups, poor performance when models encounter conditions different from training, and the difficulty clinicians face in understanding why a model makes particular recommendations.

\subsection{Algorithmic Bias and Fairness}

AI systems do not treat all patients equally. Systematic performance gaps across demographic groups represent one of the most troubling findings in recent diabetes AI research \cite{wang2024ai, prioleau2025deep}. Prioleau et al. \cite{prioleau2025deep} tested six well-known glucose prediction models and found that every single one made larger errors for women than for men. This was not a quirk of one dataset or one model; the pattern held across three different patient cohorts and multiple neural network architectures. The consistency suggests a fundamental problem with how these systems are built and evaluated.

Retinal screening systems show similar disparities. Most training datasets come from high-income countries, and models may perform differently when applied to underrepresented populations \cite{wang2024ai, olusanya2024mitigating, zhang2025systematic}. However, the picture is not uniformly bleak. Phene et al. \cite{phene2019artificial} validated a deep learning system in Zambia and found clinically acceptable accuracy, while Wolf et al. \cite{wolf2024access} demonstrated effective AI screening in diverse pediatric populations. These successes suggest that bias is not inevitable, but requires deliberate attention.

Perhaps most troubling is the pattern Prioleau et al. \cite{prioleau2025deep} and Chen et al. \cite{chen2024algorithmic} documented: models consistently underperform for patients with poor glucose control. These are precisely the patients who need the most help. If AI systems work best for people who are already doing well, they risk widening the gap between those who manage their diabetes effectively and those who struggle \cite{haider2024algorithmic, liu2025scoping}.

Where does bias come from? The sources are numerous and interconnected \cite{wang2024ai, fahmy2025exploring}. Training datasets overrepresent certain populations. Patients with better healthcare access generate more complete records. Optimization algorithms that maximize overall accuracy may sacrifice performance on minority subgroups. Standard evaluation metrics hide these problems by averaging across all patients \cite{oikonomou2023machine}.

Fixing bias requires action at every stage of model development. Training data must include diverse populations, with explicit targets rather than hopeful assumptions \cite{fahmy2025exploring, bai2024federated}. Fairness-aware algorithms can optimize for equitable performance across groups, though this often involves trade-offs with aggregate accuracy \cite{fahmy2025exploring}. Most importantly, fairness audits must become standard practice, with results broken down by demographic characteristics and clinical severity \cite{wang2024ai, liu2025scoping}.

\subsection{Generalization and Dataset Shift}

A model that performs well on its test set may still fail in practice. The challenge of generalization plagues medical AI, where the conditions of deployment rarely match those of development \cite{prioleau2025deep, zhu2021deep, zhou2023population}.

The evidence of generalization failure is stark. When Prioleau et al. \cite{prioleau2025deep} applied glucose prediction models trained on one dataset to patients from different studies, they found poor conceptual reproducibility despite good reproducibility when using the same code and evaluation dataset. All the patients had type 1 diabetes; all used similar CGM devices. The differences were in clinical practice: how aggressively doctors titrated insulin, what patients typically ate, how active they were. Models learned patterns specific to their training environment that did not transfer.

Device differences compound the problem. CGM sensors from different manufacturers have distinct noise characteristics, sampling rates, and calibration behaviors \cite{alhaddad2022sense, prioleau2025deep, xie2024processing}. A model optimized for Dexcom may struggle with Abbott data. Retinal cameras vary even more dramatically: images from research-grade devices look nothing like those from portable smartphone attachments used in community screening \cite{zhang2025systematic, raj2024federated}. Models that perform beautifully on clean research data may fail with the lower-quality images common in real screening programs.

Most concerning is the lack of external validation in published research. Studies typically report results on held-out data from the same source as training data, providing an optimistic view of real-world performance \cite{prioleau2025deep, jacobs2023artificial}. The few papers that validate on truly independent datasets consistently find substantial degradation \cite{prioleau2025deep, wang2024ai}. Over-reliance on a handful of public benchmarks like OhioT1DM and EyePACS may have led the field to overfit to these specific datasets rather than building truly robust systems \cite{prioleau2025deep}.

Improving generalization requires fundamental changes in how research is conducted. Domain adaptation techniques can help models adjust to new environments \cite{contreras2018artificial, chen2024crossmodality}. Training on data from multiple sites increases robustness \cite{bai2024federated}. Federated learning enables this without centralizing sensitive data \cite{fahmy2025exploring, raj2024federated}. External validation on independent datasets must become mandatory for publication and regulatory approval \cite{prioleau2025deep, wang2024ai}.

\subsection{Interpretability and Explainability}

Clinicians need to understand why a model makes a particular recommendation. When an AI system suggests adjusting insulin based on CGM patterns, physicians want to know which patterns drove the suggestion and whether the reasoning makes clinical sense \cite{jacobs2023artificial, mackenzie2023diabetes}. For safety-critical decisions, this is not optional. A model that predicts hypoglycemia but cannot explain why is hard to trust, and harder to debug when it fails \cite{contreras2018artificial, mackenzie2023diabetes}.

Researchers have developed various tools to peer inside black-box models. SHAP values quantify how much each input feature contributes to a prediction, revealing which variables matter most \cite{lundberg2017shap, munoz2023shap, chadaga2024interpretable}. Attention weights show which parts of a time series or image the model focuses on \cite{alam2024machine, bhati2024interpretable}. Saliency maps highlight regions of retinal images that influenced a diagnosis \cite{zhu2021deep}.

These methods help but fall short of true understanding. SHAP values can be unstable, giving different explanations for similar inputs \cite{jacobs2023artificial}. Attention weights do not necessarily reflect causal importance \cite{alam2024machine}. Saliency maps sometimes highlight artifacts or irrelevant features rather than clinically meaningful regions \cite{zhu2021deep}. Counterfactual explanations that describe what would need to change to alter a prediction may suggest modifications that are clinically impossible \cite{mackenzie2023diabetes}.

An alternative is to use inherently interpretable models: decision trees, rule-based systems, or linear combinations that expose their logic directly \cite{contreras2018artificial, alam2024machine}. The trade-off is typically lower accuracy on complex tasks. Hybrid approaches try to combine interpretable components with powerful but opaque modules, though maintaining coherent explanations across the combination remains challenging \cite{zhu2021deep}.

Regulators are paying attention. The EU AI Act and FDA guidance emphasize transparency and human oversight for medical AI \cite{mackenzie2023diabetes, khalifa2024artificial}. However, there is no consensus on what constitutes adequate explanation, and no standardized metrics for interpretability \cite{jacobs2023artificial}. This regulatory uncertainty complicates approval pathways and deployment decisions.

\subsection{Reproducibility and Reporting Standards}

The diabetes AI literature suffers from a reproducibility problem. Prioleau et al. \cite{prioleau2025deep} reviewed 60 deep learning papers for glucose prediction and found widespread issues: code rarely available, methods described incompletely, results reported only on familiar benchmark datasets. Different studies use different preprocessing pipelines, different evaluation metrics, and different ways of splitting data into training and test sets, making meaningful comparison nearly impossible \cite{jacobs2023artificial}.

Demographic reporting is particularly poor. Many papers do not describe the age, sex, or ethnic composition of their study populations \cite{wang2024ai, prioleau2025deep}. Without this information, readers cannot assess whether results might generalize to their own patients or whether bias might be a concern.

The path forward requires adoption of established reporting guidelines. TRIPOD provides standards for clinical prediction models; CONSORT-AI adapts clinical trial standards for AI studies \cite{wang2024ai, liu2025scoping}. Journals and funding agencies should require compliance. Code and data sharing, where privacy permits, enables independent verification \cite{prioleau2025deep}. Standard benchmarks with fixed evaluation protocols would allow fair comparison across methods \cite{prioleau2025deep, jacobs2023artificial}. These changes would not solve every problem, but they would make the literature more trustworthy and the science more cumulative.
