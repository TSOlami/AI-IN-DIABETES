\section{Deployment and Accessibility Challenges in Low-Resource Settings}

Moving AI from research labs into clinical practice is difficult everywhere, but the obstacles multiply in settings with limited resources. Low- and middle-income countries (LMICs) bear a disproportionate share of the global diabetes burden yet have received little attention in AI development \cite{sun2022idf, olusanya2024mitigating}. Examining these settings reveals weaknesses in current AI approaches that would otherwise remain hidden and points toward solutions that could benefit patients everywhere.

\subsection{Infrastructure and Connectivity Constraints}

Most AI systems assume conditions that do not exist in much of the world. Cloud-based analytics require stable internet; continuous glucose monitoring requires reliable electricity; sophisticated algorithms require computing power \cite{mackenzie2023diabetes, ghosh2025artificial, desai2025barriers}. Power outages interrupt data collection. Slow or expensive internet makes real-time cloud processing impractical. Eze et al. \cite{eze2022sustainability} reviewed digital health interventions in LMICs and found that infrastructure failures were among the most common reasons for failed implementations.

Health information technology lags behind as well. Paper records remain standard in many facilities, meaning the longitudinal electronic data that AI systems need simply does not exist \cite{raj2024federated, ghosh2025artificial}. Where EHRs are present, they often lack standardized coding, unique patient identifiers, and interoperability with other systems \cite{mackenzie2023diabetes, olusanya2024mitigating}.

Mobile phones offer a partial workaround. Even in areas with poor infrastructure, smartphone penetration has grown rapidly, and mobile apps can deliver decision support and patient education without requiring extensive clinical IT \cite{rodriguez2023applications, ghosh2025artificial, alam2024machine}. However, smartphones are not universal, and using them effectively for health requires digital literacy that many patients lack \cite{mackenzie2023diabetes}.

\subsection{Device Costs and Economic Barriers}

The devices that generate data for AI systems remain far too expensive for most patients in LMICs. CGM systems cost \$200-400 per month in high-income countries, an impossible expense where annual health spending per person is often below \$100 \cite{ghosh2025artificial, idf2021atlas}. Even basic glucometers and test strips create ongoing costs that many families cannot sustain \cite{alhaddad2022sense}.

Retinal imaging faces similar economics. Research-grade fundus cameras cost tens of thousands of dollars; even portable alternatives run \$1,000-5,000 \cite{raj2024federated}. The cheaper devices produce lower-quality images, which degrades AI performance, creating a tradeoff between accessibility and accuracy \cite{zhang2025systematic}. The shortage of trained camera operators and ophthalmologists compounds the problem \cite{zhang2025systematic, guan2023artificial}.

Long-term, AI might reduce costs by preventing expensive complications like blindness and amputation. But the upfront investment in devices, infrastructure, and training may be prohibitive for health systems already stretched thin \cite{mackenzie2023diabetes, guan2023artificial}. Creative financing, tiered pricing, and technology transfer will be necessary to make AI accessible where it could help most \cite{ghosh2025artificial}.

\subsection{Data Scarcity and Population Representativeness}

AI models learn from data, and the data overwhelmingly comes from wealthy countries. Raj et al. \cite{raj2024federated} and Olusanya et al. \cite{olusanya2024mitigating} document the severe underrepresentation of LMIC populations in publicly available training sets. Models optimized for European or North American patients may not work well for people with different genetics, diets, and disease patterns \cite{wang2024ai, zhang2025systematic}.

This is not merely a theoretical concern. Diabetic retinopathy prevalence and progression differ across populations \cite{yau2012global, zhang2025systematic}. Dietary patterns vary enormously; a model trained on patients eating Western diets may give poor glucose predictions for someone eating traditional foods from another region \cite{ghosh2025artificial}. Successful deployment requires local validation, yet the resources for such studies are scarce.

Some paths forward exist. Transfer learning can adapt models trained on abundant data to new populations with limited local examples \cite{contreras2018artificial, chen2024crossmodality}. Federated learning enables collaboration across institutions without centralizing sensitive data \cite{bai2024federated, fahmy2025exploring}. Phene et al. \cite{phene2019artificial} demonstrated that a retinal screening system trained primarily in Singapore performed well in Zambia, suggesting that careful validation can confirm generalization even without massive local datasets. But the risk of negative transfer, where assumptions from one setting actually hurt performance in another, remains real \cite{prioleau2025deep}.

\subsection{Clinical Validation and Regulatory Frameworks}

Proving that AI works in a new setting requires clinical trials, and trials in low-resource environments face substantial hurdles \cite{mackenzie2023diabetes, fahmy2025exploring}. Randomized controlled trials demand infrastructure, funding, and expertise that may not be available. Pragmatic implementation studies can fill some gaps but struggle to establish causality \cite{mackenzie2023diabetes}.

Regulatory frameworks add another layer of complexity. Many LMICs lack specific guidelines for AI-based medical devices \cite{mackenzie2023diabetes, khalifa2024artificial}. Developers face uncertainty about approval pathways, and patients may use unvalidated systems. At the same time, regulations designed for well-resourced health systems may impose requirements that are impractical or counterproductive in different contexts \cite{ghosh2025artificial}.

Evidence from high-income settings does not automatically apply elsewhere. Disease patterns, healthcare infrastructure, and patient populations differ enough that local validation is essential \cite{cappon2024digital, fahmy2025exploring}. Building the capacity for such validation is itself a major undertaking \cite{mackenzie2023diabetes}.

\subsection{Workforce Capacity and Training}

AI tools require people who can use them. In many LMICs, the specialists who could oversee AI-assisted care are in short supply. There are too few endocrinologists to manage complex diabetes cases, too few ophthalmologists to interpret screening results, and too few data scientists to maintain and troubleshoot AI systems \cite{guan2023artificial, ghosh2025artificial, raj2024federated}.

Primary care workers deliver most diabetes care in these settings, and they may lack the training or confidence to integrate AI tools into their practice \cite{mackenzie2023diabetes}. Clinician hesitancy is understandable: concerns about accuracy, liability, and workflow disruption are legitimate, and aggressive technology promotion without adequate support breeds resistance \cite{guan2023artificial}. Building trust requires hands-on training, transparent communication about what AI can and cannot do, and evidence that tools actually help patients \cite{jacobs2023artificial, mackenzie2023diabetes}.

Long-term solutions involve embedding AI education in medical and nursing curricula and creating frameworks that support local capacity building \cite{mackenzie2023diabetes, bahmani2025achieving}. Bahmani et al. \cite{bahmani2025achieving} propose integrating research, education, and clinical practice to democratize AI knowledge. This approach recognizes that technology alone cannot achieve equity; sustainable change requires trained people and strong institutions.

\subsection{Ethical and Socio-Cultural Considerations}

Deploying AI developed elsewhere raises ethical questions. Systems built and validated in high-income settings may not address local priorities or respect local values \cite{ghosh2025artificial, mackenzie2023diabetes}. Using such systems without adequate local validation risks a form of technological colonialism, where solutions are imposed rather than developed in partnership \cite{ghosh2025artificial}.

Privacy concerns are heightened where data protection laws are weak and institutional capacity for security is limited \cite{mackenzie2023diabetes, khalifa2024artificial}. Commercial interests in health data require scrutiny, and patients must give meaningful informed consent \cite{fahmy2025exploring}. Community engagement and participatory governance can help ensure that AI development aligns with local needs and values \cite{mackenzie2023diabetes}.

Cultural factors matter as well. Dietary recommendations must account for local foods and religious practices; interface design must accommodate language and literacy differences \cite{ghosh2025artificial, mackenzie2023diabetes}. AI systems designed with Western patients in mind may be unusable or inappropriate elsewhere.

\subsection{Pathways to Equitable AI Deployment}

Making AI work in low-resource settings requires deliberate effort on multiple fronts:

\begin{itemize}
    \item \textbf{Technology adaptation:} Design devices and algorithms for resource-constrained environments, including edge computing that works without constant internet and models that remain useful with lower-quality input data \cite{fahmy2025exploring, ghosh2025artificial, zhu2024population}.
    \item \textbf{Capacity building:} Invest in training healthcare workers, researchers, and data scientists who can adapt, validate, and maintain AI systems locally \cite{mackenzie2023diabetes, bahmani2025achieving}.
    \item \textbf{Collaborative research:} Use federated learning and multi-site partnerships to include diverse populations in model development while respecting data sovereignty and privacy \cite{bai2024federated, fahmy2025exploring, raj2024federated}.
    \item \textbf{Inclusive design:} Involve patients, clinicians, and communities in developing systems to ensure cultural appropriateness and practical usability \cite{mackenzie2023diabetes, bahmani2025achieving}.
    \item \textbf{Policy innovation:} Create regulatory frameworks and financing mechanisms suited to local contexts, balancing safety requirements with the need for accessible technology \cite{mackenzie2023diabetes, khalifa2024artificial}.
\end{itemize}

Without attention to these priorities, the benefits of AI in diabetes care will accrue primarily to those who already have good access to healthcare, widening rather than narrowing global health disparities \cite{fahmy2025exploring, wang2024ai, haider2024algorithmic}.
