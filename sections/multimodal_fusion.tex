\section{Multimodal Learning and Data Fusion}

The integration of heterogeneous data sources—combining wearable sensor time-series, retinal images, electronic health records, and genomic data—represents a frontier in AI-driven diabetes care, promising to capture the multifaceted nature of disease progression and treatment response. However, multimodal learning introduces substantial technical and methodological challenges that extend beyond single-modality approaches.

\subsection{Fusion Strategies and Architectures}

Multimodal fusion strategies can be broadly categorized into early fusion, late fusion, and hybrid approaches \cite{contreras2018artificial, wang2024ai}. Early fusion (feature-level fusion) concatenates features from different modalities before feeding them into a unified model, enabling the learning of cross-modal interactions but requiring careful normalization and alignment of heterogeneous feature spaces \cite{bai2024federated}. Late fusion (decision-level fusion) trains separate models for each modality and combines their predictions through voting, averaging, or meta-learning, offering modularity and robustness to missing modalities but potentially missing important cross-modal dependencies \cite{wang2024ai}.

Hybrid fusion architectures employ intermediate fusion strategies, learning modality-specific representations through specialized sub-networks before integrating them at intermediate layers \cite{bai2024federated}, \cite{fahmy2025exploring}, \cite{maqsood2025gluconet}. Attention-based fusion mechanisms enable models to dynamically weight the contribution of different modalities based on their relevance and reliability for specific predictions \cite{kulkarni2025diabetes}, \cite{maqsood2025gluconet}, \cite{xiong2024multimodal}. Cross-modal alignment techniques, such as canonical correlation analysis or contrastive learning, learn shared representations that capture complementary information across modalities \cite{bai2024federated}.

Recent work on federated multimodal AI demonstrates the potential of cross-modal co-learning frameworks that leverage complementary information from CGM time-series and retinal imaging for improved risk stratification and complication prediction \cite{bai2024federated}. These approaches show promise for integrating diverse data sources while preserving privacy through federated learning paradigms \cite{bai2024federated}. However, the computational complexity and data requirements of multimodal models remain substantial barriers to widespread adoption.

\subsection{Temporal Alignment and Missing Modalities}

A critical challenge in multimodal diabetes data is the temporal misalignment of different data sources. CGM provides continuous measurements at minute-scale resolution, while retinal imaging occurs at annual or semi-annual intervals, and laboratory tests follow irregular schedules determined by clinical protocols \cite{contreras2018artificial, oikonomou2023machine}. Aligning these heterogeneous temporal scales requires sophisticated interpolation, aggregation, or attention mechanisms that can handle irregular sampling and variable time lags \cite{zhu2021deep}.

Missing modalities represent a pervasive challenge in real-world deployments. Not all patients have access to CGM devices, retinal imaging may be unavailable in resource-limited settings, and EHR completeness varies substantially across healthcare systems \cite{mackenzie2023diabetes, ghosh2025artificial}. Models trained on complete multimodal data often fail catastrophically when deployed in settings where only a subset of modalities is available \cite{wang2024ai}. Robust multimodal architectures must gracefully degrade performance rather than failing completely when modalities are missing, requiring specialized training strategies such as modality dropout or auxiliary reconstruction tasks \cite{bai2024federated}.

\subsection{Cross-Modal Transfer and Domain Adaptation}

Transfer learning across modalities offers a pathway for leveraging information from data-rich modalities to improve performance on data-scarce modalities \cite{contreras2018artificial, fahmy2025exploring}. For example, representations learned from large-scale retinal imaging datasets can be transferred to improve glucose prediction from limited CGM data by capturing shared physiological processes related to vascular health and metabolic dysfunction \cite{bai2024federated}. Cross-modal domain adaptation techniques that minimize distribution shift between source and target modalities show promise for improving generalization \cite{fahmy2025exploring}.

However, the theoretical foundations and empirical validation of cross-modal transfer in medical applications remain limited. The assumption that different modalities capture related aspects of disease progression may not hold uniformly across populations or clinical contexts \cite{wang2024ai}. Negative transfer, where knowledge from source modalities degrades performance on target tasks, represents a significant risk that requires careful validation \cite{prioleau2025deep}.

\subsection{Interpretability and Clinical Integration}

The complexity of multimodal models exacerbates interpretability challenges inherent in deep learning approaches. Clinicians require transparent explanations of how different data sources contribute to predictions, particularly when recommendations conflict with clinical intuition or established guidelines \cite{jacobs2023artificial, mackenzie2023diabetes}. Attention visualization, saliency maps, and feature attribution methods provide partial insights but often fail to capture the complex interactions between modalities that drive model predictions \cite{zhu2021deep}.

The integration of multimodal AI systems into clinical workflows requires careful consideration of data availability, acquisition costs, and workflow disruption \cite{mackenzie2023diabetes, guan2023artificial}. Requiring multiple data modalities for model inference may create barriers to adoption if some modalities are expensive, invasive, or time-consuming to acquire \cite{ghosh2025artificial}. Hierarchical decision support systems that provide increasingly refined predictions as additional modalities become available offer a pragmatic approach to balancing performance and accessibility \cite{wang2024ai}.

\subsection{Evaluation Challenges and Reporting Standards}

Evaluating multimodal models presents unique challenges beyond single-modality approaches. Performance metrics must capture not only overall accuracy but also robustness to missing modalities, fairness across demographic subgroups, and calibration of uncertainty estimates \cite{wang2024ai, prioleau2025deep}. Ablation studies that systematically remove individual modalities or combinations thereof are essential for understanding the contribution of each data source and identifying potential redundancies \cite{jacobs2023artificial}.

Current literature exhibits substantial heterogeneity in evaluation protocols, with inconsistent reporting of data preprocessing, train-test splits, hyperparameter selection, and demographic characteristics of study populations \cite{prioleau2025deep, jacobs2023artificial}. The lack of standardized benchmarks and public multimodal datasets hinders reproducibility and comparison across studies \cite{prioleau2025deep}. Recent efforts to establish reporting guidelines and reproducibility standards represent important steps toward more rigorous evaluation practices \cite{jacobs2023artificial, prioleau2025deep}.

Figure~\ref{fig:multimodal_pipeline} illustrates a conceptual framework for multimodal AI in diabetes care, highlighting the integration of diverse data sources, fusion strategies, and clinical decision support outputs.

\begin{figure*}[t]
\centering
\includegraphics[width=0.95\textwidth]{figures/multimodal_pipeline.pdf}
\caption{Conceptual framework for multimodal AI in diabetes care. The pipeline integrates heterogeneous data sources (wearable sensors, retinal imaging, EHR, genomic data) through modality-specific feature extraction, cross-modal fusion, and predictive modeling to support clinical decision-making. Key challenges include temporal alignment, missing modalities, interpretability, and generalization across populations and clinical settings.}
\label{fig:multimodal_pipeline}
\end{figure*}
