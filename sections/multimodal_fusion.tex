\section{Multimodal Learning and Data Fusion}

Diabetes affects multiple organ systems and unfolds over years, leaving traces in blood sugar patterns, retinal blood vessels, kidney function, and countless other measurements. Combining these diverse data sources through multimodal AI could capture the full picture of disease progression in ways that single data types cannot. Recent studies suggest this approach holds genuine promise: Lee et al. \cite{lee2024multimodal} and Singh et al. \cite{singh2024medfusion} have demonstrated that integrating EHR data with wearable signals improves prediction accuracy compared to using either source alone. Yet the technical challenges of multimodal learning extend well beyond those of single-modality approaches.

\subsection{Fusion Strategies and Architectures}

Researchers combine data sources in three main ways \cite{contreras2018artificial, wang2024ai}. Early fusion merges features from all modalities at the start, feeding a combined representation into a single model. This allows the model to learn how different data types interact but requires careful preprocessing to put glucose numbers, image features, and clinical codes on comparable scales \cite{bai2024federated}. Late fusion takes the opposite approach: train separate models for each data type, then combine their predictions through voting or averaging. This keeps modalities independent and handles missing data gracefully, though it may miss important connections between them \cite{wang2024ai}.

Hybrid approaches seek a middle ground. Attention mechanisms let models learn which data sources matter most for each prediction, dynamically weighting CGM readings more heavily when glucose is fluctuating or relying more on retinal images when assessing long-term vascular health \cite{kulkarni2025diabetes, maqsood2025gluconet, xiong2024multimodal}. Bai et al. \cite{bai2024federated} demonstrated a federated multimodal framework that combines CGM and retinal data for complication risk prediction, achieving strong results while keeping patient data decentralized across institutions.

These advances come at a cost. Multimodal models demand more data, more computation, and more careful engineering than their single-modality counterparts. Whether the performance gains justify this complexity in practice remains an open question.

\subsection{Temporal Alignment and Missing Modalities}

Different data types arrive on radically different schedules. CGM generates measurements every few minutes; retinal exams happen once a year; lab tests occur whenever a patient visits a clinic \cite{contreras2018artificial, oikonomou2023machine}. Aligning these timescales requires deciding how to summarize continuous glucose data (daily averages? time in target range?) and how to handle the long gaps between imaging studies \cite{zhu2021deep}. No consensus has emerged on the best approach.

Missing modalities create even larger problems. In wealthy healthcare systems, a patient might have CGM data, regular eye exams, and comprehensive EHR records. In a rural clinic, only basic lab tests may be available \cite{mackenzie2023diabetes, ghosh2025artificial}. Models trained to expect all data types often fail completely when some are unavailable \cite{wang2024ai}. Robust systems must degrade gracefully, providing useful predictions from whatever information exists. Techniques like modality dropout during training can help, teaching models to cope with incomplete data \cite{bai2024federated}. Still, current methods perform best when they have everything and worst when they have least, exactly the opposite of what equitable healthcare requires.

\subsection{Cross-Modal Transfer and Domain Adaptation}

Transfer learning across modalities offers an intriguing possibility: could patterns learned from abundant retinal imaging data improve glucose prediction from scarce CGM records? The underlying idea is that different measurements capture related aspects of the same disease process \cite{contreras2018artificial, fahmy2025exploring}. Retinal changes reflect long-term vascular damage; glucose patterns reflect metabolic control. Linking them through shared learned representations might help each task \cite{bai2024federated}.

Early results are promising but limited. Zhou et al. \cite{zhou2023population} showed that foundation models trained on diverse retinal images generalize better across populations than task-specific models. Chen et al. \cite{chen2024crossmodality} demonstrated cross-modality knowledge transfer for retinopathy grading. However, the theoretical foundations of cross-modal transfer remain shaky, and negative transfer, where importing knowledge from one modality actually hurts performance on another, is a real risk \cite{prioleau2025deep, wang2024ai}. Careful validation is essential before assuming that connections between modalities will help.

\subsection{Interpretability and Clinical Integration}

Multimodal models compound the interpretability problems that already plague deep learning. When a system combines glucose traces, retinal features, lab values, and medication history to recommend insulin adjustment, clinicians want to know why \cite{jacobs2023artificial, mackenzie2023diabetes}. Which data source drove the decision? Would the recommendation change if one input were different? Attention weights and saliency maps provide partial answers but rarely capture the full picture of how modalities interact \cite{zhu2021deep}.

Practical deployment raises additional concerns. If a model requires both CGM data and recent lab work to function, it cannot help patients who have one but not the other \cite{ghosh2025artificial}. The most useful systems would provide progressively better predictions as more data become available, rather than requiring everything upfront \cite{wang2024ai, mackenzie2023diabetes, guan2023artificial}. Building such hierarchical decision support remains a research challenge.

\subsection{Evaluation Challenges and Reporting Standards}

Evaluating multimodal systems demands more than reporting overall accuracy. Researchers must assess robustness to missing inputs, fairness across patient subgroups, and calibration of confidence estimates \cite{wang2024ai, prioleau2025deep}. Ablation studies that systematically remove each modality reveal which data sources actually matter and whether seemingly complementary inputs are redundant \cite{jacobs2023artificial}.

Current practice falls short of these ideals. Studies use different preprocessing pipelines, different train-test splits, and different outcome definitions, making comparison nearly impossible \cite{prioleau2025deep, jacobs2023artificial}. Demographic information is often missing from papers, hiding potential disparities \cite{wang2024ai}. No widely accepted multimodal diabetes benchmark exists. Until the field converges on standardized evaluation protocols, claims about multimodal benefits will remain difficult to verify \cite{prioleau2025deep}.

Figure~\ref{fig:multimodal_pipeline} illustrates a conceptual framework for multimodal AI in diabetes care, showing how diverse data sources flow through specialized processors before fusion and clinical decision support.

\begin{figure*}[t]
\centering
\includegraphics[width=0.95\textwidth]{figures/multimodal_pipeline.pdf}
\caption{Conceptual framework for multimodal AI in diabetes care. The pipeline integrates heterogeneous data sources (wearable sensors, retinal imaging, EHR, genomic data) through modality-specific feature extraction, cross-modal fusion, and predictive modeling to support clinical decision-making. Key challenges include temporal alignment, missing modalities, interpretability, and generalization across populations and clinical settings.}
\label{fig:multimodal_pipeline}
\end{figure*}
