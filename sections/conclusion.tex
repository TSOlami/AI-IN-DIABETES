\section{Conclusion}

Artificial intelligence offers real promise for diabetes care. Deep learning systems can detect diabetic retinopathy as accurately as specialists, predict dangerous glucose swings before they happen, and integrate diverse data streams that no human could process unaided \cite{zhang2025systematic, sun2021dualpathway, lee2024multimodal}. Several AI-based screening tools have received regulatory approval, and clinical trials are beginning to demonstrate real-world benefit \cite{wolf2024access, boucher2024implementation}. The enthusiasm surrounding AI in diabetes is not unfounded.

Yet this review documents a substantial gap between benchmark performance and clinical reality. Models that excel on test datasets routinely fail when applied to new populations, different devices, or unfamiliar clinical settings. Prioleau et al. \cite{prioleau2025deep} found poor conceptual reproducibility when glucose prediction models were applied across different datasets, even when all patients had type 1 diabetes and used similar CGM devices. Every model they tested performed worse for women than for men, and worse for patients with poor glucose control, exactly the people who need help most. Retinal screening systems show similar patterns of variable performance across populations and settings \cite{zhang2025systematic, olusanya2024mitigating}.

These are not minor technical problems to be solved with better algorithms. They reflect fundamental issues in how diabetes AI research is conducted: training data that overrepresent certain populations, evaluation metrics that hide subgroup disparities, and validation practices that rarely test models outside their original context \cite{wang2024ai, prioleau2025deep}. The field has optimized for benchmark leaderboards while neglecting the diversity and robustness that clinical deployment demands.

Low-resource settings expose these weaknesses most starkly. Infrastructure limitations, device costs, data scarcity, and workforce shortages create barriers that models trained in well-resourced environments cannot overcome \cite{desai2025barriers, eze2022sustainability, ghosh2025artificial}. Populations in low- and middle-income countries bear a disproportionate share of the global diabetes burden yet remain nearly invisible in AI training data \cite{sun2022idf, olusanya2024mitigating, raj2024federated}. Without deliberate effort to include these populations, AI risks concentrating benefits among those who already have good healthcare access while leaving the most vulnerable behind \cite{haider2024algorithmic, wang2024ai}.

The path forward requires change at every stage of research and development. Data collection must prioritize diversity, with explicit targets for underrepresented populations rather than convenient samples from accessible institutions \cite{bai2024federated, fahmy2025exploring}. Model development must incorporate fairness constraints and domain adaptation techniques that promote generalization \cite{fahmy2025exploring, chen2024crossmodality}. Evaluation must extend beyond aggregate metrics to include subgroup analysis, external validation, calibration assessment, and uncertainty quantification \cite{prioleau2025deep, jacobs2023artificial, liu2025scoping}.

Collaboration offers a way forward. Federated learning enables institutions worldwide to contribute to model training without sharing sensitive patient data, potentially bringing underrepresented populations into AI development while respecting privacy \cite{bai2024federated, raj2024federated}. Transfer learning and domain adaptation can leverage knowledge from data-rich settings to improve performance where local data are scarce, though validation remains essential to avoid negative transfer \cite{chen2024crossmodality, zhou2023population}. International partnerships can build local capacity for AI development and validation in settings that have been excluded from research \cite{mackenzie2023diabetes, bahmani2025achieving}.

Regulatory frameworks must evolve to accommodate AI's unique characteristics while maintaining safety \cite{mackenzie2023diabetes, khalifa2024artificial}. Educational initiatives must prepare healthcare workers to use, evaluate, and maintain AI tools \cite{bahmani2025achieving}. Economic evaluations must demonstrate value in resource-constrained settings where cost matters most \cite{guan2023artificial, ghosh2025artificial}.

Ultimately, the measure of AI in diabetes care should not be benchmark accuracy or the sophistication of architectures, but whether it improves health outcomes for people who need help. That requires systems that work reliably across populations, settings, and devices; that treat all patients fairly regardless of demographics or disease severity; that explain their reasoning in ways clinicians can understand and trust; and that remain accessible even where resources are limited.

The challenges are substantial, but so is the opportunity. Diabetes affects hundreds of millions of people worldwide and will affect hundreds of millions more in the coming decades \cite{sun2022idf}. AI systems that genuinely deliver on their promise could transform prevention, diagnosis, and management at a scale that human specialists alone cannot achieve. Realizing that potential requires the research community to prioritize equity and robustness alongside performance, engage diverse stakeholders in design and evaluation, and ensure that technological advances serve not just those who are easiest to reach, but all who need them.
