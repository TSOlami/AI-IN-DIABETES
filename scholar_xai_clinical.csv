"Paper Title","Paper Link","Publication Year","Publication Type","Publication Title","Author Names","DOI","PDF Link","Snippet","Abstract"
"Interpretability in healthcare: A comparative study of local machine learning interpretability techniques","https://scispace.com/paper/interpretability-in-healthcare-a-comparative-study-of-local-42n45qanuz","2019","Proceedings Article","Computer-Based Medical Systems","Radwa Elshawi
Youssef Sherif
Mouaz H. Al-Mallah
Sherif Sakr","10.1109/CBMS.2019.00065","https://www.researchgate.net/profile/Radwa-El-Shawi/publication/347161722_Interpretability_in_healthcare_A_comparative_study_of_local_machine_learning_interpretability_techniques/links/61cedd61b8305f7c4b12b33c/Interpretability-in-healthcare-A-comparative-study-of-local-machine-learning-interpretability-techniques.pdf","… interpretability techniques for machine learning models are an area focus of research. In general, the main aim of these interpretability … process of the machine learning models and to be …","Although complex machine learning models (e.g., Random Forest, Neural Networks) are commonly outperforming the traditional simple interpretable models (e.g., Linear Regression, Decision Tree), in the healthcare domain, clinicians find it hard to understand and trust these complex models due to the lack of intuition and explanation of their predictions. With the new General Data Protection Regulation (GDPR), the importance for plausibility and verifiability of the predictions made by machine learning models has become essential. To tackle this challenge, recently, several machine learning interpretability techniques have been developed and introduced. In general, the main aim of these interpretability techniques is to shed light and provide insights into the predictions process of the machine learning models and explain how the model predictions have resulted. However, in practice, assessing the quality of the explanations provided by the various interpretability techniques is still questionable. In this paper, we present a comprehensive experimental evaluation of three recent and popular local model agnostic interpretability techniques, namely, LIME, SHAP and Anchors on different types of real-world healthcare data. Our experimental evaluation covers different aspects for its comparison including identity, stability, separability, similarity, execution time and bias detection. The results of our experiments show that LIME achieves the lowest performance for the identity metric and the highest performance for the separability metric across all datasets included in this study. On average, SHAP has the smallest average time to output explanation across all datasets included in this study. For detecting the bias, SHAP enables the participants to better detect the bias."
"Diabetes prediction using machine learning and explainable AI techniques","https://scispace.com/paper/diabetes-prediction-using-machine-learning-and-explainable-cvwfgi29","2022","Journal Article","Healthcare technology letters","Isfafuzzaman Tasin
Tansin Ullah Nabil
Sanjida Islam
Riasat Khan","10.1049/htl2.12039","https://ietresearch.onlinelibrary.wiley.com/doi/pdf/10.1049/htl2.12039","… the existing dataset, and apply an explainable AI technique. In this paper, we have employed machine learning and explainable AI techniques to detect diabetes. Along with a private …","Globally, diabetes affects 537 million people, making it the deadliest and the most common non-communicable disease. Many factors can cause a person to get affected by diabetes, like excessive body weight, abnormal cholesterol level, family history, physical inactivity, bad food habit etc. Increased urination is one of the most common symptoms of this disease. People with diabetes for a long time can get several complications like heart disorder, kidney disease, nerve damage, diabetic retinopathy etc. But its risk can be reduced if it is predicted early. In this paper, an automatic diabetes prediction system has been developed using a private dataset of female patients in Bangladesh and various machine learning techniques. The authors used the Pima Indian diabetes dataset and collected additional samples from 203 individuals from a local textile factory in Bangladesh. Feature selection algorithm mutual information has been applied in this work. A semi-supervised model with extreme gradient boosting has been utilized to predict the insulin features of the private dataset. SMOTE and ADASYN approaches have been employed to manage the class imbalance problem. The authors used machine learning classification methods, that is, decision tree, SVM, Random Forest, Logistic Regression, KNN, and various ensemble techniques, to determine which algorithm produces the best prediction results. After training on and testing all the classification models, the proposed system provided the best result in the XGBoost classifier with the ADASYN approach with 81% accuracy, 0.81 F1 coefficient and AUC of 0.84. Furthermore, the domain adaptation method has been implemented to demonstrate the versatility of the proposed system. The explainable AI approach with LIME and SHAP frameworks is implemented to understand how the model predicts the final results. Finally, a website framework and an Android smartphone application have been developed to input various features and predict diabetes instantaneously. The private dataset of female Bangladeshi patients and programming codes are available at the following link: https://github.com/tansin-nabil/Diabetes-Prediction-Using-Machine-Learning."
"Interpretability of machine learning‐based prediction models in healthcare","https://scispace.com/paper/interpretability-of-machine-learning-based-prediction-models-3lv7zarxj6","2020","Journal Article","Wiley Interdisciplinary Reviews-Data Mining and Knowledge Discovery","Gregor Stiglic
Primoz Kocbek
Nino Fijačko
Marinka Zitnik
Katrien Verbert
Leona Cilar","10.1002/WIDM.1379","https://arxiv.org/pdf/2002.08596","… of interpretability methods for prediction models in healthcare, with a focus on usability from an end-user perspective. In contrast to many other fields, decisions in healthcare are high-…","There is a need of ensuring machine learning models that are interpretable. Higher interpretability of the model means easier comprehension and explanation of future predictions for end-users. Further, interpretable machine learning models allow healthcare experts to make reasonable and data-driven decisions to provide personalized decisions that can ultimately lead to higher quality of service in healthcare. Generally, we can classify interpretability approaches in two groups where the first focuses on personalized interpretation (local interpretability) while the second summarizes prediction models on a population level (global interpretability). Alternatively, we can group interpretability methods into model-specific techniques, which are designed to interpret predictions generated by a specific model, such as a neural network, and model-agnostic approaches, which provide easy-to-understand explanations of predictions made by any machine learning model. Here, we give an overview of interpretability approaches and provide examples of practical interpretability of machine learning in different areas of healthcare, including prediction of health-related outcomes, optimizing treatments or improving the efficiency of screening for specific conditions. Further, we outline future directions for interpretable machine learning and highlight the importance of developing algorithmic solutions that can enable machine-learning driven decision making in high-stakes healthcare problems."
"Current challenges and future opportunities for XAI in machine learning-based clinical decision support systems: a systematic review","https://scispace.com/paper/current-challenges-and-future-opportunities-for-xai-in-38bcik2g8e","2021","Journal Article","Applied Sciences","Anna Markella Antoniadi
Yuhan Du
Yasmine Guendouz
Lan Wei
Claudia Mazo
Brett A. Becker
Catherine Mooney","10.3390/APP11115088","https://www.mdpi.com/2076-3417/11/11/5088/pdf","… of the use of XAI such as … XAI in the context of CDSS and, in particular, a lack of user studies exploring the needs of clinicians. We propose some guidelines for the implementation of XAI …","Machine Learning and Artificial Intelligence (AI) more broadly have great immediate and future potential for transforming almost all aspects of medicine. However, in many applications, even outside medicine, a lack of transparency in AI applications has become increasingly problematic. This is particularly pronounced where users need to interpret the output of AI systems. Explainable AI (XAI) provides a rationale that allows users to understand why a system has produced a given output. The output can then be interpreted within a given context. One area that is in great need of XAI is that of Clinical Decision Support Systems (CDSSs). These systems support medical practitioners in their clinic decision-making and in the absence of explainability may lead to issues of under or over-reliance. Providing explanations for how recommendations are arrived at will allow practitioners to make more nuanced, and in some cases, life-saving decisions. The need for XAI in CDSS, and the medical field in general, is amplified by the need for ethical and fair decision-making and the fact that AI trained with historical data can be a reinforcement agent of historical actions and biases that should be uncovered. We performed a systematic literature review of work to-date in the application of XAI in CDSS. Tabular data processing XAI-enabled systems are the most common, while XAI-enabled CDSS for text analysis are the least common in literature. There is more interest in developers for the provision of local explanations, while there was almost a balance between post-hoc and ante-hoc explanations, as well as between model-specific and model-agnostic techniques. Studies reported benefits of the use of XAI such as the fact that it could enhance decision confidence for clinicians, or generate the hypothesis about causality, which ultimately leads to increased trustworthiness and acceptability of the system and potential for its incorporation in the clinical workflow. However, we found an overall distinct lack of application of XAI in the context of CDSS and, in particular, a lack of user studies exploring the needs of clinicians. We propose some guidelines for the implementation of XAI in CDSS and explore some opportunities, challenges, and future research needs."
"An explainable machine learning-based clinical decision support system for prediction of gestational diabetes mellitus","https://scispace.com/paper/an-explainable-machine-learning-based-clinical-decision-2v9cup5c","2022","Journal Article","Dental science reports","Yuhan Du
Anthony Rafferty
Fionnuala M. McAuliffe
Lan Wei
Catherine Mooney","10.1038/s41598-022-05112-2","https://www.nature.com/articles/s41598-022-05112-2.pdf","… We have developed an explainable machine learning-based clinical decision support … technique and feature selection, five machine learning algorithms were applied with five-fold cross…","Gestational Diabetes Mellitus (GDM), a common pregnancy complication associated with many maternal and neonatal consequences, is increased in mothers with overweight and obesity. Interventions initiated early in pregnancy can reduce the rate of GDM in these women, however, untargeted interventions can be costly and time-consuming. We have developed an explainable machine learning-based clinical decision support system (CDSS) to identify at-risk women in need of targeted pregnancy intervention. Maternal characteristics and blood biomarkers at baseline from the PEARS study were used. After appropriate data preparation, synthetic minority oversampling technique and feature selection, five machine learning algorithms were applied with five-fold cross-validated grid search optimising the balanced accuracy. Our models were explained with Shapley additive explanations to increase the trustworthiness and acceptability of the system. We developed multiple models for different use cases: theoretical (AUC-PR 0.485, AUC-ROC 0.792), GDM screening during a normal antenatal visit (AUC-PR 0.208, AUC-ROC 0.659), and remote GDM risk assessment (AUC-PR 0.199, AUC-ROC 0.656). Our models have been implemented as a web server that is publicly available for academic use. Our explainable CDSS demonstrates the potential to assist clinicians in screening at risk patients who may benefit from early pregnancy GDM prevention strategies. "
"Interpretable machine learning in healthcare","https://scispace.com/paper/interpretable-machine-learning-in-healthcare-3wc5807whv","2018","Proceedings Article","IEEE International Conference on Healthcare Informatics","Muhammad Aurangzeb Ahmad
Ankur Teredesai
Carly Eckert","10.1109/ICHI.2018.00095","https://dl.acm.org/doi/pdf/10.1145/3233547.3233667","… models and systems in healthcare. We discuss many uses in which interpretable machine learning models are needed in healthcare and how they should be deployed. Additionally, we …","This tutorial extensively covers the definitions, nuances, challenges, and requirements for the design of interpretable and explainable machine learning models and systems in healthcare. We discuss many uses in which interpretable machine learning models are needed in healthcare and how they should be deployed. Additionally, we explore the landscape of recent advances to address the challenges model interpretability in healthcare and also describe how one would go about choosing the right interpretable machine learnig algorithm for a given problem in healthcare."
"Interpretability of clinical decision support systems based on artificial intelligence from technological and medical perspective: A systematic review","https://scispace.com/paper/interpretability-of-clinical-decision-support-systems-based-2roy7wkw","2023","Journal Article","Journal of Healthcare Engineering","Qian Xu
Wenzhao Xie
Bolin Liao
Chao Hu
Lu Qin
Zhengzijin Yang
Huan Xiong
Yi Lyu
Yue Zhou
Aijing Luo","10.1155/2023/9919269","https://onlinelibrary.wiley.com/doi/pdf/10.1155/2023/9919269","… [39] reviewed the application of explainable AI in machine learning-based CDSS and summarized the findings of data type, preference of developers, type of explanations, and benefits …","Background Artificial intelligence (AI) has developed rapidly, and its application extends to clinical decision support system (CDSS) for improving healthcare quality. However, the interpretability of AI-driven CDSS poses significant challenges to widespread application. Objective This study is a review of the knowledge-based and data-based CDSS literature regarding interpretability in health care. It highlights the relevance of interpretability for CDSS and the area for improvement from technological and medical perspectives. Methods A systematic search was conducted on the interpretability-related literature published from 2011 to 2020 and indexed in the five databases: Web of Science, PubMed, ScienceDirect, Cochrane, and Scopus. Journal articles that focus on the interpretability of CDSS were included for analysis. Experienced researchers also participated in manually reviewing the selected articles for inclusion/exclusion and categorization. Results Based on the inclusion and exclusion criteria, 20 articles from 16 journals were finally selected for this review. Interpretability, which means a transparent structure of the model, a clear relationship between input and output, and explainability of artificial intelligence algorithms, is essential for CDSS application in the healthcare setting. Methods for improving the interpretability of CDSS include ante-hoc methods such as fuzzy logic, decision rules, logistic regression, decision trees for knowledge-based AI, and white box models, post hoc methods such as feature importance, sensitivity analysis, visualization, and activation maximization for black box models. A number of factors, such as data type, biomarkers, human-AI interaction, needs of clinicians, and patients, can affect the interpretability of CDSS. Conclusions The review explores the meaning of the interpretability of CDSS and summarizes the current methods for improving interpretability from technological and medical perspectives. The results contribute to the understanding of the interpretability of CDSS based on AI in health care. Future studies should focus on establishing formalism for defining interpretability, identifying the properties of interpretability, and developing an appropriate and objective metric for interpretability; in addition, the user's demand for interpretability and how to express and provide explanations are also the directions for future research."
"A survey on the interpretability of deep learning in medical diagnosis","https://scispace.com/paper/a-survey-on-the-interpretability-of-deep-learning-in-medical-3i349qpy","2022","Journal Article","Multimedia Systems","Qiaoying Teng
Zhe Liu
Yuqing Song
Kyungdo Han
Yang Lu","10.1007/s00530-022-00960-4","https://pmc.ncbi.nlm.nih.gov/articles/PMC9243744/pdf/530_2022_Article_960.pdf","… In this section, we classify some interpretability methods commonly used in the medical field into three categories and provide a detailed description of each category. It is worth noting …","Deep learning has demonstrated remarkable performance in the medical domain, with accuracy that rivals or even exceeds that of human experts. However, it has a significant problem that these models are “black-box” structures, which means they are opaque, non-intuitive, and difficult for people to understand. This creates a barrier to the application of deep learning models in clinical practice due to lack of interpretability, trust, and transparency. To overcome this problem, several studies on interpretability have been proposed. Therefore, in this paper, we comprehensively review the interpretability of deep learning in medical diagnosis based on the current literature, including some common interpretability methods used in the medical domain, various applications with interpretability for disease diagnosis, prevalent evaluation metrics, and several disease datasets. In addition, the challenges of interpretability and future research directions are also discussed here. To the best of our knowledge, this is the first time that various applications of interpretability methods for disease diagnosis have been summarized. "
"Survey of explainable AI techniques in healthcare","https://scispace.com/paper/survey-of-explainable-ai-techniques-in-healthcare-3qntgd4a","2023","Journal Article","Sensors","Ahmad Chaddad
Jihao Peng
Jian Xu
Ahmed Bouridane","10.3390/s23020634","https://scispace.com/pdf/survey-of-explainable-ai-techniques-in-healthcare-3qntgd4a.pdf","… interpretability in medical imaging topics. In addition, we focus on the challenging XAI problems in medical … interpretations of deep learning models using XAI concepts in medical image …","Artificial intelligence (AI) with deep learning models has been widely applied in numerous domains, including medical imaging and healthcare tasks. In the medical field, any judgment or decision is fraught with risk. A doctor will carefully judge whether a patient is sick before forming a reasonable explanation based on the patient’s symptoms and/or an examination. Therefore, to be a viable and accepted tool, AI needs to mimic human judgment and interpretation skills. Specifically, explainable AI (XAI) aims to explain the information behind the black-box model of deep learning that reveals how the decisions are made. This paper provides a survey of the most recent XAI techniques used in healthcare and related medical imaging applications. We summarize and categorize the XAI types, and highlight the algorithms used to increase interpretability in medical imaging topics. In addition, we focus on the challenging XAI problems in medical applications and provide guidelines to develop better interpretations of deep learning models using XAI concepts in medical image and text analysis. Furthermore, this survey provides future directions to guide developers and researchers for future prospective investigations on clinical topics, particularly on applications with medical imaging."
"The importance of interpretability and visualization in machine learning for applications in medicine and health care","https://scispace.com/paper/the-importance-of-interpretability-and-visualization-in-121ag95dhp","2020","Journal Article","Neural Computing and Applications","Alfredo Vellido","10.1007/S00521-019-04051-W","https://www.cs.upc.edu/~avellido/research/NEUCOMP19_vellido.pdf","… the interpretability and explainability of machine learning … of the ways in which interpretability and explainability in this … as a goal in itself, we need to integrate the medical experts in …","In a short period of time, many areas of science have made a sharp transition towards data-dependent methods. In some cases, this process has been enabled by simultaneous advances in data acquisition and the development of networked system technologies. This new situation is particularly clear in the life sciences, where data overabundance has sparked a flurry of new methodologies for data management and analysis. This can be seen as a perfect scenario for the use of machine learning and computational intelligence techniques to address problems in which more traditional data analysis approaches might struggle. But, this scenario also poses some serious challenges. One of them is model interpretability and explainability, especially for complex nonlinear models. In some areas such as medicine and health care, not addressing such challenge might seriously limit the chances of adoption, in real practice, of computer-based systems that rely on machine learning and computational intelligence methods for data analysis. In this paper, we reflect on recent investigations about the interpretability and explainability of machine learning methods and discuss their impact on medicine and health care. We pay specific attention to one of the ways in which interpretability and explainability in this context can be addressed, which is through data and model visualization. We argue that, beyond improving model interpretability as a goal in itself, we need to integrate the medical experts in the design of data analysis interpretation strategies. Otherwise, machine learning is unlikely to become a part of routine clinical and health care practice."
"The importance of interpreting machine learning models for blood glucose prediction in diabetes: an analysis using SHAP","https://scispace.com/paper/the-importance-of-interpreting-machine-learning-models-for-4wmou2ls11","2023","Journal Article","Dental science reports","Francesco Prendin
Jacopo Pavan
Giacomo Cappon
Simone Del Favero
Giovanni Sparacino
Andrea Facchinetti","10.1038/s41598-023-44155-x","https://www.nature.com/articles/s41598-023-44155-x.pdf","… - and deep-learning approaches concerns the interpretability of the outcome. Interpretability … Whilst machine/deep-learning models can grant accurate performance, their results can be …","Abstract Machine learning has become a popular tool for learning models of complex dynamics from biomedical data. In Type 1 Diabetes (T1D) management, these models are increasingly been integrated in decision support systems (DSS) to forecast glucose levels and provide preventive therapeutic suggestions, like corrective insulin boluses (CIB), accordingly. Typically, models are chosen based on their prediction accuracy. However, since patient safety is a concern in this application, the algorithm should also be physiologically sound and its outcome should be explainable. This paper aims to discuss the importance of using tools to interpret the output of black-box models in T1D management by presenting a case-of-study on the selection of the best prediction algorithm to integrate in a DSS for CIB suggestion. By retrospectively “replaying” real patient data, we show that two long-short term memory neural networks (LSTM) (named p-LSTM and np-LSTM) with similar prediction accuracy could lead to different therapeutic decisions. An analysis with SHAP—a tool for explaining black-box models’ output—unambiguously shows that only p-LSTM learnt the physiological relationship between inputs and glucose prediction, and should therefore be preferred. This is verified by showing that, when embedded in the DSS, only p-LSTM can improve patients’ glycemic control. "
"What is interpretable? using machine learning to design interpretable decision-support systems","https://scispace.com/paper/what-is-interpretable-using-machine-learning-to-design-2l0dk4ceak","2018","Posted Content","arXiv: Learning","Owen Lahav
Nicholas Mastronarde
Mihaela van der Schaar","","https://arxiv.org/pdf/1811.10799","… designing human-interpretable systems using RL to learn what is interpretable to users. To demonstrate our approach, we designed a ML-driven DSS providing medical risk assessment…","Recent efforts in Machine Learning (ML) interpretability have focused on creating methods for explaining black-box ML models. However, these methods rely on the assumption that simple approximations, such as linear models or decision-trees, are inherently human-interpretable, which has not been empirically tested. Additionally, past efforts have focused exclusively on comprehension, neglecting to explore the trust component necessary to convince non-technical experts, such as clinicians, to utilize ML models in practice. In this paper, we posit that reinforcement learning (RL) can be used to learn what is interpretable to different users and, consequently, build their trust in ML models. To validate this idea, we first train a neural network to provide risk assessments for heart failure patients. We then design a RL-based clinical decision-support system (DSS) around the neural network model, which can learn from its interactions with users. We conduct an experiment involving a diverse set of clinicians from multiple institutions in three different countries. Our results demonstrate that ML experts cannot accurately predict which system outputs will maximize clinicians' confidence in the underlying neural network model, and suggest additional findings that have broad implications to the future of research into ML interpretability and the use of ML in medicine."
""" The human body is a black box"" supporting clinical decision-making with deep learning","https://scispace.com/paper/the-human-body-is-a-black-box-supporting-clinical-decision-3xvydlueql","2019","Posted Content","arXiv: Computers and Society","Mark Sendak
Madeleine Clare Elish
Michael Gao
Joseph Futoma
William Ratliff
Marshall Nichols
Armando Bedoya
Suresh Balu
Cara O'Brien","","https://dl.acm.org/doi/pdf/10.1145/3351095.3372827?download&","… work on machine learning healthcare applications and the … system, and is one of the first deep learning models to be fully … , and transparency when a machine learning implementation is …","Machine learning technologies are increasingly developed for use in healthcare. While research communities have focused on creating state-of-the-art models, there has been less focus on real world implementation and the associated challenges to accuracy, fairness, accountability, and transparency that come from actual, situated use. Serious questions remain under examined regarding how to ethically build models, interpret and explain model output, recognize and account for biases, and minimize disruptions to professional expertise and work cultures. We address this gap in the literature and provide a detailed case study covering the development, implementation, and evaluation of Sepsis Watch, a machine learning-driven tool that assists hospital clinicians in the early diagnosis and treatment of sepsis. We, the team that developed and evaluated the tool, discuss our conceptualization of the tool not as a model deployed in the world but instead as a socio-technical system requiring integration into existing social and professional contexts. Rather than focusing on model interpretability to ensure a fair and accountable machine learning, we point toward four key values and practices that should be considered when developing machine learning to support clinical decision-making: rigorously define the problem in context, build relationships with stakeholders, respect professional discretion, and create ongoing feedback loops with stakeholders. Our work has significant implications for future research regarding mechanisms of institutional accountability and considerations for designing machine learning systems. Our work underscores the limits of model interpretability as a solution to ensure transparency, accuracy, and accountability in practice. Instead, our work demonstrates other means and goals to achieve FATML values in design and in practice."
"Evaluating local interpretable model-agnostic explanations on clinical machine learning classification models","https://scispace.com/paper/evaluating-local-interpretable-model-agnostic-explanations-2gy9tfdnng","2020","Proceedings Article","Computer-Based Medical Systems","Nesaretnam Barr Kumarakulasinghe
Tobias Blomberg
Jintai Liu
Alexandra Saraiva Leao
Panagiotis Papapetrou","10.1109/CBMS49503.2020.00009","https://www.researchgate.net/profile/Alexandra-Saraiva-Leao/publication/344063540_Evaluating_Local_Interpretable_Model-Agnostic_Explanations_on_Clinical_Machine_Learning_Classification_Models/links/6196015607be5f31b7936b4e/Evaluating-Local-Interpretable-Model-Agnostic-Explanations-on-Clinical-Machine-Learning-Classification-Models.pdf","… in an evaluation of the interpretability of a machine learning model Specifically, in this study, … the predictions of black-box machine learning models which has not been previously …","The usage of black-box classification models within the healthcare field is highly dependent on being interpretable by the receiver. Local Interpretable Model-Agnostic Explanation (LIME) provides a patient-specific explanation for a given classification, thus enhancing the possibility for any complex classifier to serve as a safety aid within a clinical setting. However, research on if the explanation provided by LIME is relevant to clinicians is limited and there is no current framework for how an evaluation of LIME is to be performed. To evaluate the clinical relevance of the explanations provided by LIME, this study has investigated how physician's independent explanations for classified observations compare with explanations provided by LIME. Additionally, the clinical relevance and the experienced reliance on the explanations provided by LIME have been evaluated by field experts. The results indicate that the explanation provided by LIME is clinically relevant and has a very high concordance with the explanations provided by physicians. Furthermore, trust and reliance on LIME are fairly high amongst clinicians. The study proposes a framework for further research within the area."
"Explainable artificial intelligence for human decision support system in the medical domain","https://scispace.com/paper/explainable-artificial-intelligence-for-human-decision-4vjfartezd","2021","Journal Article","","Samanta Knapic
Avleen Malhi
Rohit Saluja
Kary Främling","10.3390/MAKE3030037","","… We implemented two post hoc interpretable machine learning methods, called Local … trust these decisions and how machine learning or deep learning models make their decisions. …","In this paper, we present the potential of Explainable Artificial Intelligence methods for decision support in medical image analysis scenarios. Using three types of explainable methods applied to the same medical image data set, we aimed to improve the comprehensibility of the decisions provided by the Convolutional Neural Network (CNN). In vivo gastral images obtained by a video capsule endoscopy (VCE) were the subject of visual explanations, with the goal of increasing health professionals’ trust in black-box predictions. We implemented two post hoc interpretable machine learning methods, called Local Interpretable Model-Agnostic Explanations (LIME) and SHapley Additive exPlanations (SHAP), and an alternative explanation approach, the Contextual Importance and Utility (CIU) method. The produced explanations were assessed by human evaluation. We conducted three user studies based on explanations provided by LIME, SHAP and CIU. Users from different non-medical backgrounds carried out a series of tests in a web-based survey setting and stated their experience and understanding of the given explanations. Three user groups (n = 20, 20, 20) with three distinct forms of explanations were quantitatively analyzed. We found that, as hypothesized, the CIU-explainable method performed better than both LIME and SHAP methods in terms of improving support for human decision-making and being more transparent and thus understandable to users. Additionally, CIU outperformed LIME and SHAP by generating explanations more rapidly. Our findings suggest that there are notable differences in human decision-making between various explanation support settings. In line with that, we present three potential explainable methods that, with future improvements in implementation, can be generalized to different medical data sets and can provide effective decision support to medical experts."
"DeepXplainer: An interpretable deep learning based approach for lung cancer detection using explainable artificial intelligence","https://scispace.com/paper/deepxplainer-an-interpretable-deep-learning-based-approach-rtznyaropc","2023","Journal Article","Computer Methods and Programs in Biomedicine","Niyaz Ahmad Wani
Ravinder Kumar
Jatin Bedi","10.1016/j.cmpb.2023.107879","https://shodhratna.thapar.edu:8443/jspui/bitstream/tiet/171/1/1-s2.0-S016926072300545X-main.pdf","… , particularly in healthcare. As a result, there is a need for interpretable predictors that … This study introduces “DeepXplainer”, a new interpretable hybrid deep learning-based technique …","Artificial intelligence (AI) has several uses in the healthcare industry, some of which include healthcare management, medical forecasting, practical making of decisions, and diagnosis. AI technologies have reached human-like performance, but their use is limited since they are still largely viewed as opaque black boxes. This distrust remains the primary factor for their limited real application, particularly in healthcare. As a result, there is a need for interpretable predictors that provide better predictions and also explain their predictions.This study introduces ""DeepXplainer"", a new interpretable hybrid deep learning-based technique for detecting lung cancer and providing explanations of the predictions. This technique is based on a convolutional neural network and XGBoost. XGBoost is used for class label prediction after ""DeepXplainer"" has automatically learned the features of the input using its many convolutional layers. For providing explanations or explainability of the predictions, an explainable artificial intelligence method known as ""SHAP"" is implemented.The open-source ""Survey Lung Cancer"" dataset was processed using this method. On multiple parameters, including accuracy, sensitivity, F1-score, etc., the proposed method outperformed the existing methods. The proposed method obtained an accuracy of 97.43%, a sensitivity of 98.71%, and an F1-score of 98.08. After the model has made predictions with this high degree of accuracy, each prediction is explained by implementing an explainable artificial intelligence method at both the local and global levels.A deep learning-based classification model for lung cancer is proposed with three primary components: one for feature learning, another for classification, and a third for providing explanations for the predictions made by the proposed hybrid (ConvXGB) model. The proposed ""DeepXplainer"" has been evaluated using a variety of metrics, and the results demonstrate that it outperforms the current benchmarks. Providing explanations for the predictions, the proposed approach may help doctors in detecting and treating lung cancer patients more effectively. "
"Explainable ai: A review of machine learning interpretability methods","https://scispace.com/paper/explainable-ai-a-review-of-machine-learning-interpretability-2cru5fpv8m","2020","Journal Article","Entropy","Pantelis Linardatos
Vasilis Papastefanopoulos
Sotiris Kotsiantis","10.3390/E23010018","","… In [65], Microsoft presented two case studies on real medical data, where naturally interpretable generalized additive models with pairwise interactions (GA 2 Ms), as originally proposed …","Recent advances in artificial intelligence (AI) have led to its widespread industrial adoption, with machine learning systems demonstrating superhuman performance in a significant number of tasks. However, this surge in performance, has often been achieved through increased model complexity, turning such systems into “black box” approaches and causing uncertainty regarding the way they operate and, ultimately, the way that they come to decisions. This ambiguity has made it problematic for machine learning systems to be adopted in sensitive yet critical domains, where their value could be immense, such as healthcare. As a result, scientific interest in the field of Explainable Artificial Intelligence (XAI), a field that is concerned with the development of new methods that explain and interpret machine learning models, has been tremendously reignited over recent years. This study focuses on machine learning interpretability methods; more specifically, a literature review and taxonomy of these methods are presented, as well as links to their programming implementations, in the hope that this survey would serve as a reference point for both theorists and practitioners."
"Designing ECG monitoring healthcare system with federated transfer learning and explainable AI","https://scispace.com/paper/designing-ecg-monitoring-healthcare-system-with-federated-292oj39tui","2021","Posted Content","arXiv: Learning","Ali Raza
Ali Raza
Kim Phuc Tran
Ludovic Koehl
Shujun Li","","https://www.sciencedirect.com/science/article/am/pii/S0950705121009862","… in clinical healthcare. This limits the application of deep learning in real-world health systems. … Additionally, we propose an XAI-based module on top of the proposed classifier for …","Deep learning play a vital role in classifying different arrhythmias using the electrocardiography (ECG) data. Nevertheless, training deep learning models normally requires a large amount of data and it can lead to privacy concerns. Unfortunately, a large amount of healthcare data cannot be easily collected from a single silo. Additionally, deep learning models are like black-box, with no explainability of the predicted results, which is often required in clinical healthcare. This limits the application of deep learning in real-world health systems. In this paper, we design a new explainable artificial intelligence (XAI) based deep learning framework in a federated setting for ECG-based healthcare applications. The federated setting is used to solve issues such as data availability and privacy concerns. Furthermore, the proposed framework setting effectively classifies arrhythmia's using an autoencoder and a classifier, both based on a convolutional neural network (CNN). Additionally, we propose an XAI-based module on top of the proposed classifier to explain the classification results, which help clinical practitioners make quick and reliable decisions. The proposed framework was trained and tested using the MIT-BIH Arrhythmia database. The classifier achieved accuracy up to 94% and 98% for arrhythmia detection using noisy and clean data, respectively, with five-fold cross-validation."
"Interpretability and fairness evaluation of deep learning models on MIMIC-IV dataset","https://scispace.com/paper/interpretability-and-fairness-evaluation-of-deep-learning-12hfcr0y","2022","Journal Article","Dental science reports","Bendera Kematian","10.1038/s41598-022-11012-2","https://www.nature.com/articles/s41598-022-11012-2.pdf","… interpretability of deep learning mortality prediction models and observe that (1) the best-performing interpretability … Our findings suggest that future research in AI models for healthcare …","The recent release of large-scale healthcare datasets has greatly propelled the research of data-driven deep learning models for healthcare applications. However, due to the nature of such deep black-boxed models, concerns about interpretability, fairness, and biases in healthcare scenarios where human lives are at stake call for a careful and thorough examination of both datasets and models. In this work, we focus on MIMIC-IV (Medical Information Mart for Intensive Care, version IV), the largest publicly available healthcare dataset, and conduct comprehensive analyses of interpretability as well as dataset representation bias and prediction fairness of deep learning models for in-hospital mortality prediction. First, we analyze the interpretability of deep learning mortality prediction models and observe that (1) the best-performing interpretability method successfully identifies critical features for mortality prediction on various prediction models as well as recognizing new important features that domain knowledge does not consider; (2) prediction models rely on demographic features, raising concerns in fairness. Therefore, we then evaluate the fairness of models and do observe the unfairness: (1) there exists disparate treatment in prescribing mechanical ventilation among patient groups across ethnicity, gender and age; (2) models often rely on racial attributes unequally across subgroups to generate their predictions. We further draw concrete connections between interpretability methods and fairness metrics by showing how feature importance from interpretability methods can be beneficial in quantifying potential disparities in mortality predictors. Our analysis demonstrates that the prediction performance is not the only factor to consider when evaluating models for healthcare applications, since high prediction performance might be the result of unfair utilization of demographic features. Our findings suggest that future research in AI models for healthcare applications can benefit from utilizing the analysis workflow of interpretability and fairness as well as verifying if models achieve superior performance at the cost of introducing bias. "
"On the interpretability of machine learning-based model for predicting hypertension","https://scispace.com/paper/on-the-interpretability-of-machine-learning-based-model-for-140cq939ul","2019","Journal Article","BMC Medical Informatics and Decision Making","Radwa Elshawi
Mouaz H. Al-Mallah
Sherif Sakr","10.1186/S12911-019-0874-0","https://link.springer.com/content/pdf/10.1186/s12911-019-0874-0.pdf","… users to interpret the outcomes of the complex machine learning models becomes problematic [1]. Machine learning interpretability is defined as the degree to which a machine learning …","Although complex machine learning models are commonly outperforming the traditional simple interpretable models, clinicians find it hard to understand and trust these complex models due to the lack of intuition and explanation of their predictions. The aim of this study to demonstrate the utility of various model-agnostic explanation techniques of machine learning models with a case study for analyzing the outcomes of the machine learning random forest model for predicting the individuals at risk of developing hypertension based on cardiorespiratory fitness data. The dataset used in this study contains information of 23,095 patients who underwent clinician-referred exercise treadmill stress testing at Henry Ford Health Systems between 1991 and 2009 and had a complete 10-year follow-up. Five global interpretability techniques (Feature Importance, Partial Dependence Plot, Individual Conditional Expectation, Feature Interaction, Global Surrogate Models) and two local interpretability techniques (Local Surrogate Models, Shapley Value) have been applied to present the role of the interpretability techniques on assisting the clinical staff to get better understanding and more trust of the outcomes of the machine learning-based predictions. Several experiments have been conducted and reported. The results show that different interpretability techniques can shed light on different insights on the model behavior where global interpretations can enable clinicians to understand the entire conditional distribution modeled by the trained response function. In contrast, local interpretations promote the understanding of small parts of the conditional distribution for specific instances. Various interpretability techniques can vary in their explanations for the behavior of the machine learning model. The global interpretability techniques have the advantage that it can generalize over the entire population while local interpretability techniques focus on giving explanations at the level of instances. Both methods can be equally valid depending on the application need. Both methods are effective methods for assisting clinicians on the medical decision process, however, the clinicians will always remain to hold the final say on accepting or rejecting the outcome of the machine learning models and their explanations based on their domain expertise."