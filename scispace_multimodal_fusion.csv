"Paper Title","Paper Link","Publication Year","Publication Type","Publication Title","Author Names","DOI","PDF Link","Abstract","TL;DR"
"A multimodal neural network that distinguishes between type 1 and type 2 diabetes in young persons using MRI and clinical data","https://scispace.com/paper/a-multimodal-neural-network-that-distinguishes-between-type-3fz5gub7","2023","Journal Article","Physiology","Mehrshad Sadria
Petter Bjornstad
P. Pottumarthi
Laura Pyle
Tim Vigers
Anita T. Layton","10.1152/physiol.2023.38.s1.5695980","","Early diabetic kidney disease (DKD) is common in young persons with type 1 (T1D) and type 2 diabetes (T2D) and accentuates their lifetime risk of kidney failure, requiring dialysis or a kidney transplant. Although clinical manifestations of DKD are similar in T1D and T2D, the structural lesions may differ, and it remains unclear whether DKD in T1D and T2D represent distinct diseases. Accordingly, the objective of this study is to build machine learning (ML) models using clinical and kidney MRI data to classify diabetes status, as well as structural and functional kidney differences of individuals with T1D versus T2D. We hypothesize that a highly accurate multimodal neural network can be constructed that integrates clinical and functional kidney MRI images.Data were obtained from several studies at University of Colorado in youth with T1D (n=102), T2D (n=91) as well as non-diabetic controls (n=60). A total of 253 participants were included in the analyses. First, we applied to clinical data (CASPER, IMPROVE-T2D, CROCODILE, and RENAL-HEIR trials) logistic regression and 7 ML models: extreme gradient boosting machine (XGBoost), XGBoost with grid search, k-nearest neighbors (KNN), support vector machine (SVM), decision tree, random forest, and a 3-layer neural network (NN-EHR). When a small subset of the clinical data was used as features, the NN-EHR yielded the highest accuracy of 84%. Next, we considered the MRI images (27,000 images from the 253 individuals). We applied three convoluted NN to perform the classification: AlexNet, VGG16, and a 4-layer Neural Network for Diabetes Detection (NN4DD). Considering the images alone, VGG16 and NN4DD both achieved an accuracy of > 80%. Additionally, by integrating the clinical data and the MRI images, the fusion neural network achieved an accuracy of almost 100%. Finally, an interpretability analysis of NN4DD indicated notable differences in kidney structure and function among the three groups.Although the cost of MRI is prohibitive and thus impractical for diabetes diagnosis, these results provide a proof-of-concept that fused neural networks that integrate multimodal data can be a valuable diagnostic tool, and provide structural and functional insight on kidney differences between T1D and T2D. This research is sponsored in part by the Natural Sciences and Engineering Council (Canada) and the National Institutes of Health (USA). This is the full abstract presented at the American Physiology Summit 2023 meeting and is only available in HTML format. There are no additional versions or additional content available for this abstract. Physiology was not involved in the peer review process.","In this paper , a multimodal neural network was constructed that integrates clinical and functional kidney MRI images to detect early diabetic kidney disease (DKDKA) in young persons with Type 1 (T1D) and Type 2 diabetes (T2D)."
"Wavelet Based Machine Learning Approaches Towards Precision Medicine in Diabetes Mellitus","https://scispace.com/paper/wavelet-based-machine-learning-approaches-towards-precision-1u4z3twz","2022","Journal Article","The FASEB Journal","","10.1096/fasebj.2022.36.s1.r6003","","It is estimated that 422 million people around the world have diabetes mellitus (DM)-a devastating, complex, and highly heterogeneous disease-requesting better interventions based on disease subtyping. In this research, we utilize the discrete wavelet transform (DWT) to decompose and denoise DM data. Using DWT, we enhance heart rate variability (HRV) based DM diagnosis, data visualization of the disparities in Human Microbiome Project (HMP) data (gut bacteria, metabolomics, proteomics, RNA sequencing, targeted proteomics, and transcriptomics data) using demographic features, and insulin resistance prediction. We also attempt to forecast continuous glucose monitoring (CGM) ahead by 90 minutes because CGM is unable to provide real-time blood glucose measurements. We achieve 91.9% diagnosis accuracy for Type 1 DM using Random Forest on data transformed with DWT, holding the potential for usage in clinics. In addition, our DWT-based t-SNE and UMAP explorative analysis of HMP data support subtypes of prediabetic patients stratified by sex, race, and age. Moreover, DWT-based transformations provide multi-view clustering that any other methods would not provide on metabolomics, proteomics, RNA sequencing, targeted proteomics, and transcriptomics data and outperform those without DWT. Taken together, DWT-based machine learning approaches enable a fine resolution of subtyping DM towards precision medicine. ","In this paper , the discrete wavelet transform (DWT) is used to decompose and denoise DM data. And DWT-based transformations provide multi-view clustering that any other methods would not provide on metabolomics, proteomics, RNA sequencing, targeted proteomics and transcriptomics data and outperform those without DWT."
"Intelligent medical diagnosis and treatment for diabetes with deep convolutional fuzzy neural networks","https://scispace.com/paper/intelligent-medical-diagnosis-and-treatment-for-diabetes-1a3i3i5qld","2024","Journal Article","Information Sciences","Wenhui Zhou
Xiaomin Liu
Hongtao Bai
Lina He","10.1016/j.ins.2024.120802","","The advent of smart healthcare has significantly heightened the importance of computer technologies in supporting medical diagnosis and treatment. Nevertheless, the challenges of mining latent knowledge within diagnostic data and explaining results to healthcare professionals have limited the application of many algorithms in clinical practice. To address these issues, our study introduces an Interpretable Predictor with Deep Convolutional Fuzzy Neural Network (IP-DCFNN). The proposed model is capable of assessing disease risk based on individual data and providing interpretable justifications to aid in medical diagnosis and treatment decisions. By deconstructing the fuzzy inference process and incorporating convolutional neural network, our approach enhances the ability to discover underlying information while maintaining transparency and interpretability. Furthermore, we introduce a grid partition-based method for initializing the antecedent parameters and a hybrid approach that combines gradient descent with least squares estimation for training. Compared with Adaptive Neuro-Fuzzy Inference System (ANFIS) and Deep Neural Networks (DNN) Our model has an average improvement of 7.4% on prediction accuracy. More importantly, it can extract interpretable insights from membership functions, rule bases, and fuzzy contributions, offering valuable knowledge for medical research on type 2 diabetes, supporting intelligent diagnostic processes and providing personalized healthcare recommendations. The model can also be applied on the diagnosis and treatment of various other diseases. ","An interpretable deep convolutional fuzzy neural network model for diabetes diagnosis and treatment, improving accuracy and providing insights into disease mechanisms."
"MFISN: Modality Fuzzy Information Separation Network for Disease Classification","https://scispace.com/paper/mfisn-modality-fuzzy-information-separation-network-for-13jonu9w3y","","Journal Article","IEEE Transactions on Fuzzy Systems","Fengtao Nan
Bin Pu
Jiayi Pan
Yingchun Fan
Jiewen Yang
Xingbo Dong
Zhaozhao Xu
Shuihua Wang","10.1109/tfuzz.2024.3371678","","Most of the previous machine learning-based models for multimodal medical diagnosis, primarily designed for unimodal images, usually do not fully leverage the potential of multimodal medical images, leading to limited classification accuracy. These conventional methods typically focus only on the intermodality common information, neglecting the intramodality specific information and assuming that the common information is more effective in disease diagnosis. Moreover, they do not adequately address the impact of fuzzy information between different medical imaging modalities on diagnostic results. To this end, we propose a modality fuzzy information separation network for disease classification, which extracts both common and specific information from fuzzy information to construct a comprehensive representation of multimodal medical images. Specifically, we extract modality invariant features as common information by explicitly modeling and maximizing loss constraints on mutual information. For specific information extraction, a constraint on feature space independence between specific and common information is imposed on each modality. Above-mentioned two steps, we concatenate common information and specific information to construct a comprehensive multimodal representation for separating fuzzy information. Finally, we purposely design a decoder network to reconstruct medical images from unimodal specific information and common information to demonstrate the effectiveness of the modality fuzzy information separation network. We conducted a validation of the proposed method's performance in classifying cardiomegaly, pneumothorax, edema, and skin disease. The experimental results substantiate the effectiveness of our proposed approach.","This study proposes MFISN, a modality fuzzy information separation network for disease classification, which extracts common and specific information from multimodal medical images to improve classification accuracy and address fuzzy information between imaging modalities."
"Multimodal Machine Learning in Image-Based and Clinical Biomedicine: Survey and Prospects","https://scispace.com/paper/multimodal-machine-learning-in-image-based-and-clinical-dij7ctr98y","2024","Journal Article","International Journal of Computer Vision","Elisa Warner
Joon Sang Lee
William Hsu
Tanveer Syeda-Mahmood
Charles Η. Kahn
Olivier Gevaert
Arvind Rao","10.1007/s11263-024-02032-8","https://scispace.compdf/multimodal-machine-learning-in-image-based-and-clinical-dij7ctr98y.pdf","Abstract Machine learning (ML) applications in medical artificial intelligence (AI) systems have shifted from traditional and statistical methods to increasing application of deep learning models. This survey navigates the current landscape of multimodal ML, focusing on its profound impact on medical image analysis and clinical decision support systems. Emphasizing challenges and innovations in addressing multimodal representation, fusion, translation, alignment, and co-learning, the paper explores the transformative potential of multimodal models for clinical predictions. It also highlights the need for principled assessments and practical implementation of such models, bringing attention to the dynamics between decision support systems and healthcare providers and personnel. Despite advancements, challenges such as data biases and the scarcity of “big data” in many biomedical domains persist. We conclude with a discussion on principled innovation and collaborative efforts to further the mission of seamless integration of multimodal ML models into biomedical practice. ","Multimodal ML revolutionizes image-based and clinical biomedicine by integrating various modalities of data. It enhances medical image analysis and clinical decision support systems. Challenges include data biases and scarcity of ""big data."" Despite advancements, there is a need for principled assessments and practical implementation. Collaborative efforts are essential for seamless integration into biomedical practice."
"Novel Detection and Progression Markers for Diabetes Based on Continuous Glucose Monitoring Data Dynamics","https://scispace.com/paper/novel-detection-and-progression-markers-for-diabetes-based-3ktyawar77","2024","Journal Article","The Journal of Clinical Endocrinology and Metabolism","Eslam Montaser
Leon S. Farhy
Boris Kovatchev","10.1210/clinem/dgae379","","Develop a diabetes diagnostic tool based on two markers of continuous glucose monitoring (CGM) dynamics: CGM entropy rate (ER) and Poincaré plot (PP) ellipse area (S). ","Novel diabetes detection and progression markers based on CGM data dynamics. Develop a diagnostic tool using CGM entropy rate and Poincaré plot ellipse area for diabetes diagnosis and progression prediction."
"Hypoglycaemia Prediction Models With Auto Explanation","https://scispace.com/paper/hypoglycaemia-prediction-models-with-auto-explanation-1r8djquy","2022","Journal Article","IEEE Access","","10.1109/access.2021.3117340","","World-wide statistics show a considerable growth of the occurrence of different types of Diabetes Mellitus, posing diverse challenges at many levels for public health policies. Some of these challenges may be addressed by means of computerised systems which may pave the way to provide practitioners with insight on their patient’s conditions anywhere and at anytime, but also to empower Diabetes patients as managers of their health. These systems for disease management come in many shapes and sizes, being the most promising trends the ones that involve expert systems that comprise specialised knowledge, use predictive models, feature engineering and reasoning. This study presents the state-of-the-art on reasoning and prediction models related with either blood glucose level or hypoglycaemia events. The main findings revealed are that there is room for improvement on predictive models, namely to enhance its accuracy and ability to forecast future events into a wider time frame. On the other hand, reasoning models are understudied and its usage in Diabetes management is reduced. We discuss an architecture that combines a predictive model and a reasoning system, with the objective of alerting of impending occurrences and interpret the current situation to accurately advise the diabetic user. ","In this article , the authors present the state-of-the-art on reasoning and prediction models related with either blood glucose level or hypoglycaemia events, and discuss an architecture that combines a predictive model and a reasoning system, with the objective of alerting of impending occurrences and interpret the current situation to accurately advise the diabetic user."
"A hybrid Transformer-LSTM model apply to glucose prediction","https://scispace.com/paper/a-hybrid-transformer-lstm-model-apply-to-glucose-prediction-7in214xar2sp","2024","Journal Article","PLOS ONE","QingXiang Bian
Azizan As’arry
Xiangguo Cong
Khairil Anas bin Md Rezali
Raja Mohd Kamil Raja Ahmad","10.1371/journal.pone.0310084","","The global prevalence of diabetes is escalating, with estimates indicating that over 536.6 million individuals were afflicted by 2021, accounting for approximately 10.5% of the world’s population. Effective management of diabetes, particularly monitoring and prediction of blood glucose levels, remains a significant challenge due to the severe health risks associated with inaccuracies, such as hypoglycemia and hyperglycemia. This study addresses this critical issue by employing a hybrid Transformer-LSTM (Long Short-Term Memory) model designed to enhance the accuracy of future glucose level predictions based on data from Continuous Glucose Monitoring (CGM) systems. This innovative approach aims to reduce the risk of diabetic complications and improve patient outcomes. We utilized a dataset which contain more than 32000 data points comprising CGM data from eight patients collected by Suzhou Municipal Hospital in Jiangsu Province, China. This dataset includes historical glucose readings and equipment calibration values, making it highly suitable for developing predictive models due to its richness and real-time applicability. Our findings demonstrate that the hybrid Transformer-LSTM model significantly outperforms the standard LSTM model, achieving Mean Square Error (MSE) values of 1.18, 1.70, and 2.00 at forecasting intervals of 15, 30, and 45 minutes, respectively. This research underscores the potential of advanced machine learning techniques in the proactive management of diabetes, a critical step toward mitigating its impact. ","This study develops a hybrid Transformer-LSTM model for glucose prediction using CGM data from 8 patients, achieving improved accuracy (MSE: 1.18-2.00) over standard LSTM models, with potential to enhance diabetes management and mitigate its impact."
"Multi-View Cross-Fusion Transformer Based on Kinetic Features for Non-Invasive Blood Glucose Measurement Using PPG Signal.","https://scispace.com/paper/multi-view-cross-fusion-transformer-based-on-kinetic-4mj6k8n0p3","2024","Journal Article","IEEE Journal of Biomedical and Health Informatics","Shisen Chen
Fen Qin
Xuesheng Ma
Jie Wei
Yuan-Ting Zhang
Yuan-Ting Zhang
Emil Jovanov","10.1109/jbhi.2024.3351867","","Noninvasive blood glucose (BG) measurement could significantly improve the prevention and management of diabetes. In this paper, we present a robust novel paradigm based on analyzing photoplethysmogram (PPG) signals. The method includes signal pre-processing optimization and a multi-view cross-fusion transformer (MvCFT) network for non-invasive BG assessment. Specifically, a multi-size weighted fitting (MSWF) time-domain filtering algorithm is proposed to optimally preserve the most authentic morphological features of the original signals. Meanwhile, the spatial position encoding-based kinetics features are reconstructed and embedded as prior knowledge to discern the implicit physiological patterns. In addition, a cross-view feature fusion (CVFF) module is designed to incorporate pairwise mutual information among different views to adequately capture the potential complementary features in physiological sequences. Finally, the subject- wise 5- fold cross-validation is performed on a clinical dataset of 260 subjects. The root mean square error (RMSE) and mean absolute error (MAE) of BG measurements are 1.129 mmol/L and 0.659 mmol/L, respectively, and the optimal Zone A in the Clark error grid, representing none clinical risk, is 87.89%. The results indicate that the proposed method has great potential for homecare applications.","A robust novel paradigm based on analyzing photoplethysmogram signals is presented, which includes a multi-size weighted fitting time-domain filtering algorithm and a multi-view cross-fusion transformer network for non-invasive BG assessment."
"AI-based diabetes care: risk prediction models and implementation concerns","https://scispace.com/paper/ai-based-diabetes-care-risk-prediction-models-and-53lx211ct4","2024","","npj digital medicine","Serena C Y Wang
Grace Nickel
Kaushik P. Venkatesh
Marium M. Raza
Joseph C. Kvedar","10.1038/s41746-024-01034-7","","The utilization of artificial intelligence (AI) in diabetes care has focused on early intervention and treatment management. Notably, usage has expanded to predict an individual’s risk for developing type 2 diabetes. A scoping review of 40 studies by Mohsen et al. shows that while most studies used unimodal AI models, multimodal approaches were superior because they integrate multiple types of data. However, creating multimodal models and determining model performance are challenging tasks given the multi-factored nature of diabetes. For both unimodal and multimodal models, there are also concerns of bias with the lack of external validations and representation of race, age, and gender in training data. The barriers in data quality and evaluation standardization are ripe areas for developing new technologies, especially for entrepreneurs and innovators. Collaboration amongst providers, entrepreneurs, and researchers must be prioritized to ensure that AI in diabetes care is providing quality and equitable patient care. ","Collaboration amongst providers, entrepreneurs, and researchers must be prioritized to ensure that AI in diabetes care is providing quality and equitable patient care, especially for entrepreneurs and innovators."
"F-DARTS: Foveated Differentiable Architecture Search Based Multimodal Medical Image Fusion","https://scispace.com/paper/f-darts-foveated-differentiable-architecture-search-based-1bku4b62","2023","Journal Article","IEEE Transactions on Medical Imaging","","10.1109/tmi.2023.3283517","","Multimodal medical image fusion (MMIF) is highly significant in such fields as disease diagnosis and treatment. The traditional MMIF methods are difficult to provide satisfactory fusion accuracy and robustness due to the influence of such possible human-crafted components as image transform and fusion strategies. Existing deep learning based fusion methods are generally difficult to ensure image fusion effect due to the adoption of a human-designed network structure and a relatively simple loss function and the ignorance of human visual characteristics during weight learning. To address these issues, we have presented the foveated differentiable architecture search (F-DARTS) based unsupervised MMIF method. In this method, the foveation operator is introduced into the weight learning process to fully explore human visual characteristics for the effective image fusion. Meanwhile, a distinctive unsupervised loss function is designed for network training by integrating mutual information, sum of the correlations of differences, structural similarity and edge preservation value. Based on the presented foveation operator and loss function, an end-to-end encoder-decoder network architecture will be searched using the F-DARTS to produce the fused image. Experimental results on three multimodal medical image datasets demonstrate that the F-DARTS performs better than several traditional and deep learning based fusion methods by providing visually superior fused results and better objective evaluation metrics. ","Wang et al. as mentioned in this paper proposed a foveated differentiable architecture search (F-DARTS) based unsupervised multimodal medical image fusion method, where the foveation operator is introduced into the weight learning process to fully explore human visual characteristics for the effective image fusion."
"Deep Learning-Based Glucose Prediction Models: A Guide for Practitioners and a Curated Dataset for Improved Diabetes Management","https://scispace.com/paper/deep-learning-based-glucose-prediction-models-a-guide-for-4gmuw435hu","2024","Journal Article","IEEE open journal of engineering in medicine and biology","Saúl Langarica
Diego Vega
Nawel Cariman
Martín Miranda
David C. Andrade
Felipe Núñez
María Rodríguez-Fernández","10.1109/ojemb.2024.3365290","","Accurate short- and mid-term blood glucose predictions are crucial for patients with diabetes struggling to maintain healthy glucose levels, as well as for individuals at risk of developing the disease. Consequently, numerous efforts from the scientific community have focused on developing predictive models for glucose levels. This study harnesses physiological data collected from wearable sensors to construct a series of data-driven models based on deep learning approaches. We systematically compare these models to offer insights for practitioners and researchers venturing into glucose prediction using deep learning techniques. Key questions addressed in this work encompass the comparison of various deep learning architectures for this task, determining the optimal set of input variables for accurate glucose prediction, comparing population-wide, fine-tuned, and personalized models, and assessing the impact of an individual's data volume on model performance. Additionally, as part of our outcomes, we introduce a meticulously curated dataset inclusive of data from both healthy individuals and those with diabetes, recorded in free-living conditions. This dataset aims to foster research in this domain and facilitate equitable comparisons among researchers. ","Deep learning-based glucose prediction models guide practitioners and introduce a curated dataset for improved diabetes management."
"Explainability meets uncertainty quantification: Insights from feature-based model fusion on multimodal time series","https://scispace.com/paper/explainability-meets-uncertainty-quantification-insights-1tks5gyz0l","2023","Journal Article","Information Fusion","Duarte Folgado
Marília Barandas
Lorenzo Famiglini
Ricardo Bruno Barbeiro dos Santos
Federico Cabitza
Hugo Gamboa","10.1016/j.inffus.2023.101955","","Feature importance evaluation is one of the prevalent approaches to interpreting Machine Learning (ML) models. A drawback of using these methods for high-dimensional datasets is that they often lead to high-dimensional explanation output that hinders human analysis. This is especially true for explaining multimodal ML models, where the problem’s complexity is further exacerbated by the inclusion of multiple data modalities and an increase in the overall number of features. This work proposes a novel approach to lower the complexity of feature-based explanations. The proposed approach is based on uncertainty quantification techniques, allowing for a principled way of reducing the number of modalities required to explain the model’s predictions. We evaluated our method in three multimodal datasets comprising physiological time series. Results show that the proposed method can reduce the complexity of the explanations while maintaining a high level of accuracy in the predictions. This study illustrates an innovative example of the intersection between the disciplines of uncertainty quantification and explainable AI. ","This study proposes a novel approach to simplify feature-based explanations for multimodal ML models using uncertainty quantification, reducing complexity while maintaining high accuracy in predictions on three physiological time series datasets."
"Machine-Learning-Based Diabetes Prediction Using Multisensor Data","https://scispace.com/paper/machine-learning-based-diabetes-prediction-using-multisensor-3kstivpw5s","2023","Journal Article","IEEE Sensors Journal","Aditi Site
Jari Nurmi
Elena Simon Lohan","10.1109/jsen.2023.3319360","","Diabetes is one such chronic disease that, if undetected, can result in several adverse symptoms or diseases. It requires continuous and active monitoring, for example, by using various smartphone sensors, wearable/smart watches, etc. These devices are minimally invasive in nature and can also track various physiological signals, which are important for the prediction of diabetes. Machine-learning algorithms and artificial intelligence are some of the most important tools used for the prediction/detection of diabetes using different types of physiological signals. In this study, we have focused on using multiple sensors such as glucose, electrocardiogram (ECG), accelerometer (ACC), and breathing sensors for classifying patients with diabetes disease. We analyzed whether a single sensor or multiple sensors can predict diabetes well. We identified various time-domain and interval-based features that are used for predicting diabetes and also the optimal window size for the feature calculation. We found that a multisensor combination using glucose, ECG, and ACC sensors gives the highest prediction accuracy of 98.2% with the extreme gradient boosting (XGBoost) algorithm. Moreover, multisensor prediction shows nearly 4%–5% increase in the diabetes prediction rates as compared to single sensors. We observed that breathing-sensor-related data have very little influence on the prediction of diabetes. We also used the score-fit-times curve as one of the metrics for the evaluation of models. From the performance curves, we observed that three-sensor combinations using glucose, ECG, and ACC converge faster as compared to a four-sensor combination while achieving with same accuracy.","This study focused on using multiple sensors such as glucose, electrocardiogram (ECG), accelerometer, and breathing sensors for classifying patients with diabetes disease and found that a multisensor combination using glucose, ECG, and ACC sensors gives the highest prediction accuracy with the extreme gradient boosting (XGBoost) algorithm."
"Multi-Feature Complementary Learning for Diabetes Mellitus Detection Using Pulse Signals","https://scispace.com/paper/multi-feature-complementary-learning-for-diabetes-mellitus-36ftc2v4","2022","Journal Article","IEEE Journal of Biomedical and Health Informatics","","10.1109/jbhi.2022.3198792","","Computational pulse diagnosis is a convenient, non-invasive, and effective Diabetes Mellitus (DM) detection technique. Generally, diverse pulse features are extracted from different views to represent pulse signals and then used for achieving the pulse diagnosis. However, current pulse-based DM detection methods only used one pulse feature for detection, ignoring the fact that diverse pulse features can be combined together to boost the diagnosis performance. To this end, we propose a novel Multi-Feature Complementary Learning (MFCL) model for DM detection. By designing feature-specific projections, multiple features are separately projected into a shared observation space and effectively fused into one vector. Besides, a mapping function is built to correlate the fused vectors to category labels to make the fused vectors suitable for classification. Inspired by the graph Laplacian matrix, which effectively preserves the correlations among samples from different categories, we integrate it in MFCL and design a discriminative prior to make the fused vectors sufficiently discriminative. Finally, an optimization algorithm is proposed to alternatively optimize the projection variables and then generate fused feature vectors. The proposed method reaches an accuracy of 92.85% in DM detection, outperforming state-of-the-art methods. ","Wang et al. as discussed by the authors proposed a multi-feature complementary learning (MFCL) model to combine diverse pulse features to boost the diagnosis performance, which achieved an accuracy of 92.85%."
"Tongue image fusion and analysis of thermal and visible images in diabetes mellitus using machine learning techniques","https://scispace.com/paper/tongue-image-fusion-and-analysis-of-thermal-and-visible-2qpiv4z68p","2024","Journal Article","Dental science reports","Usharani Thirunavukkarasu
U. Snekhalatha
V. Sowmya
Tahani Jaser Alahmadi","10.1038/s41598-024-64150-0","https://scispace.compdf/tongue-image-fusion-and-analysis-of-thermal-and-visible-2qpiv4z68p.pdf","Abstract The study aimed to achieve the following objectives: (1) to perform the fusion of thermal and visible tongue images with various fusion rules of discrete wavelet transform (DWT) to classify diabetes and normal subjects; (2) to obtain the statistical features in the required region of interest from the tongue image before and after fusion; (3) to distinguish the healthy and diabetes using fused tongue images based on deep and machine learning algorithms. The study participants comprised of 80 normal subjects and age- and sex-matched 80 diabetes patients. The biochemical tests such as fasting glucose, postprandial, Hba1c are taken for all the participants. The visible and thermal tongue images are acquired using digital single lens reference camera and thermal infrared cameras, respectively. The digital and thermal tongue images are fused based on the wavelet transform method. Then Gray level co-occurrence matrix features are extracted individually from the visible, thermal, and fused tongue images. The machine learning classifiers and deep learning networks such as VGG16 and ResNet50 was used to classify the normal and diabetes mellitus. Image quality metrics are implemented to compare the classifiers’ performance before and after fusion. Support vector machine outperformed the machine learning classifiers, well after fusion with an accuracy of 88.12% compared to before the fusion process (Thermal-84.37%; Visible-63.1%). VGG16 produced the classification accuracy of 94.37% after fusion and attained 90.62% and 85% before fusion of individual thermal and visible tongue images, respectively. Therefore, this study results indicates that fused tongue images might be used as a non-contact elemental tool for pre-screening type II diabetes mellitus. ","Fusion and analysis of tongue images in diabetes mellitus using machine learning techniques for pre-screening purposes. The study achieved high classification accuracy using fused tongue images and deep learning algorithms."
"Multi-Feature Complementary Learning for Diabetes Mellitus Detection Using Pulse Signals","https://scispace.com/paper/multi-feature-complementary-learning-for-diabetes-mellitus-5aysaxdf","2022","Journal Article","IEEE Journal of Biomedical and Health Informatics","Chaoxun Guo
Zhixing Jiang
David Zhang","10.1109/JBHI.2022.3198792","","Computational pulse diagnosis is a convenient, non-invasive, and effective Diabetes Mellitus (DM) detection technique. Generally, diverse pulse features are extracted from different views to represent pulse signals and then used for achieving the pulse diagnosis. However, current pulse-based DM detection methods only used one pulse feature for detection, ignoring the fact that diverse pulse features can be combined together to boost the diagnosis performance. To this end, we propose a novel Multi-Feature Complementary Learning (MFCL) model for DM detection. By designing feature-specific projections, multiple features are separately projected into a shared observation space and effectively fused into one vector. Besides, a mapping function is built to correlate the fused vectors to category labels to make the fused vectors suitable for classification. Inspired by the graph Laplacian matrix, which effectively preserves the correlations among samples from different categories, we integrate it in MFCL and design a discriminative prior to make the fused vectors sufficiently discriminative. Finally, an optimization algorithm is proposed to alternatively optimize the projection variables and then generate fused feature vectors. The proposed method reaches an accuracy of 92.85% in DM detection, outperforming state-of-the-art methods.","A novel Multi-Feature Complementary Learning (MFCL) model is proposed for DM detection that effectively preserves the correlations among samples from different categories and reaches an accuracy of 92.85% in DM detection, outperforming state-of-the-art methods."
"Leveraging the future of diagnosis and management of diabetes: From old indexes to new technologies","https://scispace.com/paper/leveraging-the-future-of-diagnosis-and-management-of-1asuuet4","2022","Journal Article","European Journal of Clinical Investigation","Maria João Meneses
Rita S. Patarrão
Tomás Pinheiro
Inês Coelho
Nuno Carriço
Ana C. Marques
Artur Romão
João Miguel Casanova Nabais
Elvira Fortunato
João Filipe Raposo
Maria Paula Macedo","10.1111/eci.13934","https://doi.org/10.1111/eci.13934","Diabetes is a heterogeneous and multifactorial disease. However, glycemia and glycated hemoglobin have been the focus of diabetes diagnosis and management for the last decades. As diabetes management goes far beyond glucose control, it has become clear that assessment of other biochemical parameters gives a much wider view of the metabolic state of each individual, enabling a precision medicine approach.","In this article , it has become clear that assessment of other biochemical parameters gives a much wider view of the metabolic state of each individual, enabling a precision medicine approach to diagnose and manage diabetes."
"Glu-ensemble: An ensemble deep learning framework for blood glucose forecasting in type 2 diabetes patients","https://scispace.com/paper/glu-ensemble-an-ensemble-deep-learning-framework-for-blood-56deng01hc","2024","Journal Article","Heliyon","Yechan Han
Dae-Yeon Kim
Jiyoung Woo
Jaeyun Kim","10.1016/j.heliyon.2024.e29030","","<h2>Abstract</h2> Diabetes is a chronic metabolic disorder characterized by elevated blood glucose levels, posing significant health risks such as cardiovascular disease, and nerve, kidney, and eye damage. Effective management of blood glucose is essential for individuals with diabetes to mitigate these risks. This study introduces the Glu-Ensemble, a deep learning framework designed for precise blood glucose forecasting in patients with type 2 diabetes. Unlike other predictive models, Glu-Ensemble addresses challenges related to small sample sizes, data quality issues, reliance on strict statistical assumptions, and the complexity of models. It enhances prediction accuracy and model generalizability by utilizing larger datasets and reduces bias inherent in many predictive models. The framework's unified approach, as opposed to patient-specific models, eliminates the need for initial calibration time, facilitating immediate blood glucose predictions for new patients. The obtained results indicate that Glu-Ensemble surpasses traditional methods in accuracy, as measured by root mean square error, mean absolute error, and error grid analysis. The Glu-Ensemble framework emerges as a promising tool for blood glucose level prediction in type 2 diabetes patients, warranting further investigation in clinical settings for its practical application. ","The Glu-Ensemble framework emerges as a promising tool for blood glucose level prediction in type 2 diabetes patients, warranting further investigation in clinical settings for its practical application."
"F-DARTS: Foveated Differentiable Architecture Search Based Multimodal Medical Image Fusion.","https://scispace.com/paper/f-darts-foveated-differentiable-architecture-search-based-2eppk8zg","2023","Journal Article","IEEE Transactions on Medical Imaging","Shaozhuang Ye
Tuo Wang
Mingyu Ding
Luming Zhang","10.1109/TMI.2023.3283517","","Multimodal medical image fusion (MMIF) is highly significant in such fields as disease diagnosis and treatment. The traditional MMIF methods are difficult to provide satisfactory fusion accuracy and robustness due to the influence of such possible human-crafted components as image transform and fusion strategies. Existing deep learning based fusion methods are generally difficult to ensure image fusion effect due to the adoption of a human-designed network structure and a relatively simple loss function and the ignorance of human visual characteristics during weight learning. To address these issues, we have presented the foveated differentiable architecture search (F-DARTS) based unsupervised MMIF method. In this method, the foveation operator is introduced into the weight learning process to fully explore human visual characteristics for the effective image fusion. Meanwhile, a distinctive unsupervised loss function is designed for network training by integrating mutual information, sum of the correlations of differences, structural similarity and edge preservation value. Based on the presented foveation operator and loss function, an end-to-end encoder-decoder network architecture will be searched using the F-DARTS to produce the fused image. Experimental results on three multimodal medical image datasets demonstrate that the F-DARTS performs better than several traditional and deep learning based fusion methods by providing visually superior fused results and better objective evaluation metrics.","Zhang et al. as discussed by the authors proposed a foveated differentiable architecture search (F-DARTS) based unsupervised multimodal medical image fusion method, where the foveation operator is introduced into the weight learning process to fully explore human visual characteristics for the effective image fusion."
"GJFusion: A Channel-Level Correlation Construction Method for Multimodal Physiological Signal Fusion","https://scispace.com/paper/gjfusion-a-channel-level-correlation-construction-method-for-mbpam140wm","2023","Journal Article","ACM Transactions on Multimedia Computing, Communications, and Applications","Wuliang Huang
Yiqiang Chen
Xin Jian
Teng Zhang
Qian Chen","10.1145/3617503","","Physiological signal based ubiquitous computing has garnered significant attention. However, the heterogeneity among multimodal physiological signals poses a critical challenge to practical applications. To traverse this heterogeneity gap, recent studies have focused on establishing inter-modality correlations. Early works only consider coarse-level correlations between the embeddings of each modality. More recent graph-based approaches incorporate prior knowledge-based correlations, although they may not be entirely accurate. In this article, we propose the Graph Joint Fusion (GJFusion) network, which leverages channel-level inter-modality correlations based on a graph joint to mitigate the heterogeneous gap. Our proposed GJFusion first represents each modality as a graph, with each vertex corresponding to a signal channel, and the edges denoting their functional connectivity. We then join each modality by constructing inter-modality correlations for each salient channel using a sampling-based matching method. Discarded channels are transformed into a virtual vertex through a lightweight pooling operation. Subsequently, the fusion network integrates intra- and inter-modality features, enabling multimodal physiological signal fusion. To validate the effectiveness of our method, we select emotional state recognition as the downstream task and conduct comprehensive experiments on two benchmark datasets. The results demonstrate that our proposed GJFusion network surpasses the latest state-of-the-art methods, achieving relative accuracy improvements of 1.22% and 0.81% on the DEAP and MAHNOB-HCI datasets, respectively. Furthermore, visualization experiments of the salient brain regions reveal the presence of interpretable knowledge within the proposed GJFusion model.","The results demonstrate that the proposed GJFusion network surpasses the latest state-of-the-art methods, achieving relative accuracy improvements of 1.22% and 0.81% on the DEAP and MAHNOB-HCI datasets, respectively."
"Transformer-Based End-to-End Anatomical and Functional Image Fusion","https://scispace.com/paper/transformer-based-end-to-end-anatomical-and-functional-image-1in3qou6","","Journal Article","IEEE Transactions on Instrumentation and Measurement","Jian Zhang
Aiping Liu
Dan Wang
Yu Biao Liu
M. J. Wang
Xun Chen","10.1109/TIM.2022.3200426","","Medical image fusion aims to derive complementary information from medical images with different modalities and is becoming increasingly important in clinical applications. The design of fusion strategy plays a key role in achieving high-quality fusion results. Existing methods usually employ handcrafted fusion rules or convolution-based networks to fuse multimodal medical images. However, these fusion strategies are insufficiently fine-grained and cannot capture global information of multimodal images effectively. Moreover, for deep learning-based fusion methods, they always concatenate source images/deep features of different modalities to input into the neural network, easily leading to inadequate information utilization from source images. To address these problems, we propose a transformer-based end-to-end framework for medical image fusion, termed TransFusion. The proposed TransFusion introduces the transformer as the fusion strategy, which utilizes its self-attention mechanism to incorporate the global contextual information of multimodal features and fuse them adequately. Besides, unlike traditional parallel multibranch architectures or shared networks used for multiple inputs, we design branch networks that interact through fusion transformers at multiple scales to utilize the information of different modalities more adequately. A natural advantage of our design is the ability to aggregate global multimodal features through self-attention. Both qualitative and quantitative experiments demonstrate the superiority of our method over the state-of-the-art fusion methods.","The proposed TransFusion introduces the transformer as the fusion strategy, which utilizes its self-attention mechanism to incorporate the global contextual information of multimodal features and fuse them adequately."
"Noninvasive Blood Glucose Monitoring Using Spatiotemporal ECG and PPG Feature Fusion and Weight-Based Choquet Integral Multimodel Approach.","https://scispace.com/paper/noninvasive-blood-glucose-monitoring-using-spatiotemporal-3038z28h","2023","Journal Article","IEEE transactions on neural networks and learning systems","Jingzhen Li
Jing Ma
Olatunji Mumini Omisore
Yuhang Liu
Huajie Tang
Yanhong Yan
Liang Wang
Zedong Nie","10.1109/TNNLS.2023.3279383","","change of blood glucose (BG) level stimulates the autonomic nervous system leading to variation in both human's electrocardiogram (ECG) and photoplethysmogram (PPG). In this article, we aimed to construct a novel multimodal framework based on ECG and PPG signal fusion to establish a universal BG monitoring model. This is proposed as a spatiotemporal decision fusion strategy that uses weight-based Choquet integral for BG monitoring. Specifically, the multimodal framework performs three-level fusion. First, ECG and PPG signals are collected and coupled into different pools. Second, the temporal statistical features and spatial morphological features in the ECG and PPG signals are extracted through numerical analysis and residual networks, respectively. Furthermore, the suitable temporal statistical features are determined with three feature selection techniques, and the spatial morphological features are compressed by deep neural networks (DNNs). Lastly, weight-based Choquet integral multimodel fusion is integrated for coupling different BG monitoring algorithms based on the temporal statistical features and spatial morphological features. To verify the feasibility of the model, a total of 103 days of ECG and PPG signals encompassing 21 participants were collected in this article. The BG levels of participants ranged between 2.2 and 21.8 mmol/L. The results obtained show that the proposed model has excellent BG monitoring performance with a root-mean-square error (RMSE) of 1.49 mmol/L, mean absolute relative difference (MARD) of 13.42%, and Zone A + B of 99.49% in tenfold cross-validation. Therefore, we conclude that the proposed fusion approach for BG monitoring has potentials in practical applications of diabetes management.","In this article , a novel multimodal framework based on ECG and photoplethysmogram (PPG) signal fusion is proposed to establish a universal BG monitoring model."
"A review of deep learning-based information fusion techniques for multimodal medical image classification","https://scispace.com/paper/a-review-of-deep-learning-based-information-fusion-4o6pxnifr9","2024","","Computers in Biology and Medicine","Yi-Hsuan Li
Mostafa El Habib Daho
P. Conze
Rachid Zeghlache
Hugo Le Boit'e
Ramin Tadayoni
Béatrice Cochener
Mathieu Lamard
Gwenole Quellec","10.1016/j.compbiomed.2024.108635","","Multimodal medical imaging plays a pivotal role in clinical diagnosis and research, as it combines information from various imaging modalities to provide a more comprehensive understanding of the underlying pathology. Recently, deep learning-based multimodal fusion techniques have emerged as powerful tools for improving medical image classification. This review offers a thorough analysis of the developments in deep learning-based multimodal fusion for medical classification tasks. We explore the complementary relationships among prevalent clinical modalities and outline three main fusion schemes for multimodal classification networks: input fusion, intermediate fusion (encompassing single-level fusion, hierarchical fusion, and attention-based fusion), and output fusion. By evaluating the performance of these fusion techniques, we provide insight into the suitability of different network architectures for various multimodal fusion scenarios and application domains. Furthermore, we delve into challenges related to network architecture selection, handling incomplete multimodal data management, and the potential limitations of multimodal fusion. Finally, we spotlight the promising future of Transformer-based multimodal fusion techniques and give recommendations for future research in this rapidly evolving field. ","This review analyzes deep learning-based multimodal fusion techniques for medical image classification, evaluating three fusion schemes and network architectures, and highlighting challenges and future directions, including the potential of Transformer-based multimodal fusion."
"AF: An Association-Based Fusion Method for Multi-Modal Classification","https://scispace.com/paper/af-an-association-based-fusion-method-for-multi-modal-3cwcta7k","2022","Journal Article","IEEE Transactions on Pattern Analysis and Machine Intelligence","","10.1109/tpami.2021.3125995","","Multi-modal classification (MMC) aims to integrate the complementary information from different modalities to improve classification performance. Existing MMC methods can be grouped into two categories: traditional methods and deep learning-based methods. The traditional methods often implement fusion in a low-level original space. Besides, they mostly focus on the inter-modal fusion and neglect the intra-modal fusion. Thus, the representation capacity of fused features induced by them is insufficient. The deep learning-based methods implement the fusion in a high-level feature space where the associations among features are considered, while the whole process is implicit and the fused space lacks interpretability. Based on these observations, we propose a novel interpretative association-based fusion method for MMC, named AF. In AF, both the association information and the high-order information extracted from feature space are simultaneously encoded into a new feature space to help to train an MMC model in an explicit manner. Moreover, AF is a general fusion framework, and most existing MMC methods can be embedded into it to improve their performance. Finally, the effectiveness and the generality of AF are validated on 22 datasets, four typically traditional MMC methods adopting best modality, early, late and model fusion strategies and a deep learning-based MMC method. ","In this paper , the authors proposed an interpretative association-based fusion method for multi-modal classification, where both the association information and the high-order information extracted from feature space are simultaneously encoded into a new feature space to help to train an MMC model in an explicit manner."
"Hierarchical Progressive Network for Multimodal Medical Image Fusion in Healthcare Systems","https://scispace.com/paper/hierarchical-progressive-network-for-multimodal-medical-2gl89ksq","2022","Journal Article","IEEE Transactions on Computational Social Systems","Si-Ya Yang
Xiaomin Yang
Rongzhu Zhang
Kai Liu","10.1109/tcss.2022.3165559","","Deep learning (DL)-based multisource information processing plays an essential role in the Internet of Medical Things (IoMT). In this field, medical image fusion integrates scan results from different devices, supporting healthcare systems to make a more informed diagnosis. This study proposes a DL-based network to fuse multimodal medical images. In our method, the lattice unit (LU) is designed to improve the representation capability of the fusion network. Moreover, to acquire hierarchical features from images, the progressive module (PM) treats the network's shallow and deep layers differently. The shallow layers represent the structure of the source image; the deep layers correspond to details. Different loss functions are utilized for these two kinds of information to retain fused images' salient structures and functional information. Experiments show that the proposed algorithm performs well on visual quality and objective evaluation, which provides a reliable reference for medical diagnosis. In addition, this method is lightweight and speedy compared to existing algorithms, facilitating further placement in the IoMT's specific devices. ","Li et al. as mentioned in this paper proposed a DL-based network to fuse multimodal medical images, in which the lattice unit was designed to improve the representation capability of the fusion network, and the progressive module treated the network's shallow and deep layers differently."
"Diabetes and artificial intelligence beyond the closed loop: a review of the landscape, promise and challenges.","https://scispace.com/paper/diabetes-and-artificial-intelligence-beyond-the-closed-loop-1yju2pdvuu","2023","","Diabetologia","Scott C. Mackenzie
Chris A R Sainsbury
Deborah J Wake","10.1007/s00125-023-06038-8","","Abstract The discourse amongst diabetes specialists and academics regarding technology and artificial intelligence (AI) typically centres around the 10% of people with diabetes who have type 1 diabetes, focusing on glucose sensors, insulin pumps and, increasingly, closed-loop systems. This focus is reflected in conference topics, strategy documents, technology appraisals and funding streams. What is often overlooked is the wider application of data and AI, as demonstrated through published literature and emerging marketplace products, that offers promising avenues for enhanced clinical care, health-service efficiency and cost-effectiveness. This review provides an overview of AI techniques and explores the use and potential of AI and data-driven systems in a broad context, covering all diabetes types, encompassing: (1) patient education and self-management; (2) clinical decision support systems and predictive analytics, including diagnostic support, treatment and screening advice, complications prediction; and (3) the use of multimodal data, such as imaging or genetic data. The review provides a perspective on how data- and AI-driven systems could transform diabetes care in the coming years and how they could be integrated into daily clinical practice. We discuss evidence for benefits and potential harms, and consider existing barriers to scalable adoption, including challenges related to data availability and exchange, health inequality, clinician hesitancy and regulation. Stakeholders, including clinicians, academics, commissioners, policymakers and those with lived experience, must proactively collaborate to realise the potential benefits that AI-supported diabetes care could bring, whilst mitigating risk and navigating the challenges along the way. Graphical Abstract ","This review provides an overview of AI techniques and explores the use and potential of AI and data-driven systems in a broad context, covering all diabetes types, encompassing: patient education and self-management; clinical decision support systems and predictive analytics; and the use of multimodal data."
"Guest Editorial: Information Fusion for Medical Data: Early, Late, and Deep Fusion Methods for Multimodal Data","https://scispace.com/paper/guest-editorial-information-fusion-for-medical-data-early-1w0ww66uph","2020","Journal Article","IEEE Journal of Biomedical and Health Informatics","Inês Domingues
Henning Muller
Andrés Ortiz
Belur V. Dasarathy
Pedro Henriques Abreu
Vince D. Calhoun","10.1109/JBHI.2019.2958429","https://scispace.com/pdf/guest-editorial-information-fusion-for-medical-data-early-1w0ww66uph.pdf","The papers in this special section examine important current topics on multimodal data fusion in the medical context. All clinical data, including genomic and proteomic, play a role in the diagnosis and in particular in the treatment planning and follow-up. This is true for all types of data analyses whether in classification, regression, retrieval, clustering, or other. The interaction between several types of information is not always well understood. Experienced clinicians automatically and even unconsciously add multiple sources of information into their decision process, but machine learning tools often concentrate on single information sources. This special issue presents five examples where several data sources are fused. The papers give several examples of fusion techniques and also the results obtained in quite different application scenarios.","The papers in this special section examine important current topics on multimodal data fusion in the medical context and give several examples of fusion techniques and also the results obtained in quite different application scenarios."
"The Present and Future of Artificial Intelligence-Based Medical Image in Diabetes Mellitus: Focus on Analytical Methods and Limitations of Clinical Use","https://scispace.com/paper/the-present-and-future-of-artificial-intelligence-based-ckh0ilxx9x","2023","","Journal of Korean Medical Science","Ji Chun
Hun-Sung Kim","10.3346/jkms.2023.38.e253","","Artificial intelligence (AI)-based diagnostic technology using medical images can be used to increase examination accessibility and support clinical decision-making for screening and diagnosis. To determine a machine learning algorithm for diabetes complications, a literature review of studies using medical image-based AI technology was conducted using the National Library of Medicine PubMed, and the Excerpta Medica databases. Lists of studies using diabetes diagnostic images and AI as keywords were combined. In total, 227 appropriate studies were selected. Diabetic retinopathy studies using the AI model were the most frequent (85.0%, 193/227 cases), followed by diabetic foot (7.9%, 18/227 cases) and diabetic neuropathy (2.7%, 6/227 cases). The studies used open datasets (42.3%, 96/227 cases) or directly constructed data from fundoscopy or optical coherence tomography (57.7%, 131/227 cases). Major limitations in AI-based detection of diabetes complications using medical images were the lack of datasets (36.1%, 82/227 cases) and severity misclassification (26.4%, 60/227 cases). Although it remains difficult to use and fully trust AI-based imaging analysis technology clinically, it reduces clinicians' time and labor, and the expectations from its decision-support roles are high. Various data collection and synthesis data technology developments according to the disease severity are required to solve data imbalance. ","Although it remains difficult to use and fully trust AI-based imaging analysis technology clinically, it reduces clinicians’ time and labor, and the expectations from its decision-support roles are high."
"Deep Learning for Automated Diabetic Retinopathy Screening Fused With Heterogeneous Data From EHRs Can Lead to Earlier Referral Decisions.","https://scispace.com/paper/deep-learning-for-automated-diabetic-retinopathy-screening-5d7c8rxyhw","2021","Journal Article","Translational Vision Science & Technology","Min Yen Hsu
Min Yen Hsu
Jeng Yuan Chiou
Jung Tzu Liu
Chee Ming Lee
Ya Wen Lee
Chien Chih Chou
Shih Chang Lo
Edy Kornelius
Yi Sun Yang
Sung Yen Chang
Yu Cheng Liu
Chien-Ning Huang
Vincent S. Tseng","10.1167/TVST.10.9.18","","Purpose Fundus images are typically used as the sole training input for automated diabetic retinopathy (DR) classification. In this study, we considered several well-known DR risk factors and attempted to improve the accuracy of DR screening. Metphods Fusing nonimage data (e.g., age, gender, smoking status, International Classification of Disease code, and laboratory tests) with data from fundus images can enable an end-to-end deep learning architecture for DR screening. We propose a neural network that simultaneously trains heterogeneous data and increases the performance of DR classification in terms of sensitivity and specificity. In the current retrospective study, 13,410 fundus images and their corresponding nonimage data were collected from the Chung Shan Medical University Hospital in Taiwan. The images were classified as either nonreferable or referable for DR by a panel of ophthalmologists. Cross-validation was used for the training models and to evaluate the classification performance. Results The proposed fusion model achieved 97.96% area under the curve with 96.84% sensitivity and 89.44% specificity for determining referable DR from multimodal data, and significantly outperformed the models that used image or nonimage information separately. Conclusions The fusion model with heterogeneous data has the potential to improve referable DR screening performance for earlier referral decisions. Translational relevance Artificial intelligence fused with heterogeneous data from electronic health records could provide earlier referral decisions from DR screening.","In this article, a neural network that simultaneously trains heterogeneous data and increases the performance of diabetic retinopathy classification in terms of sensitivity and specificity has been proposed to improve the accuracy of DR screening."
"Noninvasive Blood Glucose Monitoring Using Spatiotemporal ECG and PPG Feature Fusion and Weight-Based Choquet Integral Multimodel Approach","https://scispace.com/paper/noninvasive-blood-glucose-monitoring-using-spatiotemporal-enivozef","2023","Journal Article","IEEE transactions on neural networks and learning systems","","10.1109/tnnls.2023.3279383","","change of blood glucose (BG) level stimulates the autonomic nervous system leading to variation in both human's electrocardiogram (ECG) and photoplethysmogram (PPG). In this article, we aimed to construct a novel multimodal framework based on ECG and PPG signal fusion to establish a universal BG monitoring model. This is proposed as a spatiotemporal decision fusion strategy that uses weight-based Choquet integral for BG monitoring. Specifically, the multimodal framework performs three-level fusion. First, ECG and PPG signals are collected and coupled into different pools. Second, the temporal statistical features and spatial morphological features in the ECG and PPG signals are extracted through numerical analysis and residual networks, respectively. Furthermore, the suitable temporal statistical features are determined with three feature selection techniques, and the spatial morphological features are compressed by deep neural networks (DNNs). Lastly, weight-based Choquet integral multimodel fusion is integrated for coupling different BG monitoring algorithms based on the temporal statistical features and spatial morphological features. To verify the feasibility of the model, a total of 103 days of ECG and PPG signals encompassing 21 participants were collected in this article. The BG levels of participants ranged between 2.2 and 21.8 mmol/L. The results obtained show that the proposed model has excellent BG monitoring performance with a root-mean-square error (RMSE) of 1.49 mmol/L, mean absolute relative difference (MARD) of 13.42%, and Zone A + B of 99.49% in tenfold cross-validation. Therefore, we conclude that the proposed fusion approach for BG monitoring has potentials in practical applications of diabetes management. ","In this paper , a novel multimodal framework based on ECG and photoplethysmogram (PPG) signal fusion is proposed to establish a universal BG monitoring model."
"Deep Multitask Learning by Stacked Long Short-Term Memory for Predicting Personalized Blood Glucose Concentration","https://scispace.com/paper/deep-multitask-learning-by-stacked-long-short-term-memory-23u33v0v","2023","Journal Article","IEEE Journal of Biomedical and Health Informatics","Md. Maruf Hossain Shuvo
Syed K. Islam","10.1109/JBHI.2022.3233486","","The adverse glycemic events triggered by the inaccurate insulin infusion in Type I diabetes (T1D) can lead to fatal complications. Predicting blood glucose concentration (BGC) based on clinical health records is critical for control algorithms in the artificial pancreas (AP) and aiding in medical decision support. This paper presents a novel deep learning (DL) model incorporating multitask learning (MTL) for personalized blood glucose prediction. The network architecture consists of shared and clustered hidden layers. Two layers of stacked long short-term memory (LSTM) form the shared hidden layers that learn generalized features from all subjects. The clustered hidden layers comprise two dense layers adapting to the gender-specific variability in the data. Finally, the subject-specific dense layers offer additional fine-tuning to personalized glucose dynamics resulting in an accurate BGC prediction at the output. OhioT1DM clinical dataset is used for the training and performance evaluation of the proposed model. A detailed analytical and clinical assessment have been performed using root mean square (RMSE), mean absolute error (MAE), and Clarke error grid analysis (EGA), respectively, which demonstrates the robustness and reliability of the proposed method. Consistently leading performance has been achieved for 30- (RMSE = 16.06 <inline-formula><tex-math notation=""LaTeX"">$\pm$</tex-math></inline-formula> 2.74, MAE = 10.64 <inline-formula><tex-math notation=""LaTeX"">$\pm$</tex-math></inline-formula> 1.35), 60- (RMSE = 30.89 <inline-formula><tex-math notation=""LaTeX"">$\pm$</tex-math></inline-formula> 4.31, MAE = 22.07 <inline-formula><tex-math notation=""LaTeX"">$\pm$</tex-math></inline-formula> 2.96), 90- (RMSE = 40.51 <inline-formula><tex-math notation=""LaTeX"">$\pm$</tex-math></inline-formula> 5.16, MAE = 30.16 <inline-formula><tex-math notation=""LaTeX"">$\pm$</tex-math></inline-formula> 4.10), and 120-minute (RMSE = 47.39 <inline-formula><tex-math notation=""LaTeX"">$\pm$</tex-math></inline-formula> 5.62, MAE = 36.36 <inline-formula><tex-math notation=""LaTeX"">$\pm$</tex-math></inline-formula> 4.54) prediction horizon (PH). In addition, the EGA analysis confirms the clinical feasibility by maintaining more than 94% BGC predictions in the clinically safe zone for up to 120-minute PH. Moreover, the improvement is established by benchmarking against the state-of-the-art statistical, machine learning (ML), and deep learning (DL) methods.","In this paper , a novel deep learning (DL) model incorporating multitask learning (MTL) for personalized blood glucose prediction is presented, where two layers of stacked long short-term memory (LSTM) form the shared hidden layers that learn generalized features from all subjects."
"A Review on Data Fusion of Multidimensional Medical and Biomedical Data","https://scispace.com/paper/a-review-on-data-fusion-of-multidimensional-medical-and-3btku6y3","2022","Journal Article","Molecules","Kazi Sultana Farhana Azam
Oleg Ryabchykov
Thomas Bocklitz","10.3390/molecules27217448","https://scispace.com/pdf/a-review-on-data-fusion-of-multidimensional-medical-and-3btku6y3.pdf","Data fusion aims to provide a more accurate description of a sample than any one source of data alone. At the same time, data fusion minimizes the uncertainty of the results by combining data from multiple sources. Both aim to improve the characterization of samples and might improve clinical diagnosis and prognosis. In this paper, we present an overview of the advances achieved over the last decades in data fusion approaches in the context of the medical and biomedical fields. We collected approaches for interpreting multiple sources of data in different combinations: image to image, image to biomarker, spectra to image, spectra to spectra, spectra to biomarker, and others. We found that the most prevalent combination is the image-to-image fusion and that most data fusion approaches were applied together with deep learning or machine learning methods.","In this article , the authors present an overview of the advances achieved over the last decades in data fusion approaches in the context of the medical and biomedical fields, and they collected approaches for interpreting multiple sources of data in different combinations."
"Dual-level Deep Evidential Fusion: Integrating multimodal information for enhanced reliable decision-making in deep learning","https://scispace.com/paper/dual-level-deep-evidential-fusion-integrating-multimodal-57epj9f0ru","2023","Journal Article","Information Fusion","Zhimin Shao
Weibei Dou
Yu Pan","10.1016/j.inffus.2023.102113","","Multimodal learning has gained significant attention in recent years for combining information from different modalities using Deep Neural Networks (DNNs). However, existing approaches often overlook the varying importance of modalities and neglect uncertainty estimation, leading to limited generalization and unreliable predictions. In this paper, we propose a novel algorithm, Dual-level Deep Evidential Fusion (DDEF), to address these challenges by integrating multimodal information at both the Basic Belief Assignment (BBA) level and multimodal level, for enhancing accuracy, robustness, and reliability. The proposed DDEF approach utilizes the Dirichlet framework and BBA methods to connect neural network outputs with Dirichlet distribution parameters, enabling effective uncertainty estimation, and the Dempster-Shafer Theory (DST) is used for dual-level fusion, facilitating the fusion of evidence from two BBA methods and multiple modalities. It has been validated by two experiments on synthetic digit classification, and real-world medical prognosis after brain-computer interface (BCI) treatment, and by demonstrating superior performance compared to existing methods. Our findings emphasize the importance of considering multimodal integration and uncertainty estimation for reliable decision-making in deep learning. ","This paper proposes Dual-level Deep Evidential Fusion (DDEF), a novel algorithm integrating multimodal information at two levels to enhance accuracy, robustness, and reliability in deep learning, utilizing Dirichlet distribution and Dempster-Shafer Theory for uncertainty estimation and evidence fusion."
"Deep Multitask Learning by Stacked Long Short-Term Memory for Predicting Personalized Blood Glucose Concentration","https://scispace.com/paper/deep-multitask-learning-by-stacked-long-short-term-memory-2xufjrzz","2023","Journal Article","IEEE Journal of Biomedical and Health Informatics","","10.1109/jbhi.2022.3233486","","The adverse glycemic events triggered by the inaccurate insulin infusion in Type I diabetes (T1D) can lead to fatal complications. Predicting blood glucose concentration (BGC) based on clinical health records is critical for control algorithms in the artificial pancreas (AP) and aiding in medical decision support. This paper presents a novel deep learning (DL) model incorporating multitask learning (MTL) for personalized blood glucose prediction. The network architecture consists of shared and clustered hidden layers. Two layers of stacked long short-term memory (LSTM) form the shared hidden layers that learn generalized features from all subjects. The clustered hidden layers comprise two dense layers adapting to the gender-specific variability in the data. Finally, the subject-specific dense layers offer additional fine-tuning to personalized glucose dynamics resulting in an accurate BGC prediction at the output. OhioT1DM clinical dataset is used for the training and performance evaluation of the proposed model. A detailed analytical and clinical assessment have been performed using root mean square (RMSE), mean absolute error (MAE), and Clarke error grid analysis (EGA), respectively, which demonstrates the robustness and reliability of the proposed method. Consistently leading performance has been achieved for 30- (RMSE = 16.06 <inline-formula xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink""><tex-math notation=""LaTeX"">$\pm$</tex-math></inline-formula> 2.74, MAE = 10.64 <inline-formula xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink""><tex-math notation=""LaTeX"">$\pm$</tex-math></inline-formula> 1.35), 60- (RMSE = 30.89 <inline-formula xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink""><tex-math notation=""LaTeX"">$\pm$</tex-math></inline-formula> 4.31, MAE = 22.07 <inline-formula xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink""><tex-math notation=""LaTeX"">$\pm$</tex-math></inline-formula> 2.96), 90- (RMSE = 40.51 <inline-formula xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink""><tex-math notation=""LaTeX"">$\pm$</tex-math></inline-formula> 5.16, MAE = 30.16 <inline-formula xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink""><tex-math notation=""LaTeX"">$\pm$</tex-math></inline-formula> 4.10), and 120-minute (RMSE = 47.39 <inline-formula xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink""><tex-math notation=""LaTeX"">$\pm$</tex-math></inline-formula> 5.62, MAE = 36.36 <inline-formula xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink""><tex-math notation=""LaTeX"">$\pm$</tex-math></inline-formula> 4.54) prediction horizon (PH). In addition, the EGA analysis confirms the clinical feasibility by maintaining more than 94% BGC predictions in the clinically safe zone for up to 120-minute PH. Moreover, the improvement is established by benchmarking against the state-of-the-art statistical, machine learning (ML), and deep learning (DL) methods. ","In this paper , a novel deep learning (DL) model incorporating multitask learning (MTL) for personalized blood glucose prediction is presented, where two layers of stacked long short-term memory (LSTM) form the shared hidden layers that learn generalized features from all subjects."
"Dual Low-Rank Multimodal Fusion","https://scispace.com/paper/dual-low-rank-multimodal-fusion-4q04itrucn","2020","Proceedings Article","Empirical Methods in Natural Language Processing","Tao Jin
Siyu Huang
Yingming Li
Zhongfei Zhang","10.18653/V1/2020.FINDINGS-EMNLP.35","https://scispace.com/pdf/dual-low-rank-multimodal-fusion-4q04itrucn.pdf","Tensor-based fusion methods have been proven effective in multimodal fusion tasks. However, existing tensor-based methods make a poor use of the fine-grained temporal dynamics of multimodal sequential features. Motivated by this observation, this paper proposes a novel multimodal fusion method called Fine-Grained Temporal Low-Rank Multimodal Fusion (FT-LMF). FT-LMF correlates the features of individual time steps between multiple modalities, while it involves multiplications of high-order tensors in its calculation. This paper further proposes Dual Low-Rank Multimodal Fusion (Dual-LMF) to reduce the computational complexity of FT-LMF through low-rank tensor approximation along dual dimensions of input features. Dual-LMF is conceptually simple and practically effective and efficient. Empirical studies on benchmark multimodal analysis tasks show that our proposed methods outperform the state-of-the-art tensor-based fusion methods with a similar computational complexity.","Empirical studies on benchmark multimodal analysis tasks show that the proposed methods outperform the state-of-the-art tensor-based fusion methods with a similar computational complexity."
"A multimodal approach to cardiovascular risk stratification in patients with type 2 diabetes incorporating retinal, genomic and clinical features","https://scispace.com/paper/a-multimodal-approach-to-cardiovascular-risk-stratification-28vll1ceu5","2019","Journal Article","Scientific Reports","Ahmed E. Fetit
Alex S. F. Doney
Stephen Hogg
Ruixuan Wang
Tom MacGillivray
Joanna M. Wardlaw
Fergus N. Doubal
Gareth J. McKay
Stephen J. McKenna
Emanuele Trucco","10.1038/S41598-019-40403-1","https://scispace.com/pdf/a-multimodal-approach-to-cardiovascular-risk-stratification-28vll1ceu5.pdf","Cardiovascular diseases are a public health concern; they remain the leading cause of morbidity and mortality in patients with type 2 diabetes. Phenotypic information available from retinal fundus images and clinical measurements, in addition to genomic data, can identify relevant biomarkers of cardiovascular health. In this study, we assessed whether such biomarkers stratified risks of major adverse cardiac events (MACE). A retrospective analysis was carried out on an extract from the Tayside GoDARTS bioresource of participants with type 2 diabetes (n = 3,891). A total of 519 features were incorporated, summarising morphometric properties of the retinal vasculature, various single nucleotide polymorphisms (SNPs), as well as routine clinical measurements. After imputing missing features, a predictive model was developed on a randomly sampled set (n = 2,918) using L1-regularised logistic regression (lasso). The model was evaluated on an independent set (n = 973) and its performance associated with overall hazard rate after censoring (log-rank p < 0.0001), suggesting that multimodal features were able to capture important knowledge for MACE risk assessment. We further showed through a bootstrap analysis that all three sources of information (retinal, genetic, routine clinical) offer robust signal. Particularly robust features included: tortuousity, width gradient, and branching point retinal groupings; SNPs known to be associated with blood pressure and cardiovascular phenotypic traits; age at imaging; clinical measurements such as blood pressure and high density lipoprotein. This novel approach could be used for fast and sensitive determination of future risks associated with MACE.","A novel approach could be used for fast and sensitive determination of future risks associated with MACE, showing through a bootstrap analysis that all three sources of information (retinal, genetic, routine clinical) offer robust signal."
"Diabetes detection based on machine learning and deep learning approaches","https://scispace.com/paper/diabetes-detection-based-on-machine-learning-and-deep-2fvfx2it45","2023","Journal Article","Multimedia Tools and Applications","Boon Feng Wee
Saaveethya Sivakumar
King Hann Lim
W. K. Wong
Filbert H. Juwono","10.1007/s11042-023-16407-5","","Abstract The increasing number of diabetes individuals in the globe has alarmed the medical sector to seek alternatives to improve their medical technologies. Machine learning and deep learning approaches are active research in developing intelligent and efficient diabetes detection systems. This study profoundly investigates and discusses the impacts of the latest machine learning and deep learning approaches in diabetes identification/classifications. It is observed that diabetes data are limited in availability. Available databases comprise lab-based and invasive test measurements. Investigating anthropometric measurements and non-invasive tests must be performed to create a cost-effective yet high-performance solution. Several findings showed the possibility of reconstructing the detection models based on anthropometric measurements and non-invasive medical indicators. This study investigated the consequences of oversampling techniques and data dimensionality reduction through feature selection approaches. The future direction is highlighted in the research of feature selection approaches to improve the accuracy and reliability of diabetes identifications. ","Several findings showed the possibility of reconstructing the detection models based on anthropometric measurements and non-invasive medical indicators, and the consequences of oversampling techniques and data dimensionality reduction through feature selection approaches."
"Machine learning in precision diabetes care and cardiovascular risk prediction","https://scispace.com/paper/machine-learning-in-precision-diabetes-care-and-1d2wex6ls2","2023","","Cardiovascular Diabetology","Evangelos K Oikonomou
Rohan Khera","10.1186/s12933-023-01985-3","","Abstract Artificial intelligence and machine learning are driving a paradigm shift in medicine, promising data-driven, personalized solutions for managing diabetes and the excess cardiovascular risk it poses. In this comprehensive review of machine learning applications in the care of patients with diabetes at increased cardiovascular risk, we offer a broad overview of various data-driven methods and how they may be leveraged in developing predictive models for personalized care. We review existing as well as expected artificial intelligence solutions in the context of diagnosis, prognostication, phenotyping, and treatment of diabetes and its cardiovascular complications. In addition to discussing the key properties of such models that enable their successful application in complex risk prediction, we define challenges that arise from their misuse and the role of methodological standards in overcoming these limitations. We also identify key issues in equity and bias mitigation in healthcare and discuss how the current regulatory framework should ensure the efficacy and safety of medical artificial intelligence products in transforming cardiovascular care and outcomes in diabetes. ","This comprehensive review of machine learning applications in the care of patients with diabetes at increased cardiovascular risk, offers a broad overview of various data-driven methods and how they may be leveraged in developing predictive models for personalized care."
"Artificial intelligence and diabetes technology: A review.","https://scispace.com/paper/artificial-intelligence-and-diabetes-technology-a-review-3bqhlym2uw","2021","Journal Article","Metabolism-clinical and Experimental","Thibault Gautier
Leah B. Ziegler
Matthew S. Gerber
Enrique Campos-Náñez
Stephen D. Patek","10.1016/J.METABOL.2021.154872","","Artificial intelligence (AI) is widely discussed in the popular literature and is portrayed as impacting many aspects of human life, both in and out of the workplace. The potential for revolutionizing healthcare is significant because of the availability of increasingly powerful computational platforms and methods, along with increasingly informative sources of patient data, both in and out of clinical settings. This review aims to provide a realistic assessment of the potential for AI in understanding and managing diabetes, accounting for the state of the art in the methodology and medical devices that collect data, process data, and act accordingly. Acknowledging that many conflicting definitions of AI have been put forth, this article attempts to characterize the main elements of the field as they relate to diabetes, identifying the main perspectives and methods that can (i) affect basic understanding of the disease, (ii) affect understanding of risk factors (genetic, clinical, and behavioral) of diabetes development, (iii) improve diagnosis, (iv) improve understanding of the arc of disease (progression and personal/societal impact), and finally (v) improve treatment.","A review of the potential for AI in understanding and managing diabetes, accounting for the state of the art in the methodology and medical devices that collect data, process data, and act accordingly, is presented in this article."
"Artificial Intelligence for Predicting and Diagnosing Complications of Diabetes","https://scispace.com/paper/artificial-intelligence-for-predicting-and-diagnosing-vefwl88i","2022","Journal Article","Journal of diabetes science and technology","Jingtong Huang
Andrea M. Yeung
David G. Armstrong
Ashley N. Battarbee
Jorge Cuadros
Juan C. Espinoza
Samantha Kleinberg
Nestoras Mathioudakis
Mark Swerdlow
David C. Klonoff","10.1177/19322968221124583","","Artificial intelligence can use real-world data to create models capable of making predictions and medical diagnosis for diabetes and its complications. The aim of this commentary article is to provide a general perspective and present recent advances on how artificial intelligence can be applied to improve the prediction and diagnosis of six significant complications of diabetes including (1) gestational diabetes, (2) hypoglycemia in the hospital, (3) diabetic retinopathy, (4) diabetic foot ulcers, (5) diabetic peripheral neuropathy, and (6) diabetic nephropathy.","A general perspective is provided and recent advances on how artificial intelligence can be applied to improve the prediction and diagnosis of six significant complications of diabetes including gestational diabetes, hypoglycemia in the hospital, diabetic retinopathy, diabetes foot ulcers, diabetic peripheral neuropathy, and diabetic nephropathy are presented."
"Cross-Modal Health State Estimation","https://scispace.com/paper/cross-modal-health-state-estimation-1a9y6dmvcb","2018","Proceedings Article","ACM Multimedia","Nitish Nag
Vaibhav Pandey
Preston J. Putzel
Hari Bhimaraju
Srikanth Krishnan
Ramesh Jain","10.1145/3240508.3241913","https://scispace.com/pdf/cross-modal-health-state-estimation-1a9y6dmvcb.pdf","Individuals create and consume more diverse data about themselves today than any time in history. Sources of this data include wearable devices, images, social media, geo-spatial information and more. A tremendous opportunity rests within cross-modal data analysis that leverages existing domain knowledge methods to understand and guide human health. Especially in chronic diseases, current medical practice uses a combination of sparse hospital based biological metrics (blood tests, expensive imaging, etc.) to understand the evolving health status of an individual. Future health systems must integrate data created at the individual level to better understand health status perpetually, especially in a cybernetic framework. In this work we fuse multiple user created and open source data streams along with established biomedical domain knowledge to give two types of quantitative state estimates of cardiovascular health. First, we use wearable devices to calculate cardiorespiratory fitness (CRF), a known quantitative leading predictor of heart disease which is not routinely collected in clinical settings. Second, we estimate inherent genetic traits, living environmental risks, circadian rhythm, and biological metrics from a diverse dataset. Our experimental results on 24 subjects demonstrate how multi-modal data can provide personalized health insight. Understanding the dynamic nature of health status will pave the way for better health based recommendation engines, better clinical decision making and positive lifestyle changes.","In this article, the authors use wearable devices to calculate cardiorespiratory fitness (CRF), a known quantitative leading predictor of heart disease which is not routinely collected in clinical settings."
"Multi-Modality Approaches for Medical Support Systems: A Systematic Review of the Last Decade","https://scispace.com/paper/multi-modality-approaches-for-medical-support-systems-a-1xlec3gofh","2023","Journal Article","Information Fusion","Massimo Salvi
Hui Wen Loh
Silvia Seoni
Prabal Datta Barua
Salvador García
F. Molinari
U. R. Acharya","10.1016/j.inffus.2023.102134","","Healthcare traditionally relies on single-modality approaches, which limit the information available for medical decisions. However, advancements in technology and the availability of diverse data sources have made it feasible to integrate multiple modalities and gain a more comprehensive understanding of patients' conditions. Multi-modality approaches involve fusing and analyzing various data types, including medical images, biosignals, clinical records, and other relevant sources. This systematic review provides a comprehensive exploration of the multi-modality approaches in healthcare, with a specific focus on disease diagnosis and prognosis. The adoption of multi-modality approaches in healthcare is crucial for personalized medicine, as it enables a comprehensive profile of each patient, considering their genetic makeup, imaging characteristics, clinical history, and other relevant factors. The review also discusses the technical challenges associated with fusing heterogeneous multimodal data and highlights the emergence of deep learning approaches as a powerful paradigm for multimodal data integration. ","Multi-modality approaches involve fusing and analyzing various data types, including medical images, bio-signals, clinical records, to gain a more comprehensive understanding of patients ’ conditions."
"Multi-modal Multi-instance Learning Using Weakly Correlated Histopathological Images and Tabular Clinical Information","https://scispace.com/paper/multi-modal-multi-instance-learning-using-weakly-correlated-4hw0abd7ye","2021","Book Chapter","Medical Image Computing and Computer-Assisted Intervention","Hang Li
Fan Yang
Xiaohan Xing
Xiaohan Xing
Yu Zhao
Jun Zhang
Yueping Liu
Mengxue Han
Junzhou Huang
Liansheng Wang
Jianhua Yao","10.1007/978-3-030-87237-3_51","","The fusion of heterogeneous medical data is essential in precision medicine to assist medical experts in treatment decision-making. However, there is often little explicit correlation between data from different modalities such as histopathological images and tabular clinical data. Besides, attention-based multi-instance learning (MIL) often lacks sufficient supervision to assign appropriate attention weights for informative image patches and thus generates a good global representation for the whole image. In this paper, we propose a novel multi-modal multi-instance joint learning method, which fuses different modalities and magnification scales as a cross-modal representation to capture the potential complementary information and recalibrate the features in each modality. Furthermore, we leverage the information from tabular clinical data to optimize the MIL bag representation in the imaging modality. The proposed method is evaluated on a challenging medical task, i.e., lymph node metastasis (LNM) prediction of breast cancer, and achieves the state-of-the-art performance with AUC of 0.8844, outperforming the AUC of 0.7111 using histopathological images or the AUC of 0.8312 using tabular clinical data alone. An open-source implementation of our approach can be found at https://github.com/yfzon/Multi-modal-Multi-instance-Learning.","Zhang et al. as mentioned in this paper proposed a multi-modal multi-instance joint learning method, which fuses different modalities and magnification scales as a crossmodal representation to capture the potential complementary information and recalibrate the features in each modality."
"DMC-Fusion: Deep Multi-Cascade Fusion With Classifier-Based Feature Synthesis for Medical Multi-Modal Images","https://scispace.com/paper/dmc-fusion-deep-multi-cascade-fusion-with-classifier-based-1un8nm8t29","2021","Journal Article","IEEE Journal of Biomedical and Health Informatics","Qing Zuo
Jianping Zhang
Yin Yang","10.1109/JBHI.2021.3083752","","Multi-modal medical image fusion is a challenging yet important task for precision diagnosis and surgical planning in clinical practice. Although single feature fusion strategy such as Densefuse has achieved inspiring performance, it tends to be not fully preserved for the source image features. In this paper, a deep multi-fusion framework with classifier-based feature synthesis is proposed to automatically fuse multi-modal medical images. It consists of a pre-trained autoencoder based on dense connections, a feature classifier and a multi-cascade fusion decoder with separately fusing high-frequency and low-frequency. The encoder and decoder are transferred from MS-COCO datasets and pre-trained simultaneously on multi-modal medical image public datasets to extract features. The feature classification is conducted through Gaussian high-pass filtering and the peak signal to noise ratio thresholding, then feature maps in each layer of the pre-trained Dense-Block and decoder are divided into high-frequency and low-frequency sequences. Specifically, in proposed feature fusion block, parameter-adaptive pulse coupled neural network and $\ell _1$ -weighted are employed to fuse high-frequency and low-frequency, respectively. Finally, we design a novel multi-cascade fusion decoder on total decoding feature stage to selectively fuse useful information from different modalities. We also validate our approach for the brain disease classification using the fused images, and a statistical significance test is performed to illustrate that the improvement in classification performance is due to the fusion. Experimental results demonstrate that the proposed method achieves the state-of-the-art performance in both qualitative and quantitative evaluations.","In this article, a deep multi-fusion framework with classifier-based feature synthesis is proposed to automatically fuse multi-modal medical images, which consists of a pre-trained autoencoder based on dense connections, a feature classifier and a multi-cascade fusion decoder with separately fusing high-frequency and low-frequency."
"Personalized Blood Glucose Prediction for Type 1 Diabetes Using Evidential Deep Learning and Meta-Learning","https://scispace.com/paper/personalized-blood-glucose-prediction-for-type-1-diabetes-1cvgsquy","2023","Journal Article","IEEE Transactions on Biomedical Engineering","","10.1109/tbme.2022.3187703","https://scispace.com/pdf/personalized-blood-glucose-prediction-for-type-1-diabetes-1cvgsquy.pdf","The availability of large amounts of data from continuous glucose monitoring (CGM), together with the latest advances in deep learning techniques, have opened the door to a new paradigm of algorithm design for personalized blood glucose (BG) prediction in type 1 diabetes (T1D) with superior performance. However, there are several challenges that prevent the widespread implementation of deep learning algorithms in actual clinical settings, including unclear prediction confidence and limited training data for new T1D subjects. To this end, we propose a novel deep learning framework, Fast-adaptive and Confident Neural Network (FCNN), to meet these clinical challenges. In particular, an attention-based recurrent neural network is used to learn representations from CGM input and forward a weighted sum of hidden states to an evidential output layer, aiming to compute personalized BG predictions with theoretically supported model confidence. The model-agnostic meta-learning is employed to enable fast adaptation for a new T1D subject with limited training data. The proposed framework has been validated on three clinical datasets. In particular, for a dataset including 12 subjects with T1D, FCNN achieved a root mean square error of 18.64±2.60 mg/dL and 31.07±3.62 mg/dL for 30 and 60-minute prediction horizons, respectively, which outperformed all the considered baseline methods with significant improvements. These results indicate that FCNN is a viable and effective approach for predicting BG levels in T1D. The well-trained models can be implemented in smartphone apps to improve glycemic control by enabling proactive actions through real-time glucose alerts. ","In this article , an attention-based recurrent neural network is used to learn representations from continuous glucose monitoring (CGM) input and forward a weighted sum of hidden states to an evidential output layer, aiming to compute personalized BG predictions with theoretically supported model confidence."
"DiabDeep: Pervasive Diabetes Diagnosis Based on Wearable Medical Sensors and Efficient Neural Networks","https://scispace.com/paper/diabdeep-pervasive-diabetes-diagnosis-based-on-wearable-2kyzt3gi29","2021","Journal Article","IEEE Transactions on Emerging Topics in Computing","Hongxu Yin
Bilal Mukadam
Xiaoliang Dai
Niraj K. Jha","10.1109/TETC.2019.2958946","http://arxiv.org/pdf/1910.04925.pdf","Diabetes impacts the quality of life of millions of people around the globe. However, diabetes diagnosis is still an arduous process, given that this disease develops and gets treated outside the clinic. The emergence of wearable medical sensors (WMSs) and machine learning points to a potential way forward to address this challenge. WMSs enable a continuous, yet user-transparent, mechanism to collect and analyze physiological signals. However, disease diagnosis based on WMS data and its effective deployment on resource-constrained edge devices remain challenging due to inefficient feature extraction and vast computation cost. To address these problems, we propose a framework called DiabDeep that combines efficient neural networks (called DiabNNs) with off-the-shelf WMSs for pervasive diabetes diagnosis. DiabDeep bypasses the feature extraction stage and acts directly on WMS data. It enables both an (i) accurate inference on the server, e.g., a desktop, and (ii) efficient inference on an edge device, e.g., a smartphone, to obtain a balance between accuracy and efficiency based on varying resource budgets and design goals. On the resource-rich server, we stack sparsely connected layers to deliver high accuracy. On the resource-scarce edge device, we use a hidden-layer long short-term memory based recurrent layer to substantially cut down on computation and storage costs while incurring only a minor accuracy loss. At the core of our system lies a grow-and-prune training flow: it leverages gradient-based growth and magnitude-based pruning algorithms to enable DiabNNs to learn both weights and connections, while improving accuracy and efficiency. We demonstrate the effectiveness of DiabDeep through a detailed analysis of data collected from 52 participants. For server (edge) side inference, we achieve a 96.3 percent (95.3 percent) accuracy in classifying diabetics against healthy individuals, and a 95.7 percent (94.6 percent) accuracy in distinguishing among type-1 diabetic, type-2 diabetic, and healthy individuals. Against conventional baselines, such as support vector machines with linear and radial basis function kernels, k-nearest neighbor, random forest, and linear ridge classifiers, DiabNNs achieve higher accuracy, while reducing the model size (floating-point operations) by up to 454.5× (8.9×). Therefore, the system can be viewed as pervasive and efficient, yet very accurate.","DiabDeep as mentioned in this paper combines efficient neural networks (called DiabNNs) with off-the-shelf wearable medical sensors (WMSs) for pervasive diabetes diagnosis, which enables both accurate inference on the server and efficient inference on an edge device to obtain a balance between accuracy and efficiency based on varying resource budgets and design goals."
"DiaNet: A Deep Learning Based Architecture to Diagnose Diabetes Using Retinal Images Only","https://scispace.com/paper/dianet-a-deep-learning-based-architecture-to-diagnose-55n4wko0h4","2021","Journal Article","IEEE Access","Mohammad Tariqul Islam
Hamada R. H. Al-Absi
Essam A. Ruagh
Tanvir Alam","10.1109/ACCESS.2021.3052477","https://scispace.com/pdf/dianet-a-deep-learning-based-architecture-to-diagnose-55n4wko0h4.pdf","Diabetes is one of the leading fatal diseases globally, putting a huge burden on the global healthcare system. Early diagnosis of diabetes is hence, of utmost importance and could save many lives. However, current techniques to determine whether a person has diabetes or has the risk of developing diabetes are primarily reliant upon clinical biomarkers. In this article, we propose a novel deep learning architecture to predict if a person has diabetes or not from a photograph of his/her retina. Using a relatively small-sized dataset, we develop a multi-stage convolutional neural network (CNN)-based model DiaNet that can reach an accuracy level of over 84% on this task, and in doing so, successfully identifies the regions on the retina images that contribute to its decision-making process, as corroborated by the medical experts in the field. This is the first study that highlights the distinguishing capability of the retinal images for diabetes patients in the Qatari population to the best of our knowledge. Comparing the performance of DiaNet against the existing clinical data-based machine learning models, we conclude that the retinal images contain sufficient information to distinguish the Qatari diabetes cohort from the control group. In addition, our study reveals that retinal images may contain prognosis markers for diabetes and other comorbidities like hypertension and ischemic heart disease. The results led us to believe that the inclusion of retinal images into the clinical setup for the diagnosis of diabetes is warranted in the near future.","In this article, a multi-stage convolutional neural network (CNN)-based model was proposed to predict whether a person has diabetes or not from a photograph of his/her retina."
"A Review on Methods and Applications in Multimodal Deep Learning","https://scispace.com/paper/a-review-on-methods-and-applications-in-multimodal-deep-2dfiuh9v","2022","Journal Article","ACM Transactions on Multimedia Computing, Communications, and Applications","Summaira Jabeen
Xi Li
M. Amin
Omar El Farouk Bourahla
Songyuan Li
Abdul Jabbar","10.1145/3545572","https://scispace.com/pdf/a-review-on-methods-and-applications-in-multimodal-deep-2dfiuh9v.pdf","Deep Learning has implemented a wide range of applications and has become increasingly popular in recent years. The goal of multimodal deep learning (MMDL) is to create models that can process and link information using various modalities. Despite the extensive development made for unimodal learning, it still cannot cover all the aspects of human learning. Multimodal learning helps to understand and analyze better when various senses are engaged in the processing of information. This article focuses on multiple types of modalities, i.e., image, video, text, audio, body gestures, facial expressions, physiological signals, flow, RGB, pose, depth, mesh, and point cloud. Detailed analysis of the baseline approaches and an in-depth study of recent advancements during the past five years (2017 to 2021) in multimodal deep learning applications has been provided. A fine-grained taxonomy of various multimodal deep learning methods is proposed, elaborating on different applications in more depth. Last, main issues are highlighted separately for each domain, along with their possible future research directions.","A fine-grained taxonomy of various multimodal deep learning methods is proposed, elaborating on different applications in more depth, and main issues are highlighted separately for each domain, along with their possible future research directions."
"Multimodal Classification: Current Landscape, Taxonomy and Future Directions","https://scispace.com/paper/multimodal-classification-current-landscape-taxonomy-and-37bqn7ta","2022","Journal Article","ACM Computing Surveys","","10.1145/3543848","","Multimodal classification research has been gaining popularity with new datasets in domains such as satellite imagery, biometrics, and medicine. Prior research has shown the benefits of combining data from multiple sources compared to traditional unimodal data that has led to the development of many novel multimodal architectures. However, the lack of consistent terminologies and architectural descriptions makes it difficult to compare different solutions. We address these challenges by proposing a new taxonomy for describing multimodal classification models based on trends found in recent publications. Examples of how this taxonomy could be applied to existing models are presented as well as a checklist to aid in the clear and complete presentation of future models. Many of the most difficult aspects of unimodal classification have not yet been fully addressed for multimodal datasets, including big data, class imbalance, and instance-level difficulty. We also provide a discussion of these challenges and future directions of research. ","A taxonomy for describing multimodal classification models based on trends found in recent publications is proposed in this paper , where examples of how this taxonomy could be applied to existing models are presented as well as a checklist to aid in the clear and complete presentation of future models."
"Artificial intelligence-based methods for fusion of electronic health records and imaging data","https://scispace.com/paper/artificial-intelligence-based-methods-for-fusion-of-xa4h9v14","2022","Journal Article","Dental science reports","Farida Mohsen
Hazrat Ali
Nady El Hajj
Zubair Shah","10.1038/s41598-022-22514-4","https://www.nature.com/articles/s41598-022-22514-4.pdf","Healthcare data are inherently multimodal, including electronic health records (EHR), medical images, and multi-omics data. Combining these multimodal data sources contributes to a better understanding of human health and provides optimal personalized healthcare. The most important question when using multimodal data is how to fuse them-a field of growing interest among researchers. Advances in artificial intelligence (AI) technologies, particularly machine learning (ML), enable the fusion of these different data modalities to provide multimodal insights. To this end, in this scoping review, we focus on synthesizing and analyzing the literature that uses AI techniques to fuse multimodal medical data for different clinical applications. More specifically, we focus on studies that only fused EHR with medical imaging data to develop various AI methods for clinical applications. We present a comprehensive analysis of the various fusion strategies, the diseases and clinical outcomes for which multimodal fusion was used, the ML algorithms used to perform multimodal fusion for each clinical application, and the available multimodal medical datasets. We followed the PRISMA-ScR (Preferred Reporting Items for Systematic Reviews and Meta-Analyses Extension for Scoping Reviews) guidelines. We searched Embase, PubMed, Scopus, and Google Scholar to retrieve relevant studies. After pre-processing and screening, we extracted data from 34 studies that fulfilled the inclusion criteria. We found that studies fusing imaging data with EHR are increasing and doubling from 2020 to 2021. In our analysis, a typical workflow was observed: feeding raw data, fusing different data modalities by applying conventional machine learning (ML) or deep learning (DL) algorithms, and finally, evaluating the multimodal fusion through clinical outcome predictions. Specifically, early fusion was the most used technique in most applications for multimodal learning (22 out of 34 studies). We found that multimodality fusion models outperformed traditional single-modality models for the same task. Disease diagnosis and prediction were the most common clinical outcomes (reported in 20 and 10 studies, respectively) from a clinical outcome perspective. Neurological disorders were the dominant category (16 studies). From an AI perspective, conventional ML models were the most used (19 studies), followed by DL models (16 studies). Multimodal data used in the included studies were mostly from private repositories (21 studies). Through this scoping review, we offer new insights for researchers interested in knowing the current state of knowledge within this research field. ","In this paper , the authors present a comprehensive analysis of the various fusion strategies, the diseases and clinical outcomes for which multimodal fusion was used, the ML algorithms used to perform multi-modal fusion for each clinical application, and the available multimmodal medical datasets."
"Deep multimodal fusion of image and non-image data in disease diagnosis and prognosis: a review","https://scispace.com/paper/deep-multimodal-fusion-of-image-and-non-image-data-in-21529rl8","2022","Journal Article","Progress in Biomedical Engineering","Can Cui
Haichun Yang
Yaohong Wang
Shilin Zhao
Zuhayr Asad
Lori A. Coburn
Keith T. Wilson
Bennett A. Landman
Yuankai Huo","10.1088/2516-1091/acc2fe","https://iopscience.iop.org/article/10.1088/2516-1091/acc2fe/pdf","The rapid development of diagnostic technologies in healthcare is leading to higher requirements for physicians to handle and integrate the heterogeneous, yet complementary data that are produced during routine practice. For instance, the personalized diagnosis and treatment planning for a single cancer patient relies on various images (e.g. radiology, pathology and camera images) and non-image data (e.g. clinical data and genomic data). However, such decision-making procedures can be subjective, qualitative, and have large inter-subject variabilities. With the recent advances in multimodal deep learning technologies, an increasingly large number of efforts have been devoted to a key question: how do we extract and aggregate multimodal information to ultimately provide more objective, quantitative computer-aided clinical decision making? This paper reviews the recent studies on dealing with such a question. Briefly, this review will include the (a) overview of current multimodal learning workflows, (b) summarization of multimodal fusion methods, (c) discussion of the performance, (d) applications in disease diagnosis and prognosis, and (e) challenges and future directions.","A review of multimodal deep learning workflows for clinical decision making can be found in this article , where the authors provide an overview of current multimodality workflows and a discussion of their performance, challenges and future directions."
"Use of Multi-Modal Data and Machine Learning to Improve Cardiovascular Disease Care","https://scispace.com/paper/use-of-multi-modal-data-and-machine-learning-to-improve-2fs7n2w1","2022","Journal Article","Frontiers in Cardiovascular Medicine","S Amal
Lida Safarnejad
Jesutofunmi A. Omiye
I. Ghanzouri
John Cabot
Elsie Gyang Ross","10.3389/fcvm.2022.840262","https://scispace.com/pdf/use-of-multi-modal-data-and-machine-learning-to-improve-2fs7n2w1.pdf","Today's digital health revolution aims to improve the efficiency of healthcare delivery and make care more personalized and timely. Sources of data for digital health tools include multiple modalities such as electronic medical records (EMR), radiology images, and genetic repositories, to name a few. While historically, these data were utilized in silos, new machine learning (ML) and deep learning (DL) technologies enable the integration of these data sources to produce multi-modal insights. Data fusion, which integrates data from multiple modalities using ML and DL techniques, has been of growing interest in its application to medicine. In this paper, we review the state-of-the-art research that focuses on how the latest techniques in data fusion are providing scientific and clinical insights specific to the field of cardiovascular medicine. With these new data fusion capabilities, clinicians and researchers alike will advance the diagnosis and treatment of cardiovascular diseases (CVD) to deliver more timely, accurate, and precise patient care.","The state-of-the-art research that focuses on how the latest techniques in data fusion are providing scientific and clinical insights specific to the field of cardiovascular medicine are reviewed."
"HGMF: Heterogeneous Graph-based Fusion for Multimodal Data with Incompleteness","https://scispace.com/paper/hgmf-heterogeneous-graph-based-fusion-for-multimodal-data-gnwgabdoew","2020","Proceedings Article","Knowledge Discovery and Data Mining","Jiayi Chen
Aidong Zhang","10.1145/3394486.3403182","https://dl.acm.org/doi/pdf/10.1145/3394486.3403182","With the advances in data collection techniques, large amounts of multimodal data collected from multiple sources are becoming available. Such multimodal data can provide complementary information that can reveal fundamental characteristics of real-world subjects. Thus, multimodal machine learning has become an active research area. Extensive works have been developed to exploit multimodal interactions and integrate multi-source information. However, multimodal data in the real world usually comes with missing modalities due to various reasons, such as sensor damage, data corruption, and human mistakes in recording. Effectively integrating and analyzing multimodal data with incompleteness remains a challenging problem. We propose a Heterogeneous Graph-based Multimodal Fusion (HGMF) approach to enable multimodal fusion of incomplete data within a heterogeneous graph structure. The proposed approach develops a unique strategy for learning on incomplete multimodal data without data deletion or data imputation. More specifically, we construct a heterogeneous hypernode graph to model the multimodal data having different combinations of missing modalities, and then we formulate a graph neural network based transductive learning framework to project the heterogeneous incomplete data onto a unified embedding space, and multi-modalities are fused along the way. The learning framework captures modality interactions from available data, and leverages the relationships between different incompleteness patterns. Our experimental results demonstrate that the proposed method outperforms existing graph-based as well as non-graph based baselines on three different datasets.","This work proposes a Heterogeneous Graph-based Multimodal Fusion (HGMF) approach to enable multimodal fusion of incomplete data within a heterogeneous graph structure that outperforms existing graph-based as well as non-graph based baselines on three different datasets."
"Detection and Prediction of Diabetes Using Data Mining: A Comprehensive Review","https://scispace.com/paper/detection-and-prediction-of-diabetes-using-data-mining-a-3r7s30w429","2021","Journal Article","IEEE Access","Farrukh Aslam Khan
Khan Zeb
Mabrook Al-Rakhami
Abdelouahid Derhab
Syed Ahmad Chan Bukhari","10.1109/ACCESS.2021.3059343","https://scispace.com/pdf/detection-and-prediction-of-diabetes-using-data-mining-a-3r7s30w429.pdf","Diabetes is one of the most rapidly growing chronic diseases, which has affected millions of people around the globe. Its diagnosis, prediction, proper cure, and management are crucial. Data mining based forecasting techniques for data analysis of diabetes can help in the early detection and prediction of the disease and the related critical events such as hypo/hyperglycemia. Numerous techniques have been developed in this domain for diabetes detection, prediction, and classification. In this paper, we present a comprehensive review of the state-of-the-art in the area of diabetes diagnosis and prediction using data mining. The aim of this paper is twofold; firstly, we explore and investigate the data mining based diagnosis and prediction solutions in the field of glycemic control for diabetes. Secondly, in the light of this investigation, we provide a comprehensive classification and comparison of the techniques that have been frequently used for diagnosis and prediction of diabetes based on important key metrics. Moreover, we highlight the challenges and future research directions in this area that can be considered in order to develop optimized solutions for diabetes detection and prediction.","A comprehensive review of the state-of-the-art in the area of diabetes diagnosis and prediction using data mining is presented in this article, where a comprehensive classification and comparison of the techniques that have been frequently used to diagnose and prediction of diabetes based on important key metrics is provided."
"A transformer-based representation-learning model with unified processing of multimodal input for clinical diagnostics","https://scispace.com/paper/a-transformer-based-representation-learning-model-with-vvolqfvl","2023","Journal Article","Nature Biomedical Engineering","Yizhou Yu
Kang Zhang
Aalyia Feroz Ali Sadruddin
Mehdi Rahmani-Andebili","10.1038/s41551-023-01045-x","https://scispace.com/pdf/a-transformer-based-representation-learning-model-with-vvolqfvl.pdf","During the diagnostic process, clinicians leverage multimodal information, such as the chief complaint, medical images and laboratory test results. Deep-learning models for aiding diagnosis have yet to meet this requirement of leveraging multimodal information. Here we report a transformer-based representation-learning model as a clinical diagnostic aid that processes multimodal input in a unified manner. Rather than learning modality-specific features, the model leverages embedding layers to convert images and unstructured and structured text into visual tokens and text tokens, and uses bidirectional blocks with intramodal and intermodal attention to learn holistic representations of radiographs, the unstructured chief complaint and clinical history, and structured clinical information such as laboratory test results and patient demographic information. The unified model outperformed an image-only model and non-unified multimodal diagnosis models in the identification of pulmonary disease (by 12% and 9%, respectively) and in the prediction of adverse clinical outcomes in patients with COVID-19 (by 29% and 7%, respectively). Unified multimodal transformer-based models may help streamline the triaging of patients and facilitate the clinical decision-making process. A transformer-based representation-learning model that processes multimodal input in a unified manner outperformed non-unified multimodal models in two clinical diagnostic tasks. ","In this paper , a transformer-based representation-learning model was proposed for clinical diagnostic aid that processes multimodal input in a unified manner, which leverages embedding layers to convert images and unstructured and structured text into visual tokens and text tokens, and uses bidirectional blocks with intramodal and intermodal attention."
"An overview of deep learning methods for multimodal medical data mining","https://scispace.com/paper/an-overview-of-deep-learning-methods-for-multimodal-medical-1uvvqh25","2022","Journal Article","Expert Systems With Applications","F. Behrad
Mohammad Saniee Abadeh","10.1016/j.eswa.2022.117006","","Deep learning methods have achieved significant results in various fields. Due to the success of these methods, many researchers have used deep learning algorithms in medical analyses. Using multimodal data to achieve more accurate results is a successful strategy because multimodal data provide complementary information. This paper first introduces the most popular modalities, fusion strategies, and deep learning architectures. We also explain learning strategies, including transfer learning, end-to-end learning, and multitask learning. Then, we give an overview of deep learning methods for multimodal medical data analysis. We have focused on articles published over the last four years. We end with a summary of the current state-of-the-art, common problems, and directions for future research. ","Deep learning methods have achieved significant results in various fields as discussed by the authors , and many researchers have used deep learning algorithms in medical analyses, using multimodal data to achieve more accurate results."
"Advanced Diabetes Management Using Artificial Intelligence and Continuous Glucose Monitoring Sensors.","https://scispace.com/paper/advanced-diabetes-management-using-artificial-intelligence-2stag99m7u","2020","Journal Article","Sensors","Martina Vettoretti
Giacomo Cappon
Andrea Facchinetti
Giovanni Sparacino","10.3390/S20143870","https://scispace.com/pdf/advanced-diabetes-management-using-artificial-intelligence-2stag99m7u.pdf","Wearable continuous glucose monitoring (CGM) sensors are revolutionizing the treatment of type 1 diabetes (T1D). These sensors provide in real-time, every 1–5 min, the current blood glucose concentration and its rate-of-change, two key pieces of information for improving the determination of exogenous insulin administration and the prediction of forthcoming adverse events, such as hypo-/hyper-glycemia. The current research in diabetes technology is putting considerable effort into developing decision support systems for patient use, which automatically analyze the patient’s data collected by CGM sensors and other portable devices, as well as providing personalized recommendations about therapy adjustments to patients. Due to the large amount of data collected by patients with T1D and their variety, artificial intelligence (AI) techniques are increasingly being adopted in these decision support systems. In this paper, we review the state-of-the-art methodologies using AI and CGM sensors for decision support in advanced T1D management, including techniques for personalized insulin bolus calculation, adaptive tuning of bolus calculator parameters and glucose prediction.","The state-of-the-art methodologies using AI and CGM sensors for decision support in advanced T1D management are reviewed, including techniques for personalized insulin bolus calculation, adaptive tuning of bolus calculator parameters and glucose prediction."
"Artificial Intelligence Methodologies and Their Application to Diabetes.","https://scispace.com/paper/artificial-intelligence-methodologies-and-their-application-3h7tspju4p","2018","Journal Article","Journal of diabetes science and technology","Mercedes Rigla
Gema García-Sáez
Belén Pons
María Hernando","10.1177/1932296817710475","https://journals.sagepub.com/doi/pdf/10.1177/1932296817710475","In the past decade diabetes management has been transformed by the addition of continuous glucose monitoring and insulin pump data More recently, a wide variety of functions and physiologic variables, such as heart rate, hours of sleep, number of steps walked and movement, have been available through wristbands or watches New data, hydration, geolocation, and barometric pressure, among others, will be incorporated in the future All these parameters, when analyzed, can be helpful for patients and doctors' decision support Similar new scenarios have appeared in most medical fields, in such a way that in recent years, there has been an increased interest in the development and application of the methods of artificial intelligence (AI) to decision support and knowledge acquisition Multidisciplinary research teams integrated by computer engineers and doctors are more and more frequent, mirroring the need of cooperation in this new topic AI, as a science, can be defined as the ability to make computers do things that would require intelligence if done by humans Increasingly, diabetes-related journals have been incorporating publications focused on AI tools applied to diabetes In summary, diabetes management scenarios have suffered a deep transformation that forces diabetologists to incorporate skills from new areas This recently needed knowledge includes AI tools, which have become part of the diabetes health care The aim of this article is to explain in an easy and plane way the most used AI methodologies to promote the implication of health care providers-doctors and nurses-in this field","The aim of this article is to explain in an easy and plane way the most used AI methodologies to promote the implication of health care providers—doctors and nurses—in this field."
"MATR: Multimodal Medical Image Fusion via Multiscale Adaptive Transformer","https://scispace.com/paper/matr-multimodal-medical-image-fusion-via-multiscale-adaptive-3qnl3pye","2022","Journal Article","IEEE Transactions on Image Processing","Wei Tang
Fazhi He
Yu Liu
Yansong Duan","10.1109/TIP.2022.3193288","","Owing to the limitations of imaging sensors, it is challenging to obtain a medical image that simultaneously contains functional metabolic information and structural tissue details. Multimodal medical image fusion, an effective way to merge the complementary information in different modalities, has become a significant technique to facilitate clinical diagnosis and surgical navigation. With powerful feature representation ability, deep learning (DL)-based methods have improved such fusion results but still have not achieved satisfactory performance. Specifically, existing DL-based methods generally depend on convolutional operations, which can well extract local patterns but have limited capability in preserving global context information. To compensate for this defect and achieve accurate fusion, we propose a novel unsupervised method to fuse multimodal medical images via a multiscale adaptive Transformer termed MATR. In the proposed method, instead of directly employing vanilla convolution, we introduce an adaptive convolution for adaptively modulating the convolutional kernel based on the global complementary context. To further model long-range dependencies, an adaptive Transformer is employed to enhance the global semantic extraction capability. Our network architecture is designed in a multiscale fashion so that useful multimodal information can be adequately acquired from the perspective of different scales. Moreover, an objective function composed of a structural loss and a region mutual information loss is devised to construct constraints for information preservation at both the structural-level and the feature-level. Extensive experiments on a mainstream database demonstrate that the proposed method outperforms other representative and state-of-the-art methods in terms of both visual quality and quantitative evaluation. We also extend the proposed method to address other biomedical image fusion issues, and the pleasing fusion results illustrate that MATR has good generalization capability. The code of the proposed method is available at https://github.com/tthinking/MATR.","This work proposes a novel unsupervised method to fuse multimodal medical images via a multiscale adaptive Transformer termed MATR, and demonstrates that the proposed method outperforms other representative and state-of-the-art methods in terms of both visual quality and quantitative evaluation."
"Intelligent Machine Learning Approach for Effective Recognition of Diabetes in E-Healthcare Using Clinical Data","https://scispace.com/paper/intelligent-machine-learning-approach-for-effective-enmxbvs31o","2020","Journal Article","Sensors","Amin Ul Haq
Jianping Li
Jalaluddin Khan
Muhammad Hammad Memon
Shah Nazir
Sultan Ahmad
Ghufran Ahmad Khan
Amjad Ali","10.3390/S20092649","https://scispace.com/pdf/intelligent-machine-learning-approach-for-effective-enmxbvs31o.pdf","Significant attention has been paid to the accurate detection of diabetes. It is a big challenge for the research community to develop a diagnosis system to detect diabetes in a successful way in the e-healthcare environment. Machine learning techniques have an emerging role in healthcare services by delivering a system to analyze the medical data for diagnosis of diseases. The existing diagnosis systems have some drawbacks, such as high computation time, and low prediction accuracy. To handle these issues, we have proposed a diagnosis system using machine learning methods for the detection of diabetes. The proposed method has been tested on the diabetes data set which is a clinical dataset designed from patient's clinical history. Further, model validation methods, such as hold out, K-fold, leave one subject out and performance evaluation metrics, includes accuracy, specificity, sensitivity, F1-score, receiver operating characteristic curve, and execution time have been used to check the validity of the proposed system. We have proposed a filter method based on the Decision Tree (Iterative Dichotomiser 3) algorithm for highly important feature selection. Two ensemble learning algorithms, Ada Boost and Random Forest, are also used for feature selection and we also compared the classifier performance with wrapper based feature selection algorithms. Classifier Decision Tree has been used for the classification of healthy and diabetic subjects. The experimental results show that the proposed feature selection algorithm selected features improve the classification performance of the predictive model and achieved optimal accuracy. Additionally, the proposed system performance is high compared to the previous state-of-the-art methods. High performance of the proposed method is due to the different combinations of selected features set and Plasma glucose concentrations, Diabetes pedigree function, and Blood mass index are more significantly important features in the dataset for prediction of diabetes. Furthermore, the experimental results statistical analysis demonstrated that the proposed method would effectively detect diabetes and can be deployed in an e-healthcare environment.","The experimental results show that the proposed feature selection algorithm selected features improve the classification performance of the predictive model and achieved optimal accuracy, and the proposed system performance is high compared to the previous state-of-the-art methods."
"A survey on multimodal data-driven smart healthcare systems: approaches and applications","https://scispace.com/paper/a-survey-on-multimodal-data-driven-smart-healthcare-systems-n1f6ngn55z","2019","Journal Article","IEEE Access","Qiong Cai
Hao Wang
Zhenmin Li
Xiao Liu","10.1109/ACCESS.2019.2941419","https://scispace.com/pdf/a-survey-on-multimodal-data-driven-smart-healthcare-systems-n1f6ngn55z.pdf","Multimodal data-driven approach has emerged as an important driving force for smart healthcare systems with applications ranging from disease analysis to triage, diagnosis and treatment. Smart healthcare system necessitates new demands for data management and decision-making, which has inspired the rapid development of medical services using artificial intelligence and new transformations in the healthcare industry. In this paper, we provide a comprehensive survey of existing techniques which include not only state-of-the-art methods but also the most recent trends in the field. In particular, this review focuses on the types of decision-making processes used in smart healthcare systems. Firstly, approaches that utilize multimodal association mining with fine-grained data semantics in smart healthcare systems are introduced. We review the smart healthcare-oriented semantic perception, semantic alignment, entity association mining, and discuss the pros and cons of these approaches. Secondly, we discuss approaches for multimodal data fusion and cross-border association that have been employed in developing smart healthcare systems. Finally, we focus specifically on the use of the panoramic decision framework, interactive decision making, and intelligent decision support systems. We introduce how smart healthcare systems can be applied to and benefit a wide variety of fields, including knowledge discovery and privacy protection.","A comprehensive survey of existing techniques which include not only state-of-the-art methods but also the most recent trends in the field on the types of decision-making processes used in smart healthcare systems is provided."
"MATR: Multimodal Medical Image Fusion via Multiscale Adaptive Transformer","https://scispace.com/paper/matr-multimodal-medical-image-fusion-via-multiscale-adaptive-100lxm43","2022","Journal Article","IEEE Transactions on Image Processing","","10.1109/tip.2022.3193288","","Owing to the limitations of imaging sensors, it is challenging to obtain a medical image that simultaneously contains functional metabolic information and structural tissue details. Multimodal medical image fusion, an effective way to merge the complementary information in different modalities, has become a significant technique to facilitate clinical diagnosis and surgical navigation. With powerful feature representation ability, deep learning (DL)-based methods have improved such fusion results but still have not achieved satisfactory performance. Specifically, existing DL-based methods generally depend on convolutional operations, which can well extract local patterns but have limited capability in preserving global context information. To compensate for this defect and achieve accurate fusion, we propose a novel unsupervised method to fuse multimodal medical images via a multiscale adaptive Transformer termed MATR. In the proposed method, instead of directly employing vanilla convolution, we introduce an adaptive convolution for adaptively modulating the convolutional kernel based on the global complementary context. To further model long-range dependencies, an adaptive Transformer is employed to enhance the global semantic extraction capability. Our network architecture is designed in a multiscale fashion so that useful multimodal information can be adequately acquired from the perspective of different scales. Moreover, an objective function composed of a structural loss and a region mutual information loss is devised to construct constraints for information preservation at both the structural-level and the feature-level. Extensive experiments on a mainstream database demonstrate that the proposed method outperforms other representative and state-of-the-art methods in terms of both visual quality and quantitative evaluation. We also extend the proposed method to address other biomedical image fusion issues, and the pleasing fusion results illustrate that MATR has good generalization capability. The code of the proposed method is available at https://github.com/tthinking/MATR. ","Zhang et al. as discussed by the authors proposed an adaptive convolution for adaptively modulating the convolutional kernel based on the global complementary context, which can enhance the global semantic extraction capability."
"Multimodal deep learning for biomedical data fusion: a review","https://scispace.com/paper/multimodal-deep-learning-for-biomedical-data-fusion-a-review-gbf5elmr","2022","Journal Article","Briefings in Bioinformatics","Sören Richard Stahlschmidt
Benjamin Ulfenborg
Jane Synnergren","10.1093/bib/bbab569","https://scispace.com/pdf/multimodal-deep-learning-for-biomedical-data-fusion-a-review-gbf5elmr.pdf","Abstract Biomedical data are becoming increasingly multimodal and thereby capture the underlying complex relationships among biological processes. Deep learning (DL)-based data fusion strategies are a popular approach for modeling these nonlinear relationships. Therefore, we review the current state-of-the-art of such methods and propose a detailed taxonomy that facilitates more informed choices of fusion strategies for biomedical applications, as well as research on novel methods. By doing so, we find that deep fusion strategies often outperform unimodal and shallow approaches. Additionally, the proposed subcategories of fusion strategies show different advantages and drawbacks. The review of current methods has shown that, especially for intermediate fusion strategies, joint representation learning is the preferred approach as it effectively models the complex interactions of different levels of biological organization. Finally, we note that gradual fusion, based on prior biological knowledge or on search strategies, is a promising future research path. Similarly, utilizing transfer learning might overcome sample size limitations of multimodal data sets. As these data sets become increasingly available, multimodal DL approaches present the opportunity to train holistic models that can learn the complex regulatory dynamics behind health and disease.","A detailed taxonomy that facilitates more informed choices of fusion strategies for biomedical applications, as well as research on novel methods is proposed that finds that deep fusion strategies often outperform unimodal and shallow approaches."
"Artificial Intelligence for Diabetes Management and Decision Support: Literature Review","https://scispace.com/paper/artificial-intelligence-for-diabetes-management-and-decision-qzn2ktfld1","2018","Journal Article","Journal of Medical Internet Research","Ivan Contreras
Josep Vehí","10.2196/10775","https://scispace.com/pdf/artificial-intelligence-for-diabetes-management-and-decision-qzn2ktfld1.pdf","Background: Artificial intelligence methods in combination with the latest technologies, including medical devices, mobile computing, and sensor technologies, have the potential to enable the creation and delivery of better management services to deal with chronic diseases. One of the most lethal and prevalent chronic diseases is diabetes mellitus, which is characterized by dysfunction of glucose homeostasis. Objective: The objective of this paper is to review recent efforts to use artificial intelligence techniques to assist in the management of diabetes, along with the associated challenges. Methods: A review of the literature was conducted using PubMed and related bibliographic resources. Analyses of the literature from 2010 to 2018 yielded 1849 pertinent articles, of which we selected 141 for detailed review. Results: We propose a functional taxonomy for diabetes management and artificial intelligence. Additionally, a detailed analysis of each subject category was performed using related key outcomes. This approach revealed that the experiments and studies reviewed yielded encouraging results. Conclusions: We obtained evidence of an acceleration of research activity aimed at developing artificial intelligence-powered tools for prediction and prevention of complications associated with diabetes. Our results indicate that artificial intelligence methods are being progressively established as suitable for use in clinical daily practice, as well as for the self-management of diabetes. Consequently, these methods provide powerful tools for improving patients’ quality of life.","Evidence of an acceleration of research activity aimed at developing artificial intelligence-powered tools for prediction and prevention of complications associated with diabetes is obtained, indicating that artificial intelligence methods are being progressively established as suitable for use in clinical daily practice, as well as for the self-management of diabetes."
"Multimodal biomedical AI","https://scispace.com/paper/multimodal-biomedical-ai-1l2w2dny","2022","Journal Article","Nature Medicine","Julian N Acosta
Guido J. Falcone
Pranav Rajpurkar
Eric J. Topol","10.1038/s41591-022-01981-2","https://scispace.com/pdf/multimodal-biomedical-ai-1l2w2dny.pdf","The increasing availability of biomedical data from large biobanks, electronic health records, medical imaging, wearable and ambient biosensors, and the lower cost of genome and microbiome sequencing have set the stage for the development of multimodal artificial intelligence solutions that capture the complexity of human health and disease. In this Review, we outline the key applications enabled, along with the technical and analytical challenges. We explore opportunities in personalized medicine, digital clinical trials, remote monitoring and care, pandemic surveillance, digital twin technology and virtual health assistants. Further, we survey the data, modeling and privacy challenges that must be overcome to realize the full potential of multimodal artificial intelligence in health. ","In this article , the authors outline the key applications enabled by multimodal artificial intelligence, along with the technical and analytical challenges, and survey the data, modeling and privacy challenges that must be overcome to realize the full potential of multimodAL artificial intelligence in health."
"MARIA: A multimodal transformer model for incomplete healthcare data","https://scispace.com/paper/maria-a-multimodal-transformer-model-for-incomplete-en0raf63vixc","2025","Journal Article","Computers in Biology and Medicine","Camillo Maria Caruso
Paolo Soda
Valerio Guarrasi","10.1016/j.compbiomed.2025.110843","","In healthcare, the integration of multimodal data is pivotal for developing comprehensive diagnostic and predictive models. However, managing missing data remains a significant challenge in real-world applications. We introduce MARIA (Multimodal Attention Resilient to Incomplete datA), a novel transformer-based deep learning model designed to address these challenges through an intermediate fusion strategy. Unlike conventional approaches that depend on imputation, MARIA utilizes a modified masked self-attention mechanism, which processes only the available data without generating synthetic values. This approach enables it to effectively handle incomplete datasets, enhancing robustness and minimizing biases introduced by imputation methods. We evaluated MARIA against 10 state-of-the-art machine learning and deep learning models across 8 diagnostic and prognostic tasks. The results demonstrate that MARIA outperforms existing methods in terms of performance and resilience to varying levels of data incompleteness, underscoring its potential for critical healthcare applications. To support transparency and encourage further research, the source code is openly available at https://github.com/cosbidev/MARIA. ","MARIA, a transformer-based model, addresses incomplete healthcare data challenges through a modified masked self-attention mechanism, outperforming 10 state-of-the-art models across 8 diagnostic and prognostic tasks with enhanced robustness and minimal bias."
"<scp>GlucoNet</scp>‐<scp>MM</scp>: A multimodal attention‐based multi‐task learning framework with decision transformer for personalised and explainable blood glucose forecasting","https://scispace.com/paper/scp-gluconet-scp-scp-mm-scp-a-multimodal-attention-based-io3ap30t1q9h","2025","Journal Article","Diabetes, Obesity and Metabolism","Sarmad Maqsood
Muhammad Abdullah Sarwar
Eglė Belousovienė
Rytis Maskeliūnas","10.1111/dom.70147","","Abstract Aims Accurate and personalized blood glucose prediction is critical for proactive diabetes management. Conventional machine learning (ML) models often struggle to generalize across patients due to individual variability, nonlinear glycemic dynamics, and sparse multimodal input data. This study aims to develop an advanced, interpretable deep learning (DL) framework for patient‐specific, policy‐aware blood glucose forecasting. Materials and Methods We propose GlucoNet‐MM, a novel multimodal DL framework that combines attention‐based multi‐task learning (MTL) with a Decision Transformer (DT), a reinforcement learning paradigm that frames policy learning as sequence modeling. The model integrates heterogeneous physiological and behavioral data, continuous glucose monitoring (CGM), insulin dosage, carbohydrate intake, and physical activity, to capture complex temporal dependencies. The MTL backbone learns shared representations across multiple prediction horizons, while the DT module conditions future glucose predictions on desired glycemic outcomes. Temporal attention visualizations and integrated gradient‐based attribution methods are used to provide interpretability, and Monte Carlo dropout is employed for uncertainty quantification. Results GlucoNet‐MM was evaluated on two publicly available datasets, BrisT1D and OhioT1DM. The model achieved R 2 scores of 0.94 and 0.96 and mean absolute error (MAE) values of 0.031 and 0.027, respectively. These results outperform single‐modality and conventional non‐adaptive baseline models, demonstrating superior predictive accuracy and generalizability. Conclusion GlucoNet‐MM represents a promising step toward intelligent, personalized clinical decision support for diabetes care. Its multimodal design, policy‐aware forecasting, and interpretability features enhance both prediction accuracy and clinical trust, enabling proactive glycemic management tailored to individual patient needs. ","This study presents GlucoNet-MM, a multimodal deep learning framework that integrates heterogeneous data for personalized blood glucose forecasting, achieving superior predictive accuracy and generalizability on two public datasets with R² scores of 0.94-0.96 and MAE of 0.027-0.031."
"Fused-AETNet: A Variational Transformer-Based Framework for Diabetic Retinopathy Classification Using OCT Biomarkers","https://scispace.com/paper/fused-aetnet-a-variational-transformer-based-framework-for-o6czukr266n0","2025","Journal Article","IEEE Access","Mohamed Elsharkawy
Ibrahim Abdelhalim
Ali Mahmoud
Ahmed Gamal
Mohamed AbdelHady
Ashraf Sewelam
Ayman El‐Baz","10.1109/access.2025.3609175","","Early detection and accurate diagnosis of Diabetic Retinopathy (DR) using Optical Coherence Tomography (OCT) images remain challenging due to subtle pathological alterations across multiple retinal layers. In this paper, we present Fused-AETNet (Fused AutoEncoder–Transformer Network), a novel deep-learning framework that uniquely combines clinically interpretable handcrafted OCT biomarkers with a robust variational autoencoder and a transformer-based classifier. Unlike prior OCT–DR approaches that directly utilize raw intensities or generic CNN-derived features, our method begins with precise segmentation of eleven retinal layers in the central OCT B-scan using an atlas-guided probabilistic approach. From each layer, we extract three clinically validated morphological descriptors—reflectivity, Laplacian-based thickness, and boundary tortuosity—and summarize them through percentile-based Cumulative Distribution Functions (CDFs) to capture regional variability in a compact, size-invariant form. These descriptors are fused into a smooth latent embedding via variational autoencoding, enabling robust modelling of cross-layer dependencies and uncertainty. The latent space is then refined using a lightweight transformer-inspired attention module that selectively amplifies diagnostically relevant embeddings while suppressing irrelevant variations. This biomarker-driven design enhances both the specificity and interpretability of the diagnostic decision process. Our training employs a composite loss function that jointly enforces accurate descriptor reconstruction, latent space regularization, and high classification accuracy. We validated the framework on OCT scans from 481 subjects, achieving superior performance compared to traditional machine learning classifiers and state-of-the-art deep networks, with an average accuracy of <inline-formula> <tex-math notation=""LaTeX"">$93.08\% \pm 4.21$ </tex-math></inline-formula>, precision of <inline-formula> <tex-math notation=""LaTeX"">$93.33\% \pm 4.84$ </tex-math></inline-formula>, recall of <inline-formula> <tex-math notation=""LaTeX"">$96.00\% \pm 5.96$ </tex-math></inline-formula>, and F1 score of <inline-formula> <tex-math notation=""LaTeX"">$94.48\% \pm 3.21$ </tex-math></inline-formula>. By explicitly modelling layer-wise biomarkers and integrating attention-guided latent feature selection, Fused-AETNet offers a compact, interpretable, and clinically relevant solution for OCT-based DR diagnosis.","This paper presents Fused-AETNet, a variational transformer-based framework for Diabetic Retinopathy classification using OCT biomarkers, achieving superior performance with 93.08% accuracy, precision, and recall, and F1 score of 94.48% on 481 subjects."
"Large language multimodal models for new-onset type 2 diabetes prediction using five-year cohort electronic health records","https://scispace.com/paper/large-language-multimodal-models-for-new-onset-type-2-4lppw2ogafpn","2024","Journal Article","Dental science reports","Jun-En Ding
Phan Nguyen Minh Thao
Wen-Chih Peng
Jian-Zhe Wang
Chun-Cheng Chug
Min-Chen Hsieh
Yun-Chien Tseng
Ling Chen
Dongsheng Luo
Chenwei Wu
Chi‐Te Wang
Chih-Ho Hsu
Yi-Tui Chen
Pei-Fu Chen
Feng Liu
Fang‐Ming Hung","10.1038/s41598-024-71020-2","","Type 2 diabetes mellitus (T2DM) is a prevalent health challenge faced by countries worldwide. In this study, we propose a novel large language multimodal models (LLMMs) framework incorporating multimodal data from clinical notes and laboratory results for diabetes risk prediction. We collected five years of electronic health records (EHRs) dating from 2017 to 2021 from a Taiwan hospital database. This dataset included 1,420,596 clinical notes, 387,392 laboratory results, and more than 1505 laboratory test items. Our method combined a text embedding encoder and multi-head attention layer to learn laboratory values, and utilized a deep neural network (DNN) module to merge blood features with chronic disease semantics into a latent space. In our experiments, we observed that integrating clinical notes with predictions based on textual laboratory values significantly enhanced the predictive capability of the unimodal model in the early detection of T2DM. Moreover, we achieved an area greater than 0.70 under the receiver operating characteristic curve (AUC) for new-onset T2DM prediction, demonstrating the effectiveness of leveraging textual laboratory data for training and inference in LLMs and improving the accuracy of new-onset diabetes prediction. ","This study proposes a novel large language multimodal model (LLMM) framework using electronic health records to predict new-onset type 2 diabetes, achieving an AUC of 0.70 with improved predictive capability through integrating clinical notes and laboratory values."
"Machine Learning to Diagnose Complications of Diabetes","https://scispace.com/paper/machine-learning-to-diagnose-complications-of-diabetes-mrjdyramuiio","2025","Journal Article","Journal of diabetes science and technology","Agatha F. Scheideman
M. Shao
Henry Zelada
Jorge Cuadros
Joshua Foreman
Pinaki Sarder
Cindy Ho
Niels Ejskjær
Jesper Fleischer
Simon Lebech Cichosz
David G. Armstrong
Nestoras Mathioudakis
Tao Wang
Yih‐Chung Tham
David C. Klonoff","10.1177/19322968251365245","","Machine learning (ML) uses computer systems to develop statistical algorithms and statistical models that can draw inferences from demographic data, structured behavioral data, continuous glucose monitor (CGM) tracings, laboratory data, cardiovascular and neurological physiology measurements, and images from a variety of sources. ML is becoming increasingly used to diagnose complications of diabetes based on these types of datasets. In this article, we review the current status, barriers to progress, and future prospects for using ML to diagnose seven complications of diabetes, including five traditional complications, one set of other systemic complications, and one prediction that can result in favorable or unfavorable outcomes. The complications include (1) diabetic retinopathy, (2) diabetic nephropathy, (3) peripheral neuropathy, (4) autonomic neuropathy, (5) diabetic foot ulcers, and (6) other systemic complications. The prediction is for outcomes in hospitalized patients with diabetes. ML for these purposes is in its infancy, as evidenced by only a limited number of products having received regulatory clearance at this time. However, as multicenter reference datasets become available, it will become possible to train algorithms on increasingly larger and more complex datasets and patterns so that diagnoses and predictions will become increasingly accurate. The use of novel choices of images and imaging technologies will contribute to progress in this field. ML is poised to become a widely used tool for the diagnosis of complications and predictions of outcomes and glycemia in people with diabetes. ","This study reviews the application of machine learning (ML) in diagnosing diabetes complications, including retinopathy, nephropathy, neuropathy, and foot ulcers, and predicting outcomes in hospitalized patients, highlighting current status, barriers, and future prospects for ML in diabetes care."
"Multi-datasets transfer multitask learning for simultaneous blood glucose and blood pressure monitoring using common PPG features","https://scispace.com/paper/multi-datasets-transfer-multitask-learning-for-simultaneous-85h09t3qx4u4","2025","Journal Article","Computers in Biology and Medicine","Noor Faris Ali
Ibrahim M. Elfadel
Mohamed Atef","10.1016/j.compbiomed.2025.110434","","The simultaneous monitoring of both blood glucose level (BGL) and blood pressure (BP) has rarely been studied directly. The exploitation of physiological interactions between them will advance the learning of either task. However, the lack of available datasets with labels of both targets presents an obstacle. Therefore, in this paper, we propose three methods for multi-dataset (MD) learning. First, we extract PPG features from three datasets: a source dataset comprising diabetes mellitus (DM) and hypertension (HTN) classes labels and two target datasets for each BP and BGL. Subsequently, we select a common merged feature set for all datasets tasks. This study experiments with the three proposed multi-dataset methods: 1) transfer learning (MD-TL) to transfer knowledge from a source task-DM and HTN single-task classifier, to a target task, BGL and BP single-task regression, respectively, 2) multitask learning (MD-MTL) of both targets via a shared two-way TL, and 3) combined TL with MTL (MD-TL-MTL) to transfer knowledge from a source multitasking classifier to the target tasks in the MD-MTL network. The final proposed MD-TL-MTL achieves an MAE±SD of 2.53 ± 3.73 for SBP, 1.47 ± 1.84 for DBP, and MAE of 1.36 for BGL. Clarke error grid analysis shows that 99.86 % of samples fall into zone A. Overall, the MD-TL-MTL improves performance in all tasks compared to baseline models. An interpretability analysis using Shapley Additive Explanations (SHAP) and permutation importance is conducted to facilitate the clinical understanding behind predictions. ","This study proposes three multi-dataset transfer learning methods for simultaneous blood glucose and blood pressure monitoring using PPG features, achieving improved performance with a mean absolute error of 1.36 for BGL and 2.53 ± 3.73 for SBP."
"Towards an Interpretable Continuous Glucose Monitoring Data Modeling","https://scispace.com/paper/towards-an-interpretable-continuous-glucose-monitoring-data-1xgarewth7","2024","Journal Article","IEEE Internet of Things Journal","Juan Guerrero
José Luis López Ruiz
Macarena Espinilla
Carmen Martínez-Cruz","10.1109/jiot.2024.3419260","","The ongoing global health challenge posed by diabetes necessitates a critical understanding of all generated data streamed from sensors. To address this, our study presents a robust fuzzy-logic-based descriptive analysis of glucose sensor data. This analysis is embedded within the context of an innovative architecture designed to support multipatient monitoring, with the goal of assisting healthcare professionals in their daily tasks and providing essential decision-making tools. Our novel approach captures and interprets complex data patterns from glucose sensors, and also introduces the capability of creating high-quality linguistic summaries, to highlight the most relevant phenomena through the use of natural language (NL). These descriptions facilitate clear communication between healthcare professionals and people with diabetes, enhancing a deeper understanding of intricate data patterns and promoting collaboration in diabetes care. A comparative evaluation between our proposal and the one obtained using GPT-4 underscores the sustainability, effectiveness, and efficiency of our methodology, positioning it as a new standard for empowering diabetic patients in terms of care and prevention, contributing to their progress and well-being.","This study presents a fuzzy-logic-based analysis of continuous glucose monitoring data, embedded in a multipatient monitoring architecture, to provide interpretable data patterns and linguistic summaries, enhancing collaboration and decision-making in diabetes care."
"A Longitudinal Multimodal Dataset of Type 1 Diabetes","https://scispace.com/paper/a-longitudinal-multimodal-dataset-of-type-1-diabetes-106cpb1rqvxn","2025","Journal Article","Scientific Data","Ashwaq Alsuhaymi
Ahmad Bilal
Daniel Gasca García
Rujiravee Kongdee
Nicole Lubasinski
Hood Thabit
Paul W. Nutter
Simon Harper","10.1038/s41597-025-05695-1","","People living with Type 1 Diabetes (PwT1D) must continuously monitor blood glucose levels and make critical clinical and safety-related decisions multiple times a day to maintain glycaemic control within recommended ranges. While significant efforts have been made to develop algorithms that assist PwT1D in managing blood glucose more effectively, access to automated insulin delivery (AID) systems remains highly variable across the world. Moreover, there is a lack of publicly available, comprehensive datasets necessary for developing algorithms to support scenarios where AID systems revert to manual mode. This study addresses this gap by providing a detailed, multimodal dataset encompassing five key aspects: blood glucose levels; basal and bolus insulin dosages; nutritional intake (carbohydrates, protein, fat, and fibre content); physical activity (step count, active calories, distance covered, MET, and intensity level); and sleep patterns. The dataset includes longitudinal (3-month) real-world data collected from 17 PwT1D participants. By making this resource available, the study aims to advance algorithm development and improve diabetes management, particularly in settings where AID technology is less accessible. ","This study presents a 3-month longitudinal multimodal dataset of 17 Type 1 Diabetes patients, including blood glucose levels, insulin dosages, nutritional intake, physical activity, and sleep patterns, to advance algorithm development and improve diabetes management worldwide."
"Personalized blood glucose prediction in type 1 diabetes using meta-learning with bidirectional long short term memory-transformer hybrid model","https://scispace.com/paper/personalized-blood-glucose-prediction-in-type-1-diabetes-4oo7crgdwrw2","2025","Journal Article","Dental science reports","Kyungsun Moon
Jaehong Kim
Seohyun Yoo
Jaehyuk Cho","10.1038/s41598-025-13491-5","","Personalized blood glucose (BG) prediction in Type 1 Diabetes (T1D) is challenged by significant inter-patient heterogeneity. To address this, we propose BiT-MAML, a hybrid model combining a Bidirectional LSTM-Transformer with Model-Agnostic Meta-Learning. We evaluated our model using a rigorous Leave-One-Patient-Out Cross-Validation (LOPO-CV) on the OhioT1DM dataset, ensuring a fair comparison against re-implemented LSTM and Edge-LSTM baselines. The results show our model achieved a mean RMSE of 24.89 mg/dL for the 30 min prediction horizon, marking a substantial improvement of 19.3% over the standard LSTM and 14.2% over the Edge-LSTM. Notably, our model also achieved the lowest standard deviation (±4.60 mg/dL), indicating more consistent and generalizable performance across the patient cohort. A key finding of our study is the confirmation of significant performance variability across individuals, a known clinical challenge. This was evident as our model's 30 min RMSE ranged from an excellent 19.64 mg/dL to a more challenging 30.57 mg/dL, reflecting the inherent difficulty of personalizing predictions rather than model instability. From a clinical safety perspective, Clarke Error Grid Analysis confirmed the model's robustness, with over 92% of predictions falling within the clinically acceptable Zones A and B. This study concludes that the development of effective personalized BG prediction requires not only advanced model architectures but also robust evaluation methods that transparently report the full spectrum of performance, providing a realistic pathway toward reliable clinical tools. ","This study proposes BiT-MAML, a hybrid model combining LSTM and Transformer with Model-Agnostic Meta-Learning, achieving a 19.3% improvement in blood glucose prediction for Type 1 Diabetes patients, with consistent and generalizable performance across the patient cohort."
"Integrating Innovation in Healthcare: The Evolution of ""CURA’s"" AI-Driven Virtual Wards for Enhanced Diabetes and Kidney Disease Monitoring","https://scispace.com/paper/integrating-innovation-in-healthcare-the-evolution-of-curas-6qpto8v0464z","2024","Journal Article","IEEE Access","Mohammed Aljaafari
Shorouk E. El-Deep
Amr Abo Hany
Shaymaa E. Sorour","10.1109/access.2024.3451369","","The healthcare sector faces intricate challenges that demand innovative solutions to enhance patient outcomes and streamline operations. The advent of Artificial Intelligence (AI) has unleashed groundbreaking potential in numerous healthcare domains, including diagnostics, patient care, and disease management. This study explores the incorporation of AI-driven methodologies for the advanced monitoring of diabetes and kidney diseases. It underscores the development of predictive models that utilize six Machine Learning (ML) and four deep learning (DL) models: Our comprehensive data analysis and rigorous model evaluation showcase AI’s capability to significantly enhance clinical practices, fostering a proactive healthcare environment marked by precision, personalization, and predictive care. Our results demonstrate substantial enhancements in the accuracy of disease monitoring. For diabetes prediction, the Gradient Boosting (GB) and Random Forest (RF) models achieved up to 89.61% accuracy, while the hybrid LSTM-CNN model outperformed other DL models with an accuracy of 89.7%. For kidney disease prediction, the RF model reached 97.5% accuracy, and the LSTM-CNN model demonstrated a remarkable accuracy of 98.9%. These findings underscore the transformative potential of AI in healthcare, fostering a proactive environment characterized by precision, personalization, and predictive care. Integrating AI within CURA’s virtual wards facilitates earlier disease detection and timely interventions and enables more tailored treatment plans, ultimately optimizing healthcare delivery and patient management.","This study integrates AI-driven methodologies for diabetes and kidney disease monitoring, showcasing substantial enhancements in accuracy using machine learning and deep learning models, ultimately optimizing healthcare delivery and patient management through predictive care and personalized treatment plans."
"An Innovative Ensemble Deep Learning Clinical Decision Support System for Diabetes Prediction","https://scispace.com/paper/an-innovative-ensemble-deep-learning-clinical-decision-2ho4ukstivlm","","Journal Article","IEEE Access","Mana Saleh Al Reshan
Samina Amin
Muhammad Ali Zeb
Adel Sulaiman
Hani Alshahrani
Asadullah Shaikh
M.A. Elmagzoub","10.1109/access.2024.3436641","","Diabetes is a significant global health concern, with an increasing number of diabetic people at risk. It is considered a chronic disease and leads to a significant number of fatalities annually. Early prediction of diabetes is essential for preventing its progression and reducing the risk of severe complications such as kidney and heart diseases. This study proposes an innovative Ensemble Deep Learning (EDL) clinical decision support system for diabetes prediction with high accuracy. The proposed EDL model uses Deep Learning (DL) architectures such as Artificial Neural Networks (ANN), Long Short-Term Memory (LSTM), and Convolutional Neural Networks (CNN), integrated with an ensemble learning-based stacking model. The EDL is implemented based on a stack ensemble model that applies meta-level models, including stack-ANN, stack-CNN, and stack-LSTM, to improve the prediction of diabetes. Three diabetes datasets, such as I. Pima Indian Diabetes Dataset (PIMA-IDD-I), II. Diabetes Dataset Frankfurt Hospital Germany (DDFH-G), and III. Iraqi Diabetes Patient Dataset (IDPD-I) are used to train the novel EDL models. The Extra Tree Classifier (ETC) approach is used to extract the relevant features from the data. The performance of the proposed EDL models is evaluated based on major evaluation metrics such as accuracy, precision, sensitivity, specificity, F-score, Matthews Correlation Coefficient (MCC), and ROC/AUC. Among the proposed EDL models, the stack-ANN achieved robust performance using DDFH-G, PIMA-IDD-I, and IDPD-I datasets with accuracy scores of 99.51%, 98.81%, and 98.45%, respectively. The overall results demonstrate that the proposed EDL models outperform previous studies in predicting diabetes.","This study proposes an Ensemble Deep Learning (EDL) clinical decision support system for diabetes prediction, achieving high accuracy with stack-ANN, stack-CNN, and stack-LSTM models, outperforming previous studies with accuracy scores up to 99.51% on three diabetes datasets."
"Continuous Glucose, Insulin and Lifestyle Data Augmentation in Artificial Pancreas Using Adaptive Generative and Discriminative Models.","https://scispace.com/paper/continuous-glucose-insulin-and-lifestyle-data-augmentation-42b9pn7evz","2024","Journal Article","IEEE Journal of Biomedical and Health Informatics","Deepjyoti Kalita
Hrishita Sharma
Khalid B. Mirza","10.1109/jbhi.2024.3396880","","Artificial pancreas requires data from multiple sources for accurate insulin dose estimation. These include data from continuous glucose sensors, past insulin dosage information, meal quantity and time and physical activity data. The effectiveness of closed-loop diabetes management systems might be hampered by the absence of these data caused by device error or lack of compliance by patients. In this study, we demonstrate the effect of output sequence length-driven generative and discriminative model selection in high quality data generation and augmentation. This novel generative adversarial network (GAN) based architecture automatically selects the generator and discriminator architecture based on the desired output sequence length. The proposed model is able to generate glucose, physical activity, meal information data for individual patients. The discriminative scores for Ohio T1DM (2018) dataset were 0.17 ±0.03 (Inputs: CGM, CHO, Insulin) and 0.15 ±0.02 (Inputs: CGM, CHO, Insulin, Heart Rate, Steps) and for Ohio T1D (2020) dataset was 0.16 ±0.02 (Inputs: CGM, CHO, Insulin) and 0.15 ±0.02 (Inputs: CGM, CHO, Insulin, acceleration). A mixture of generated and real data was used to test predictive scores for glucose forecasting models. The best RMSE and MARD achieved for OhioT1DM patients were 17.19 ±3.22 and 7.14 ±1.76 for PH=30 min with CGM, CHO, Insulin, heartrate and steps as inputs. Similarly, the RMSE and MARD for real+synthetic data were 15.63 ±2.57 and 5.86 ±1.69 respectively. Compared to existing generative models, we demonstrate that sequence length based architecture selection leads to better synthetic data generation for multiple output sequences (CGM, CHO, Insulin) and forecasting accuracy.","The proposed GAN-based model effectively generates and augments data for artificial pancreas systems, improving forecasting accuracy and addressing data scarcity issues."
"CIRF: Coupled Image Reconstruction and Fusion Strategy for Deep Learning Based Multi-Modal Image Fusion","https://scispace.com/paper/cirf-coupled-image-reconstruction-and-fusion-strategy-for-45kq3k1tlz","2024","Journal Article","Sensors","Jiawen Zheng
Junyan Xiao
Yaowei Wang
Xuming Zhang","10.3390/s24113545","","Multi-modal medical image fusion (MMIF) is crucial for disease diagnosis and treatment because the images reconstructed from signals collected by different sensors can provide complementary information. In recent years, deep learning (DL) based methods have been widely used in MMIF. However, these methods often adopt a serial fusion strategy without feature decomposition, causing error accumulation and confusion of characteristics across different scales. To address these issues, we have proposed the Coupled Image Reconstruction and Fusion (CIRF) strategy. Our method parallels the image fusion and reconstruction branches which are linked by a common encoder. Firstly, CIRF uses the lightweight encoder to extract base and detail features, respectively, through the Vision Transformer (ViT) and the Convolutional Neural Network (CNN) branches, where the two branches interact to supplement information. Then, two types of features are fused separately via different blocks and finally decoded into fusion results. In the loss function, both the supervised loss from the reconstruction branch and the unsupervised loss from the fusion branch are included. As a whole, CIRF increases its expressivity by adding multi-task learning and feature decomposition. Additionally, we have also explored the impact of image masking on the network's feature extraction ability and validated the generalization capability of the model. Through experiments on three datasets, it has been demonstrated both subjectively and objectively, that the images fused by CIRF exhibit appropriate brightness and smooth edge transition with more competitive evaluation metrics than those fused by several other traditional and DL-based methods. ","CIRF is a novel deep learning based multi-modal image fusion strategy that incorporates image reconstruction and fusion branches with feature decomposition. It utilizes a lightweight encoder to extract base and detail features, followed by separate fusion and reconstruction branches. CIRF exhibits improved expressivity and generalization capability compared to traditional and DL-based methods."
"Noninvasive On-Skin Biosensors for Monitoring Diabetes Mellitus","https://scispace.com/paper/noninvasive-on-skin-biosensors-for-monitoring-diabetes-lle0vf6ns290","2025","Journal Article","Nano-micro Letters","Ali Sedighi
Tianyu Kou
Hui Huang
Yi Li","10.1007/s40820-025-01843-9","","Abstract Diabetes mellitus represents a major global health issue, driving the need for noninvasive alternatives to traditional blood glucose monitoring methods. Recent advancements in wearable technology have introduced skin-interfaced biosensors capable of analyzing sweat and skin biomarkers, providing innovative solutions for diabetes diagnosis and monitoring. This review comprehensively discusses the current developments in noninvasive wearable biosensors, emphasizing simultaneous detection of biochemical biomarkers (such as glucose, cortisol, lactate, branched-chain amino acids, and cytokines) and physiological signals (including heart rate, blood pressure, and sweat rate) for accurate, personalized diabetes management. We explore innovations in multimodal sensor design, materials science, biorecognition elements, and integration techniques, highlighting the importance of advanced data analytics, artificial intelligence-driven predictive algorithms, and closed-loop therapeutic systems. Additionally, the review addresses ongoing challenges in biomarker validation, sensor stability, user compliance, data privacy, and regulatory considerations. A holistic, multimodal approach enabled by these next-generation wearable biosensors holds significant potential for improving patient outcomes and facilitating proactive healthcare interventions in diabetes management. ","This review discusses noninvasive wearable biosensors for diabetes monitoring, highlighting advancements in multimodal sensor design, data analytics, and AI-driven predictive algorithms for accurate, personalized management, addressing ongoing challenges in biomarker validation and regulatory considerations."
"Towards Personalized AI-Based Diabetes Therapy: A Review","https://scispace.com/paper/towards-personalized-ai-based-diabetes-therapy-a-review-45t9not5wti5","2024","Journal Article","IEEE Journal of Biomedical and Health Informatics","Sara Campanella
Giovanni Paragliola
Valentino Cherubini
Paola Pierleoni
Lorenzo Palma","10.1109/jbhi.2024.3443137","","Insulin pumps and other smart devices have recently made significant advancements in the treatment of diabetes, a disorder that affects people all over the world. The development of medical AI has been influenced by AI methods designed to help physicians make diagnoses, choose a course of therapy, and predict outcomes. In this article, we thoroughly analyse how AI is being used to enhance and personalize diabetes treatment. The search turned up 77 original research papers, from which we've selected the most crucial information regarding the learning models employed, the data typology, the deployment stage, and the application domains. We identified two key trends, enabled mostly by AI: patient-based therapy personalization and therapeutic algorithm optimization. In the meanwhile, we point out various shortcomings in the existing literature, like a lack of multimodal database analysis or a lack of interpretability. The rapid improvements in AI and the expansion of the amount of data already available offer the possibility to overcome these difficulties shortly and enable a wider deployment of this technology in clinical settings. ","This review examines AI-based advancements in diabetes treatment, analyzing 77 papers on personalized therapy, learning models, and data typology, highlighting trends and limitations in AI-assisted diabetes care and its potential for clinical deployment."
"Discriminative fusion of moments-aligned latent representation of multimodality medical data.","https://scispace.com/paper/discriminative-fusion-of-moments-aligned-latent-1w6ry0ynfy","2023","Journal Article","Physics in Medicine and Biology","Jincheng Xie
Weixiong Zhong
Ruimeng Yang
Linjing Wang
Xin Zhen","10.1088/1361-6560/ad1271","","Fusion of multimodal medical data provides multifaceted, disease-relevant information for diagnosis or prognosis prediction modeling. Traditional fusion strategies such as feature concatenation often fail to learn hidden complementary and discriminative manifestations from high-dimensional multimodal data. To this end, we proposed a methodology for the integration of multimodality medical data by matching their moments in a latent space, where the hidden, shared information of multimodal data is gradually learned by optimization with multiple feature collinearity and correlation constrains. We first obtained the multimodal hidden representations by learning mappings between the original domain and shared latent space. Within this shared space, we utilized several relational regularizations, including data attribute preservation, feature collinearity and feature-task correlation, to encourage learning of the underlying associations inherent in multimodal data. The fused multimodal latent features were finally fed to a logistic regression classifier for diagnostic prediction. Extensive evaluations on three independent clinical datasets have demonstrated the effectiveness of the proposed method in fusing multimodal data for medical prediction modeling. .","This work proposed a methodology for the integration of multimodality medical data by matching their moments in a latent space, where the hidden, shared information of multimodal data is gradually learned by optimization with multiple feature collinearity and correlation constrains."
"Differential Absorbance and PPG-Based Non-Invasive Blood Glucose Measurement Using Spatiotemporal Multimodal Fused LSTM Model","https://scispace.com/paper/differential-absorbance-and-ppg-based-non-invasive-blood-m513a8x8d9ax","2025","Journal Article","Sensors","Jinxiu Cheng
Pengfei Xie
Hao Zhao
Zhong Ji","10.3390/s25175260","","Blood glucose monitoring is crucial for the daily management of diabetic patients. In this study, we developed a differential absorbance and photoplethysmography (PPG)-based non-invasive blood glucose measurement system (NIBGMS) using visible–near-infrared (Vis-NIR) light. Three light-emitting diodes (LEDs) (625 nm, 850 nm, and 940 nm) and three photodetectors (PDs) with different source–detector separation distances were used to detect the differential absorbance of tissues at different depths and PPG signals of the index finger. A spatiotemporal multimodal fused long short-term memory (STMF-LSTM) model was developed to improve the prediction accuracy of blood glucose levels by multimodal fusion of optical spatial information (differential absorbance and PPG signals) and glucose temporal information. The validity of the NIBGMS was preliminarily verified using multilayer perceptron (MLP), support vector regression (SVR), random forest regression (RFR), and extreme gradient boosting (XG Boost) models on datasets collected from 15 non-diabetic subjects and 3 type-2 diabetic subjects, with a total of 805 samples. Additionally, a continuous dataset consisting 272 samples from four non-diabetic subjects was used to validate the developed STMF-LSTM model. The results demonstrate that the STMF-LSTM model indicated improved prediction performance with a root mean square error (RMSE) of 0.811 mmol/L and a percentage of 100% for Parkes error grid analysis (EGA) Zone A and B in 8-fold cross validation. Therefore, the developed NIBGMS and STMF-LSTM model show potential in practical non-invasive blood glucose monitoring. ","A non-invasive blood glucose measurement system (NIBGMS) using differential absorbance and photoplethysmography (PPG) signals is developed, achieving improved prediction accuracy with a root mean square error (RMSE) of 0.811 mmol/L and 100% accuracy in Parkes error grid analysis Zone A and B."
"CGM Data Analysis 2.0: Functional Data Pattern Recognition and Artificial Intelligence Applications","https://scispace.com/paper/cgm-data-analysis-2-0-functional-data-pattern-recognition-nbc79zldd9gh","2025","Journal Article","Journal of diabetes science and technology","David C. Klonoff
Richard M. Bergenstal
Eda Cengiz
Mark A. Clements
Daniel Espes
Juan Espinoza
David Kerr
Boris Kovatchev
David M. Maahs
Julia K. Mader
Nestoras Mathioudakis
Ahmed A. Metwally
Shahid N. Shah
Bin Sheng
M Snyder
Guillermo E. Umpierrez
M. Shao
Agatha F. Scheideman
Alessandra T. Ayers
Cindy Ho
Elizabeth Healey","10.1177/19322968251353228","https://scispace.compdf/cgm-data-analysis-2-0-functional-data-pattern-recognition-nbc79zldd9gh.pdf","New methods of continuous glucose monitoring (CGM) data analysis are emerging that are valuable for interpreting CGM patterns and underlying metabolic physiology. These new methods use functional data analysis and artificial intelligence (AI), including machine learning (ML). Compared to traditional metrics for evaluating CGM tracing results (CGM Data Analysis 1.0), these new methods, which we refer to as CGM Data Analysis 2.0, can provide a more detailed understanding of glucose fluctuations and trends and enable more personalized and effective diabetes management strategies once translated into practical clinical solutions. ","CGM Data Analysis 2.0 integrates functional data analysis and artificial intelligence to provide a more detailed understanding of glucose fluctuations and trends, enabling personalized and effective diabetes management strategies through machine learning and practical clinical solutions."
"Multi-Horizon Glucose Prediction Across Populations with Deep Domain Generalization","https://scispace.com/paper/multi-horizon-glucose-prediction-across-populations-with-5bzjvdcgef","2024","Journal Article","IEEE Journal of Biomedical and Health Informatics","Taiyu Zhu
Ioannis Afentakis
Kezhi Li
Ryan Armiger
Neil Hill
Nick Oliver
Pantelis Georgiou","10.1109/jbhi.2024.3428921","","Real-time continuous glucose monitoring (CGM), augmented with accurate glucose prediction, offers an effective strategy for maintaining blood glucose levels within a therapeutically appropriate range. This is particularly crucial for individuals with type 1 diabetes (T1D) who require long-term self-management. However, with extensive glycemic variability, developing a prediction algorithm applicable across diverse populations remains a significant challenge. Leveraging meta-learning for domain generalization, we propose GPFormer, a Transformer-based zero-shot learning method designed for multi-horizon glucose prediction. We developed GPFormer on the REPLACE-BG dataset, comprising 226 participants with T1D, and proceeded to evaluate its performance using three external clinical datasets with CGM data. These included the OhioT1DM dataset, a publicly available dataset including 12 T1D participants, as well as two proprietary datasets. The first proprietary dataset included 22 participants, while the second contained 45 participants, encompassing a diverse group with T1D, type 2 diabetes, and those without diabetes, including patients admitted to hospitals. These four datasets include both outpatient and inpatient settings, various intervention strategies, and demographic variability, which effectively reflect real-world scenarios of CGM usage. When compared with a group of machine learning baseline methods, GPFormer consistently demonstrated superior performance and achieved the lowest root mean square error for all the evaluated datasets up to a prediction horizon of two hours. These experimental results highlight the effectiveness and generalizability of the proposed model across a variety of populations, demonstrating its substantial potential to enhance glucose management in a wide range of practical clinical settings. ","GPFormer, a Transformer-based zero-shot learning method designed for multi-horizon glucose prediction, demonstrates its substantial potential to enhance glucose management in a wide range of practical clinical settings."
"Multimodal Integration in Healthcare: Development with Applications in Disease Management (Preprint)","https://scispace.com/paper/multimodal-integration-in-healthcare-development-with-noocjf828mqb","2025","Journal Article","Journal of Medical Internet Research","Yan Hao
C. H. Cheng
Juanjuan Li
Hongwen Li
Xingsi Di
Xueqing Zeng
Shan Jin
Xiaobo Han
Chongsong Liu
Qianqian Wang
Bingying Luo
Xi Zeng
K Li","10.2196/76557","","Abstract Multimodal data integration has emerged as a transformative approach in the health care sector, systematically combining complementary biological and clinical data sources such as genomics, medical imaging, electronic health records, and wearable device outputs. This approach provides a multidimensional perspective of patient health that enhances the diagnosis, treatment, and management of various medical conditions. This viewpoint presents an overview of the current state of multimodal integration in health care, spanning clinical applications, current challenges, and future directions. We focus primarily on its applications across different disease domains, particularly in oncology and ophthalmology. Other diseases are briefly discussed due to the few available literature. In oncology, the integration of multimodal data enables more precise tumor characterization and personalized treatment plans. Multimodal fusion demonstrates accurate prediction of anti–human epidermal growth factor receptor 2 therapy response (area under the curve=0.91). In ophthalmology, multimodal integration through the combination of genetic and imaging data facilitates the early diagnosis of retinal diseases. However, substantial challenges remain regarding data standardization, model deployment, and model interpretability. We also highlight the future directions of multimodal integration, including its expanded disease applications, such as neurological and otolaryngological diseases, and the trend toward large-scale multimodal models, which enhance accuracy. Overall, the innovative potential of multimodal integration is expected to further revolutionize the health care industry, providing more comprehensive and personalized solutions for disease management.","Multimodal data integration in healthcare combines biological and clinical data to enhance disease diagnosis, treatment, and management, with applications in oncology and ophthalmology, and future directions in neurological and otolaryngological diseases, improving personalized solutions."
"Physical Activity Integration in Blood Glucose Level Prediction: Different Levels of Data Fusion","https://scispace.com/paper/physical-activity-integration-in-blood-glucose-level-66z1gz1044mg","2024","Journal Article","IEEE Journal of Biomedical and Health Informatics","Hoda Nemat
Heydar Khadem
Jackie Elliott
Mohammed Benaissa","10.1109/jbhi.2024.3483999","","Blood glucose level (BGL) prediction contributes to more effective management of type 1 diabetes. Physical activity (PA) is a crucial factor in diabetes management. It affects BGL, and it is imperative to effectively deploy PA in BGL prediction to support diabetes management systems by incorporating this crucial factor. Due to the erratic nature of PA's impact on BGL inter- and intra-patients and insufficient knowledge, deploying PA in BGL prediction is challenging. Hence, optimal approaches for PA fusion with BGL are demanded to improve the performance of BGL prediction. To address this gap, we propose novel methodologies for extracting and integrating information from PA data into BGL prediction. This paper proposes several novel PA-informed prediction models by developing different approaches for extracting information from PA data and fusing this information with BGL data in signal, feature, and decision levels to find the optimal approach for deploying PA in BGL prediction models. For signal-level fusion, different automatically-recorded PA data are fused with BGL data. Also, three feature engineering approaches are developed for feature-level fusion: subjective assessments of PA, objective assessments of PA, and statistics of PA. Furthermore, in decision-level fusion, ensemble learning is used to combine predictions from models trained with different inputs. Then, a comparative investigation is performed between the developed PA-informed approaches and the no-fusion approach, as well as between themselves. The analyses are performed on the publicly available Ohio dataset with rigorous evaluation. The results show that deploying PA can statistically significantly improve BGL prediction performance. The results show that deploying PA can statistically significantly improve BGL prediction performance. Also, among the developed approaches to leveraging PA in BGL prediction, fusing heart rate data at the signal-level and PA intensity categories at the feature-level with BGL data are the most effective ways. Our developed methodologies contribute to determining optimal approaches, including the kind of PA information and fusion method, to improve the performance of BGL prediction effectively. ","This study proposes novel methodologies for integrating physical activity data into blood glucose level prediction, demonstrating that deploying physical activity can statistically significantly improve prediction performance, particularly through signal-level fusion of heart rate data and feature-level fusion of PA intensity categories."
"A multimodal deep learning architecture for predicting interstitial glucose for effective type 2 diabetes management","https://scispace.com/paper/a-multimodal-deep-learning-architecture-for-predicting-8t2ekzhehdu6","2025","Journal Article","Dental science reports","Muhammad Salman Haleem
Daphne N. Katsarou
Eleni I. Georga
George Dafoulas
Αlexandra Bargiota
Laura Lopez-Perez
Miguel Rujas
Giuseppe Fico
Leandro Pecchia
Dimitrios I. Fotiadis","10.1038/s41598-025-07272-3","","Abstract The accurate prediction of blood glucose is critical for the effective management of diabetes. Modern continuous glucose monitoring (CGM) technology enables real-time acquisition of interstitial glucose concentrations, which can be calibrated against blood glucose measurements. However, a key challenge in the effective management of type 2 diabetes lies in forecasting critical events driven by glucose variability. While recent advances in deep learning enable modeling of temporal patterns in glucose fluctuations, most of the existing methods rely on unimodal inputs and fail to account for individual physiological differences that influence interstitial glucose dynamics. These limitations highlight the need for multimodal approaches that integrate additional personalized physiological information. One of the primary reasons for multimodal approaches not being widely studied in this field is the bottleneck associated with the availability of subjects’ health records. In this paper, we propose a multimodal approach trained on sequences of CGM values and enriched with physiological context derived from health records of 40 individuals with type 2 diabetes. The CGM time series were processed using a stacked Convolutional Neural Network (CNN) and a Bidirectional Long Short-Term Memory (BiLSTM) network followed by an attention mechanism. The BiLSTM learned long-term temporal dependencies, while the CNN captured local sequential features. Physiological heterogeneity was incorporated through a separate pipeline of neural networks that processed baseline health records and was later fused with the CGM modeling stream. To validate our model, we utilized CGM values of 30 min sampled with a moving window of 5 min to predict the CGM values with a prediction horizon of (a) 15 min, (b) 30 min, and (c) 60 min. We achieved the multimodal architecture prediction results with Mean Absolute Point Error (MAPE) between 14 and 24 mg/dL, 19–22 mg/dL, 25–26 mg/dL in case of Menarini sensor and 6–11 mg/dL, 9–14 mg/dL, 12–18 mg/dL in case of Abbot sensor for 15, 30 and 60 min prediction horizon respectively. The results suggested that the proposed multimodal model achieved higher prediction accuracy compared to unimodal approaches; with upto 96.7% prediction accuracy; supporting its potential as a generalizable solution for interstitial glucose prediction and personalized management in the type 2 diabetes population. ","This study proposes a multimodal deep learning architecture that integrates continuous glucose monitoring data with physiological context from health records to predict interstitial glucose levels in type 2 diabetes patients with high accuracy (up to 96.7%) and low error (MAPE 6-26 mg/dL)."
"A Comprehensive Review On Deep Learning-Based Data Fusion","https://scispace.com/paper/a-comprehensive-review-on-deep-learning-based-data-fusion-3j9yye7aq2lz","2024","Journal Article","IEEE Access","Mazhar Hussain
Mattias O′Nils
Jan Lundgren
Seyed Jalaleddin Mousavirad","10.1109/access.2024.3508271","","The rapid progress in sensor technology and computational capabilities has significantly improved real-time data collection, enabling precise monitoring of various phenomena and industrial processes. However, the volume and complexity of heterogeneous data present substantial processing challenges. Traditional data-processing techniques, such as data aggregation, filtering, and statistical analysis, are increasingly supplemented by data fusion methods. These methods can be broadly categorised into traditional analytics-based approaches, like the Kalman Filter and Particle Filter, and learning-based approaches, utilising machine learning and deep learning techniques such as Artificial Neural Networks (ANN) and Convolutional Neural Networks (CNN). These techniques combine data from multiple sources to provide a comprehensive and accurate representation of information, which is critical in number of fields. Despite this, a comprehensive review of learning-based, particularly deep learning-based, data fusion strategies is lacking. This paper presents a thorough review of deep learning-based data fusion methodologies across various fields, examining their evolution over the past five years. It highlights applications in remote sensing, healthcare, industrial fault diagnosis, intelligent transportation, and other domains. The paper categories fusion strategies into early-level, intermediate-level, late-level, and hybrid fusion, emphasising their synergies, challenges, and suitability. It outlines significant advancements, the comparative advantages of deep learning-based methods over traditional approaches, and emerging trends and future directions. To ensure a comprehensive analysis, the review is structured using the ProKnow-C methodology, a rigorous selection process that focuses on relevant literature from recent years.","This paper reviews deep learning-based data fusion methodologies across various fields, highlighting applications in remote sensing, healthcare, and industrial fault diagnosis, and categorizing fusion strategies into early-level, intermediate-level, late-level, and hybrid fusion."
"Rethinking Multimodality: Optimizing Multimodal Deep Learning for Biomedical Signal Classification","https://scispace.com/paper/rethinking-multimodality-optimizing-multimodal-deep-learning-go6smhm43m8g","2025","Journal Article","IEEE Access","Timothy Oladunni
Alex Wong","10.1109/access.2025.3605315","https://scispace.compdf/rethinking-multimodality-optimizing-multimodal-deep-learning-go6smhm43m8g.pdf","This study presents a novel perspective on multimodal deep learning for biomedical signal classification, systematically analyzing the impact of complementary feature domains on model performance. While fusing multiple modalities often presumes enhanced accuracy, this research demonstrates that adding modalities can yield diminishing returns, as not all fusions are inherently advantageous. To validate this, five deep learning models were rigorously evaluated for electrocardiogram (ECG) classification: three unimodal models (1D-CNN for time, 2D-CNN for time-frequency, and a 1D-CNN-Transformer for frequency) and two multimodal models. Our findings, supported by Bayesian inference and p-values <0.05, show that the fusion of the time and time-frequency domains consistently outperformed the unimodal baselines. This confirmed the synergistic complementarity of these specific domains. Conversely, including the frequency domain offered no further improvement and, at times, led to a marginal decline in performance, a phenomenon we substantiated with an ablation study. This indicates that the frequency domain provided representational redundancy rather than new, useful information. Redefining the fundamental principles of multimodal design in biomedical signal analysis, we introduce the Complementary Domain Theory, which posits that optimal domain fusion is not about the number of modalities but the quality of their inherent complementarity. This paradigm shifts the focus from traditional, heuristic feature selection to a mathematically quantifiable framework for identifying ideal domain combinations. Our findings, while applied to ECG, have broader relevance to other biomedical domains involving multimodal time-series data. The core insight is that optimal multimodal performance arises from the intrinsic information-theoretic complementarity among fused domains.","This study redefines multimodal deep learning for biomedical signal classification, introducing the Complementary Domain Theory, which prioritizes domain complementarity over modality count, and demonstrates that optimal fusion arises from synergistic complementarity among specific domains, not merely adding more modalities."
"A Lesion-Fusion Neural Network for Multi-View Diabetic Retinopathy Grading.","https://scispace.com/paper/a-lesion-fusion-neural-network-for-multi-view-diabetic-fl696zeexu","2024","Journal Article","IEEE Journal of Biomedical and Health Informatics","Xiaoling Luo
QiHao Xu
Zhihua Wang
Chao Huang
Chengliang Liu
Xiaopeng Jin
Jianguo Zhang","10.1109/jbhi.2024.3384251","","As the most common complication of diabetes, diabetic retinopathy (DR) is one of the main causes of irreversible blindness. Automatic DR grading plays a crucial role in early diagnosis and intervention, reducing the risk of vision loss in people with diabetes. In these years, various deep-learning approaches for DR grading have been proposed. Most previous DR grading models are trained using the dataset of single-field fundus images, but the entire retina cannot be fully visualized in a single field of view. There are also problems of scattered location and great differences in the appearance of lesions in fundus images. To address the limitations caused by incomplete fundus features, and the difficulty in obtaining lesion information. This work introduces a novel multi-view DR grading framework, which solves the problem of incomplete fundus features by jointly learning fundus images from multiple fields of view. Furthermore, the proposed model combines multi-view inputs such as fundus images and lesion snapshots. It utilizes heterogeneous convolution blocks (HCB) and scalable self-attention classes (SSAC), which enhance the ability of the model to obtain lesion information. The experimental results show that our proposed method performs better than the benchmark methods on the large-scale dataset.","This work introduces a novel multi-view DR grading framework, which solves the problem of incomplete fundus features by jointly learning fundus images from multiple fields of view by jointly learning fundus images from multiple fields of view."
"A sensitivity indicator screening and intelligent classification method for the diagnosis of T2D-CHD","https://scispace.com/paper/a-sensitivity-indicator-screening-and-intelligent-1po6896qyo","2024","Journal Article","Frontiers in Cardiovascular Medicine","Jiarui Li
Changjiang Ying","10.3389/fcvm.2024.1358066","https://www.frontiersin.org/articles/10.3389/fcvm.2024.1358066/pdf?isPublishedV2=False","The prevalence of Type 2 Diabetes Mellitus (T2D) and its significant role in increasing Coronary Heart Disease (CHD) risk highlights the urgent need for effective CHD screening within this population. Despite current advancements in T2D management, the complexity of cardiovascular complications persists. Our study aims to develop a comprehensive CHD screening model for T2D patients, employing multimodal data to improve early detection and management, addressing a critical gap in clinical practice.We analyzed data from 699 patients, including 471 with CHD (221 of these also had T2D) and a control group of 228 without CHD. Employing strict diagnostic criteria, we conducted significance testing and multivariate analysis to identify key indicators for T2D-CHD diagnosis. This led to the creation of a neural network model using 21 indicators and a logistic regression model based on an 8-indicator subset. External validation was performed with an independent dataset from an additional 212 patients to confirm the models’ generalizability.The neural network model achieved an accuracy of 90.7%, recall of 90.78%, precision of 90.83%, and an F-1 score of 0.908. The logistic regression model demonstrated an accuracy of 90.13%, recall of 90.1%, precision of 90.22%, and an F-1 score of 0.9016. External validation reinforced the models’ reliability and effectiveness in broader clinical settings.Our AI-driven diagnostic models significantly enhance early CHD detection and management in T2D patients, offering a novel, efficient approach to addressing the complex interplay between these conditions. By leveraging advanced analytics and comprehensive patient data, we present a scalable solution for improving clinical outcomes in this high-risk population, potentially setting a new standard in personalized care and preventative medicine.","A comprehensive CHD screening model for T2D patients is developed, employing multimodal data to improve early detection and management, addressing a critical gap in clinical practice and potentially setting a new standard in personalized care and preventative medicine."
"T2Net: Tongue Image-Based T2DM Detection Via Simulated Clinical Diagnostic Reasoning","https://scispace.com/paper/t2net-tongue-image-based-t2dm-detection-via-simulated-nalwbywmj7l0","2025","Journal Article","IEEE Journal of Biomedical and Health Informatics","Yang Liu
Peiyu Liu
Yanyi Huang
Liyun Li
Xiaojie Feng
Miao Xie
Junhao Chen
Jiayu Ye
An Zeng
Jianlu Bi","10.1109/jbhi.2025.3609982","","Clinical studies indicate that the progression of Type 2 Diabetes Mellitus (T2DM) is associated with characteristic alterations in tongue features, which may facilitate non-invasive early detection. However, current deep learning-based tongue imaging approaches for diabetes diagnosis remain constrained by limited datasets, subtle feature variations, dependence on clinical expertise, and the lack of quantitative evaluation. To address these issues, we developed an open-source dataset for T2DM tongue diagnosis (DMT) and benchmarked it using multiple baseline models. Building on DMT, we propose T2Net, a tongue image recognition model for T2DM that simulates the clinical diagnostic process. T2Net comprises four core components: local inspection, pathological clue integration, syndrome identification, and diagnostic confidence estimation. First, T2Net automatically extracts key ROIs by combining large-kernel decomposition with multi-scale learning. Then, a multi-order feature interaction module enables effective fusion of tongue image features across scales to capture pathological clues. Meanwhile, we design a context-aware dynamic aggregation convolution to model long-range dependencies, and propose a flexible focal loss to mimic the diagnostic reasoning process of clinicians, enabling brain-inspired inference. Finally, we propose a clustering-based confidence estimation approach to quantitatively evaluate the reliability of model predictions. Experimental results demonstrate that T2Net achieves highly competitive performance on the DMT dataset, outperforming the second-best baseline by 2.7% in accuracy and 2.0% in F1 score. Moreover, the quantitative evaluation scores are largely consistent with clinical assessments by physicians. DMT dataset is available at https://github.com/yjy-97/DMT. ","Researchers propose T2Net, a tongue image recognition model for Type 2 Diabetes Mellitus (T2DM) detection, simulating clinical diagnostic reasoning with four core components, achieving competitive performance on a newly created open-source dataset and consistent with clinical assessments."
"Heterogeneous temporal representation for diabetic blood glucose prediction","https://scispace.com/paper/heterogeneous-temporal-representation-for-diabetic-blood-1q0t4505d5","2023","Journal Article","Frontiers in Physiology","Yaohui Huang
Zhikai Ni
Zhenkun Lu
Xinqi He
Jinbo Hu
Boxuan Li
Houguan Ya
Yuexi Shi","10.3389/fphys.2023.1225638","https://scispace.compdf/heterogeneous-temporal-representation-for-diabetic-blood-1q0t4505d5.pdf","Background and aims: Blood glucose prediction (BGP) has increasingly been adopted for personalized monitoring of blood glucose levels in diabetic patients, providing valuable support for physicians in diagnosis and treatment planning. Despite the remarkable success achieved, applying BGP in multi-patient scenarios remains problematic, largely due to the inherent heterogeneity and uncertain nature of continuous glucose monitoring (CGM) data obtained from diverse patient profiles. Methodology: This study proposes the first graph-based Heterogeneous Temporal Representation (HETER) network for multi-patient Blood Glucose Prediction (BGP). Specifically, HETER employs a flexible subsequence repetition method (SSR) to align the heterogeneous input samples, in contrast to the traditional padding or truncation methods. Then, the relationships between multiple samples are constructed as a graph and learned by HETER to capture global temporal characteristics. Moreover, to address the limitations of conventional graph neural networks in capturing local temporal dependencies and providing linear representations, HETER incorporates both a temporally-enhanced mechanism and a linear residual fusion into its architecture. Results: Comprehensive experiments were conducted to validate the proposed method using real-world data from 112 patients in two hospitals, comparing it with five well-known baseline methods. The experimental results verify the robustness and accuracy of the proposed HETER, which achieves the maximal improvement of 31.42%, 27.18%, and 34.85% in terms of MAE, MAPE, and RMSE, respectively, over the second-best comparable method. Discussions: HETER integrates global and local temporal information from multi-patient samples to alleviate the impact of heterogeneity and uncertainty. This method can also be extended to other clinical tasks, thereby facilitating efficient and accurate capture of crucial pattern information in structured medical data. ","Heterogeneous temporal representation for diabetic blood glucose prediction is a novel method for improving blood glucose prediction accuracy in multi-patient scenarios. It utilizes a graph-based heterogeneous temporal representation network to capture global and local temporal information from diverse patient profiles."
"Fusion-driven multimodal learning for biomedical time series in surgical care","https://scispace.com/paper/fusion-driven-multimodal-learning-for-biomedical-time-series-bg3481av62ph","2025","Journal Article","Frontiers in Physiology","Jinshan Che
Mingming Sun
Yuhong Wang
Zhendan Xu","10.3389/fphys.2025.1605406","https://scispace.compdf/fusion-driven-multimodal-learning-for-biomedical-time-series-bg3481av62ph.pdf","Introduction The integration of multimodal data has become a crucial aspect of biomedical time series prediction, offering improved accuracy and robustness in clinical decision-making. Traditional approaches often rely on unimodal learning paradigms, which fail to fully exploit the complementary information across heterogeneous data sources such as physiological signals, imaging, and electronic health records. These methods suffer from modality misalignment, suboptimal feature fusion, and lack of adaptive learning mechanisms, leading to performance degradation in complex biomedical scenarios. Methods To address these challenges, we propose a novel multimodal Deep Learning framework that dynamically captures inter-modal dependencies and optimizes cross-modal interactions for time series prediction. Our approach introduces an Adaptive Multimodal Fusion Network (AMFN), which leverages attention-based alignment, graph-based representation learning, and a modality-adaptive fusion mechanism to enhance information integration. Furthermore, we develop a Dynamic Cross-Modal Learning Strategy (DCMLS) that optimally selects relevant features, mitigates modality-specific noise, and incorporates uncertainty-aware learning to improve model generalization. Results Experimental evaluations on biomedical datasets demonstrate that our method outperforms state-of-the-art techniques in predictive accuracy, robustness, and interpretability. Discussion By effectively bridging the gap between heterogeneous biomedical data sources, our framework offers a promising direction for AI-driven disease diagnosis and treatment planning. ","This study proposes a novel multimodal deep learning framework, AMFN, with a dynamic cross-modal learning strategy, DCMLS, to improve time series prediction in surgical care by effectively integrating heterogeneous biomedical data sources."
"PPG based Noninvasive Blood Glucose Monitoring using Multi-view Attention and Cascaded BiLSTM Hierarchical Feature Fusion Approach.","https://scispace.com/paper/ppg-based-noninvasive-blood-glucose-monitoring-using-multi-5y2yf354rckc","2024","Journal Article","IEEE Journal of Biomedical and Health Informatics","Mubashir Ali
Jingzhen Li
Bokun Fan
Zedong Nie","10.1109/jbhi.2024.3464098","","Diabetes is a chronic disease with exponential growth and poses significant challenges to global healthcare. Regular blood glucose (BG) monitoring is key for avoiding diabetic complications. Traditional BG measurement techniques are invasive and minimally invasive, causing pain, discomfort, cost, and infection risks. To address these issues, we developed a noninvasive BG monitoring approach on photoplethysmography (PPG) signals using multi-view attention and cascaded BiLSTM hierarchical feature fusion approach. Firstly, we implemented a convolutional multi-view attention block to extract the temporal features through adaptive contextual information aggregation. Secondly, we built a cascaded BiLSTM network to efficiently extract the fine-grained features through bidirectional learning. Finally, we developed a hierarchical feature fusion with bilinear polling through cross-layer interaction to obtain higher-order features for BG monitoring. For validation, we conducted comprehensive experimentation on up to 6 days of PPG and BG data from 21 participants. The proposed approach showed competitive results compared to existing approaches by RMSE of 1.67 mmol/L and MARD of 17.88%. Additionally, the clinical accuracy using Clarke error grid (CEG) analysis showed 98.80% of BG values in Zone A+B. Therefore, the proposed approach offers a favorable solution in diabetes management by noninvasively monitoring the BG levels.","A noninvasive blood glucose monitoring approach using photoplethysmography (PPG) signals is proposed, achieving competitive results with RMSE of 1.67 mmol/L and MARD of 17.88%, and 98.80% clinical accuracy using Clarke error grid analysis."
"Transfer learning prediction of type 2 diabetes with unpaired clinical and genetic data","https://scispace.com/paper/transfer-learning-prediction-of-type-2-diabetes-with-toxtadvnmvti","2025","Journal Article","Dental science reports","YounSung Jung
Sohee Han
Eun‐Hee Kang
So‐Young Park
Min‐Hee Kim
Nan Hee Kim
TaeJin Ahn","10.1038/s41598-025-05532-w","","The prevalence of type 2 diabetes mellitus (T2DM) in Korea has risen in recent years, yet many cases remain undiagnosed. Advanced artificial intelligence models using multi-modal data have shown promise in disease prediction, but two major challenges persist: the scarcity of samples containing all desired data modalities and class imbalance in T2DM datasets. We propose a novel transfer learning framework to predict T2DM onset within five years, using two Korean cohorts (KoGES and SNUH). To utilize unpaired multi-modal data, our approach transfers knowledge between clinical and genetic domains, leveraging unpaired clinical data alongside paired data. We also address class imbalance by applying a positively weighted binary cross-entropy (BCE) loss and a weighted random sampler (WRS). The transfer learning framework improved T2DM prediction performance. Using WRS and weighted BCE loss increased the model's balanced accuracy and AUC (achieving test AUC 0.8441). Furthermore, combining transfer learning with intermediate data fusion yielded even higher performance (test AUC 0.8715). These enhancements were achieved despite limited paired multi-modal samples. Our framework effectively handles scarce paired data and class imbalance, leading to improved T2DM risk prediction. This approach can be adapted to other medical prediction tasks and integrated with additional data modalities, potentially aiding earlier diagnosis and better disease management in clinical settings. ","This study proposes a transfer learning framework to predict type 2 diabetes onset using unpaired clinical and genetic data from two Korean cohorts, improving performance with weighted BCE loss and WRS, and achieving a test AUC of 0.8715."
"LLM-Powered Prediction of Hyperglycemia and Discovery of Behavioral Treatment Pathways from Wearables and Diet","https://scispace.com/paper/llm-powered-prediction-of-hyperglycemia-and-discovery-of-n2qgdgfhe0co","2025","Journal Article","Sensors","Abdullah Mamun
Asiful Arefeen
Susan B. Racette
Dorothy D. Sears
Corrie M. Whisner
Matthew P. Buman
Hassan Ghasemzadeh","10.3390/s25175372","","Postprandial hyperglycemia, marked by the blood glucose level exceeding the normal range after consuming a meal, is a critical indicator of progression toward type 2 diabetes in people with prediabetes and in healthy individuals. A key metric for understanding blood glucose dynamics after eating is the postprandial Area Under the Curve (AUC). Predicting postprandial AUC in advance based on a person’s lifestyle factors, such as diet and physical activity level, and explaining the factors that affect postprandial blood glucose could allow an individual to adjust their behavioral choices accordingly to maintain normal glucose levels. In this work, we develop an explainable machine learning solution, GlucoLens, that takes sensor-driven inputs and utilizes advanced data processing, large language models, and trainable machine learning models to estimate postprandial AUC and predict hyperglycemia from diet, physical activity, and recent glucose patterns. We use data obtained using wearables in a five-week clinical trial of 10 adults who worked full-time to develop and evaluate the proposed computational model that integrates wearable sensing, multimodal data, and machine learning. Our machine learning model takes multimodal data from wearable activity and glucose monitoring sensors, along with food and work logs, and provides an interpretable prediction of the postprandial glucose patterns. GlucoLens achieves a normalized root mean squared error (NRMSE) of 0.123 in its best configuration. On average, the proposed technology provides a 16% better predictive performance compared to the comparison models. Additionally, our technique predicts hyperglycemia with an accuracy of 79% and an F1 score of 0.749 and recommends different treatment options to help avoid hyperglycemia through diverse counterfactual explanations. With systematic experiments and discussion supported by established prior research, we show that our method is generalizable and consistent with clinical understanding. ","This study develops GlucoLens, an explainable machine learning model that predicts postprandial hyperglycemia and recommends behavioral treatment pathways using wearable data, diet, and physical activity, achieving 16% better predictive performance and 79% accuracy in hyperglycemia prediction."
"BAF-Net: bidirectional attention-aware fluid pyramid feature integrated multimodal fusion network for diagnosis and prognosis.","https://scispace.com/paper/baf-net-bidirectional-attention-aware-fluid-pyramid-feature-1ji3hcgluu","2024","Journal Article","Physics in Medicine and Biology","Huiqin Wu
Lihong Peng
Dongyang Du
Hui Xu
Guoyu Lin
Zidong Zhou
Lijun Liu
Wenbing Lv","10.1088/1361-6560/ad3cb2","","OBJECTIVE
To go beyond the deficiencies of the three conventional multimodal fusion strategies (i.e., input-, feature- and output-level fusion), we propose a bidirectional attention-aware fluid pyramid feature integrated fusion network (BAF-Net) with cross-modal interactions for multimodal medical image diagnosis and prognosis. Approach: BAF-Net is composed of two identical branches to preserve the unimodal features and one bidirectional attention-aware distillation stream to progressively assimilate cross-modal complements and to learn supplementary features in both bottom-up and top-down processes. Fluid pyramid connections were adopted to integrate the hierarchical features at different levels of the network, and channel-wise attention modules were exploited to mitigate cross-modal cross-level incompatibility. Furthermore, depth-wise separable convolution was introduced to fuse the cross-modal cross-level features to alleviate the increase in parameters to a great extent. The generalization abilities of BAF-Net were evaluated in terms of two clinical tasks: (1) An in-house PET-CT dataset with 174 patients for differentiation between lung cancer and pulmonary tuberculosis. (2) A public multicenter PET-CT head and neck cancer dataset with 800 patients from nine centers for overall survival prediction. Main results: On the LC-PTB dataset, improved performance was found in BAF-Net (AUC = 0.7342) compared with input-level fusion model (AUC = 0.6825; p < 0.05), feature-level fusion model (AUC = 0.6968; p = 0.0547), output-level fusion model (AUC = 0.7011; p < 0.05). On the H&N cancer dataset, BAF-Net (C-index = 0.7241) outperformed the input-, feature-, and output-level fusion model, with 2.95%, 3.77%, and 1.52% increments of C-index (p = 0.3336, 0.0479 and 0.2911, respectively). The ablation experiments demonstrated the effectiveness of all the designed modules regarding all the evaluated metrics in both datasets. Significance: Extensive experiments on two datasets demonstrated better performance and robustness of BAF-Net than three conventional fusion strategies and PET or CT unimodal network in terms of diagnosis and prognosis.","Extensive experiments on two datasets demonstrated better performance and robustness of BAF-Net than three conventional fusion strategies and PET or CT unimodal network in terms of diagnosis and prognosis."
"Incorporating Uncertainty Estimation and Interpretability in Personalized Glucose Prediction Using the Temporal Fusion Transformer","https://scispace.com/paper/incorporating-uncertainty-estimation-and-interpretability-in-3homlu1a6bvd","2025","Journal Article","Sensors","Antonio J. Rodríguez-Almeida
Carmelo Betancort
Ana M. Wägner
Gustavo M. Callicó
Himar Fabelo","10.3390/s25154647","","More than 14% of the world’s population suffered from diabetes mellitus in 2022. This metabolic condition is defined by increased blood glucose concentrations. Among the different types of diabetes, type 1 diabetes, caused by a lack of insulin secretion, is particularly challenging to treat. In this regard, automatic glucose level estimation implements Continuous Glucose Monitoring (CGM) devices, showing positive therapeutic outcomes. AI-based glucose prediction has commonly followed a deterministic approach, usually with a lack of interpretability. Therefore, these AI-based methods do not provide enough information in critical decision-making scenarios, like in the medical field. This work intends to provide accurate, interpretable, and personalized glucose prediction using the Temporal Fusion Transformer (TFT), and also includes an uncertainty estimation. The TFT was trained using two databases, an in-house-collected dataset and the OhioT1DM dataset, commonly used for glucose forecasting benchmarking. For both datasets, the set of input features to train the model was varied to assess their impact on model interpretability and prediction performance. Models were evaluated using common prediction metrics, diabetes-specific metrics, uncertainty estimation, and interpretability of the model, including feature importance and attention. The obtained results showed that TFT outperforms existing methods in terms of RMSE by at least 13% for both datasets. ","This study develops a personalized glucose prediction model using the Temporal Fusion Transformer (TFT) with uncertainty estimation and interpretability, outperforming existing methods by at least 13% in RMSE for both in-house and OhioT1DM datasets."