"Paper Title","Paper Link","Publication Year","Publication Type","Publication Title","Author Names","DOI","PDF Link","Abstract","TL;DR"
"Toward a responsible future: recommendations for AI-enabled clinical decision support","https://scispace.com/paper/toward-a-responsible-future-recommendations-for-ai-enabled-4zsaqj5z6gd5","2024","Journal Article","Journal of the American Medical Informatics Association","Steven E. Labkoff
Bilikis J. Oladimeji
Joseph Kannry
Anthony Solomonides
Russell Leftwich
Eileen Koski
Amanda L. Joseph
Mónica López-González
Lee A. Fleisher
Kimberly Nolen
Sayon Dutta
Deborah R Levy
Amy Price
Paul Barr
Jonathan D. Hron
Baihan Lin
Gyana Srivastava
Núria Pastor
Unai Sanchez Luque
Tien Thi Thuy Bui
Reva Singh
T. A. Williams
Mark G. Weiner
Tristan Naumann
Dean F. Sittig
Gretchen Purcell Jackson
Yuri Quintana","10.1093/jamia/ocae209","","Abstract Background Integrating artificial intelligence (AI) in healthcare settings has the potential to benefit clinical decision-making. Addressing challenges such as ensuring trustworthiness, mitigating bias, and maintaining safety is paramount. The lack of established methodologies for pre- and post-deployment evaluation of AI tools regarding crucial attributes such as transparency, performance monitoring, and adverse event reporting makes this situation challenging. Objectives This paper aims to make practical suggestions for creating methods, rules, and guidelines to ensure that the development, testing, supervision, and use of AI in clinical decision support (CDS) systems are done well and safely for patients. Materials and Methods In May 2023, the Division of Clinical Informatics at Beth Israel Deaconess Medical Center and the American Medical Informatics Association co-sponsored a working group on AI in healthcare. In August 2023, there were 4 webinars on AI topics and a 2-day workshop in September 2023 for consensus-building. The event included over 200 industry stakeholders, including clinicians, software developers, academics, ethicists, attorneys, government policy experts, scientists, and patients. The goal was to identify challenges associated with the trusted use of AI-enabled CDS in medical practice. Key issues were identified, and solutions were proposed through qualitative analysis and a 4-month iterative consensus process. Results Our work culminated in several key recommendations: (1) building safe and trustworthy systems; (2) developing validation, verification, and certification processes for AI-CDS systems; (3) providing a means of safety monitoring and reporting at the national level; and (4) ensuring that appropriate documentation and end-user training are provided. Discussion AI-enabled Clinical Decision Support (AI-CDS) systems promise to revolutionize healthcare decision-making, necessitating a comprehensive framework for their development, implementation, and regulation that emphasizes trustworthiness, transparency, and safety. This framework encompasses various aspects including model training, explainability, validation, certification, monitoring, and continuous evaluation, while also addressing challenges such as data privacy, fairness, and the need for regulatory oversight to ensure responsible integration of AI into clinical workflow. Conclusions Achieving responsible AI-CDS systems requires a collective effort from many healthcare stakeholders. This involves implementing robust safety, monitoring, and transparency measures while fostering innovation. Future steps include testing and piloting proposed trust mechanisms, such as safety reporting protocols, and establishing best practice guidelines. ","This paper proposes a framework for responsible AI-enabled clinical decision support, emphasizing trustworthiness, transparency, and safety through validation, verification, certification, and monitoring processes, to ensure safe and effective integration into healthcare workflow."
"Prediction and explainability in AI: Striking a new balance?","https://scispace.com/paper/prediction-and-explainability-in-ai-striking-a-new-balance-453sgp1lkz","2024","Journal Article","Big Data & Society","Aviad E. Raz
Bert Heinrichs
Netta Avnoon
Gil Eyal
Yael Inbar","10.1177/20539517241235871","","The debate regarding prediction and explainability in artificial intelligence (AI) centers around the trade-off between achieving high-performance accurate models and the ability to understand and interpret the decisionmaking process of those models. In recent years, this debate has gained significant attention due to the increasing adoption of AI systems in various domains, including healthcare, finance, and criminal justice. While prediction and explainability are desirable goals in principle, the recent spread of high accuracy yet opaque machine learning (ML) algorithms has highlighted the trade-off between the two, marking this debate as an inter-disciplinary, inter-professional arena for negotiating expertise. There is no longer an agreement about what should be the “default” balance of prediction and explainability, with various positions reflecting claims for professional jurisdiction. Overall, there appears to be a growing schism between the regulatory and ethics-based call for explainability as a condition for trustworthy AI, and how it is being designed, assimilated, and negotiated. The impetus for writing this commentary comes from recent suggestions that explainability is overrated, including the argument that explainability is not guaranteed in human healthcare experts either. To shed light on this debate, its premises, and its recent twists, we provide an overview of key arguments representing different frames, focusing on AI in healthcare. ","The AI prediction-explainability trade-off is a debated topic, with varying opinions on the balance between accuracy and interpretability, particularly in high-stakes domains like healthcare, finance, and criminal justice, where trustworthiness is paramount."
"Enhancing medical decision-making with ChatGPT and explainable AI.","https://scispace.com/paper/enhancing-medical-decision-making-with-chatgpt-and-5vmbbnk63n","2024","Journal Article","International Journal of Surgery","Aryan Chopra
Dharmendra Singh Rajput
Harshita Patel","10.1097/js9.0000000000001464","","In light of the accelerating advancements in generative arti ﬁ cial intelligence (AI) and its applications in healthcare","This study explores the integration of ChatGPT and explainable AI to enhance medical decision-making, leveraging generative AI's advancements in healthcare to improve diagnosis, treatment, and patient outcomes through transparent and interpretable AI-driven insights."
"1036-P: Developing an Explainable AI Model to Identify Members with Diabetes at High Risk for Worsening Glycemic Control in a Large Accountable Care Network (ACN)","https://scispace.com/paper/1036-p-developing-an-explainable-ai-model-to-identify-3goayr2w","2023","Journal Article","Diabetes","Asra Kermani
Usha K. Kollipara
Michael E. Bowen
Jaime P. Almandoz
Carol J McCall
David DeCaprio
Jacqueline Mutz","10.2337/db23-1036-p","","People with diabetes and poor glycemic control (HbA1c ≥9%) have a greater likelihood of diabetes complications, increased healthcare utilization, and higher total cost of care (TCOC). Novel approaches to identify patients at high risk for worsening glycemic control can improve outcomes and contain costs within ACNs. We used XGBoost Decision Trees to build and validate a model predicting worsening glycemic control (HbA1c increase ≥1.5% over 12 months) using Medicare ACN administrative claims and electronic health record data. Eligible members had ≥18 mos. continuous enrollment, were age ≥18 years with Type 1 or 2 diabetes, had ≥2 HbA1c values between 6.5 and 14%, and no disability. Members were split into HbA1c outcome stratified training (80%) and validation (20%) datasets for 10-fold cross-validation. The model utilized 1,490 features including medical history, labs, demographics, SDOH, utilization, and quality of care. Of the 44,007 unique members (mean age 72 years, 51% Female, 61% Non-Hispanic White, mean HbA1c 7.2%, 9% on insulin, HTN 92%, Obesity 71%, CKD 33%, CAD 32%, Heart Failure 15% and COPD 13%), 5.23% had an HbA1c increase ≥1.5% over 12 months. Key features in the final model included duration of diabetes, last HbA1c value, treatment intensification, prescribed insulin or sulfonylureas, weight change, depression, acute renal failure, area deprivation index, and no car access. Model AUC was 0.672 (95% CI ±0.005) in the validation sample. The model was able to identify 23.5% of members with worsening glycemic control in the highest predicted decile of HbA1c progression (sensitivity 23.5% at 10% alert rate). Mean TCOC at 10% alert rate was $43,850 PMPY which was substantially higher than training population at $18,208 PMPY. Our model can identify ACN members with diabetes at high risk for worsening glycemic control that may benefit from education, medication adherence, food insecurity and transportation interventions.
 
 
 A.Kermani: None. C.J.Mccall: None. J.A.Gartner: None. D.Decaprio: Employee; ClosedLoop.ai. J.M.Mutz: None. J.S.Fish: None. U.Kollipara: None. J.Acosta: None. M.E.Bowen: Research Support; Boehringer-Ingelheim. J.P.Almandoz: Advisory Panel; Eli Lilly and Company, Novo Nordisk. B.Goldberg: None. A.Rozich: None. M.He: None. S.V.J.Acosta: None.
","In this article , the authors used XGBoost Decision Trees to build and validate a model predicting worsening glycemic control (HbA1c increase ≥ 1.5% over 12 months) using Medicare ACN administrative claims and electronic health record data."
"1055-P: Clinical Decision Support Systems (CDSSs) in Diabetes Management—A Scoping Review","https://scispace.com/paper/1055-p-clinical-decision-support-systems-cdsss-in-diabetes-1kqrajea","2023","Journal Article","Diabetes","Yu Liang","10.2337/db23-1055-p","","This scoping review aimed to summarize the application scenarios, effectiveness and safety of CDSS in diabetes management. PubMed, Embase, Cochrane Library, Web of Science, CNKI, Wanfang, VIP, and conference databases were searched from 1985 to 2022. There were 11,145 studies identified, and 72 articles and 8 abstracts were included, involving 137,710 subjects. Most CDSSs (77%) were knowledge-based, while the non-knowledge-based systems have been increasing in recent years. CDSS was mainly utilized in treatment recommendations (Table 1). Most CDSSs significantly improved outcomes (Table 2) without increase of hypoglycemia risk. The results demonstrated the value of CDSS as a reliable assistant in diabetes management especially in primary care with limited resources, and advanced software is needed to improve CDSS functionalities.
 
 
 
 
 S.Huang: None. Y.Liang: None. J.Li: None. X.Li: None.
 
 
 
 Beijing Medical Award Foundation (2019-1585)
","Wang et al. as discussed by the authors summarized the application scenarios, effectiveness and safety of CDSS in diabetes management, and demonstrated the value of a reliable assistant in diabetic management especially in primary care with limited resources."
"The importance of ensuring artificial intelligence and machine learning can be understood at the human level","https://scispace.com/paper/the-importance-of-ensuring-artificial-intelligence-and-r0zqica92l","2019","Journal Article","European Journal of Public Health","Sudha Ram","10.1093/EURPUB/CKZ185.259","http://academic.oup.com/eurpub/article-pdf/29/Supplement_4/ckz185.259/32631620/ckz185.259.pdf","\n With rapid developments in big data technology and the prevalence of large-scale datasets from diverse sources, the healthcare predictive analytics (HPA) field is witnessing a dramatic surge in interest. In healthcare, it is not only important to provide accurate predictions, but also critical to provide reliable explanations to the underlying black-box models making the predictions. Such explanations can play a crucial role in not only supporting clinical decision-making but also facilitating user engagement and patient safety. If users and decision makers do not have faith in the HPA model, it is highly likely that they will reject its use. Furthermore, it is extremely risky to blindly accept and apply the results derived from black-box models, which might lead to undesirable consequences or life-threatening outcomes in domains with high stakes such as healthcare. As machine learning and artificial intelligence systems are becoming more capable and ubiquitous, explainable artificial intelligence and machine learning interpretability are garnering significant attention among practitioners and researchers. The introduction of policies such as the General Data Protection Regulation (GDPR), has amplified the need for ensuring human interpretability of prediction models. In this talk I will discuss methods and applications for developing local as well as global explanations from machine learning and the value they can provide for healthcare prediction.","Methods and applications for developing local as well as global explanations from machine learning and the value they can provide for healthcare prediction are discussed."
"An innovative artificial intelligence-based method to compress complex models into explainable, model-agnostic and reduced decision support systems with application to healthcare (NEAR)","https://scispace.com/paper/an-innovative-artificial-intelligence-based-method-to-1fy43x4brl","2024","Journal Article","Artificial Intelligence in Medicine","Karim Kassem
M. Sperti
Andrea Cavallo
Andrea Mario Vergani
Davide Fassino
Monica Moz
Alessandro Liscio
Riccardo Banali
Michael Dahlweid
Luciano Benetti
F. Bruno
Guglielmo Gallone
Ovidio De Filippo
Mario Iannaccone
Fabrizio d’Ascenzo
G. D. De Ferrari
Umberto Morbiducci
E. Della Valle
Marco Agostino Deriu","10.1016/j.artmed.2024.102841","","In everyday clinical practice, medical decision is currently based on clinical guidelines which are often static and rigid, and do not account for population variability, while individualized, patient-oriented decision and/or treatment are the paradigm change necessary to enter into the era of precision medicine. Most of the limitations of a guideline-based system could be overcome through the adoption of Clinical Decision Support Systems (CDSSs) based on Artificial Intelligence (AI) algorithms. However, the black-box nature of AI algorithms has hampered a large adoption of AI-based CDSSs in clinical practice. In this study, an innovative AI-based method to compress AI-based prediction models into explainable, model-agnostic, and reduced decision support systems (NEAR) with application to healthcare is presented and validated. NEAR is based on the Shapley Additive Explanations framework and can be applied to complex input models to obtain the contributions of each input feature to the output. Technically, the simplified NEAR models approximate contributions from input features using a custom library and merge them to determine the final output. Finally, NEAR estimates the confidence error associated with the single input feature contributing to the final score, making the result more interpretable. Here, NEAR is evaluated on a clinical real-world use case, the mortality prediction in patients who experienced Acute Coronary Syndrome (ACS), applying three different Machine Learning/Deep Learning models as implementation examples. NEAR, when applied to the ACS use case, exhibits performances like the ones of the AI-based model from which it is derived, as in the case of the Adaptive Boosting classifier, whose Area Under the Curve is not statistically different from the NEAR one, even the model's simplification. Moreover, NEAR comes with intrinsic explainability and modularity, as it can be tested on the developed web application platform (https://neardashboard.pythonanywhere.com/). An explainable and reliable CDSS tailored to single-patient analysis has been developed. The proposed AI-based system has the potential to be used alongside the clinical guidelines currently employed in the medical setting making them more personalized and dynamic and assisting doctors in taking their everyday clinical decisions. ","An explainable and reliable CDSS tailored to single-patient analysis has been developed and has the potential to be used alongside the clinical guidelines currently employed in the medical setting making them more personalized and dynamic and assisting doctors in taking their everyday clinical decisions."
"Unraveling the Black Box: A Review of Explainable Deep Learning Healthcare Techniques","https://scispace.com/paper/unraveling-the-black-box-a-review-of-explainable-deep-2sp1g9enne","","Journal Article","IEEE Access","Nafeesa Yousuf Murad
Muhammad Hamza Azam
Nadia Yousuf
Jameel Shehu Yalli","10.1109/access.2024.3398203","","The integration of deep learning in healthcare has propelled advancements in diagnostics and decision support. However, the inherent opacity of deep neural networks (DNNs) poses challenges to their acceptance and trust in clinical settings. This survey paper delves into the landscape of explainable deep learning techniques within the healthcare domain, offering a thorough examination of deep learning explainability techniques. Recognizing the pressing need for nuanced interpretability, we extend our focus to include the integration of fuzzy logic as a novel and vital category. The survey begins by categorizing and critically analyzing existing intrinsic, visualization, and distillation techniques, shedding light on their strengths and limitations in healthcare applications. Building upon this foundation, we introduce fuzzy logic as a distinct category, emphasizing its capacity to address uncertainties inherent in medical data, thus contributing to the interpretability of DNNs. Fuzzy logic, traditionally applied in decision-making contexts, offers a unique perspective on unraveling the black box of DNNs, providing a structured framework for capturing and explaining complex decision processes. Through a comprehensive exploration of techniques, we showcase the effectiveness of fuzzy logic as an additional layer of interpretability, complementing intrinsic, visualization, and distillation methods. Our survey contributes to a holistic understanding of explainable deep learning in healthcare, facilitating the seamless integration of DNNs into clinical workflows. By combining traditional methods with the novel inclusion of fuzzy logic, we aim to provide a nuanced and comprehensive view of interpretability techniques, advancing the transparency and trustworthiness of deep learning models in the healthcare landscape.","Unraveling the black box of deep learning healthcare techniques through explainable deep learning and fuzzy logic. The survey explores existing intrinsic, visualization, and distillation techniques, and introduces fuzzy logic as a novel category for addressing uncertainties in medical data."
"89-OR: Effective Real-World Usage of Artificial Intelligence–Based Decision Support System for Diabetes Management","https://scispace.com/paper/89-or-effective-real-world-usage-of-artificial-intelligence-c98ja1o6","2022","Journal Article","Diabetes","Revital Nimri
Amir Tirosh
Ido Muller
Yael Shtrit
Moshe Phillip","10.2337/db22-89-or","","Background: Artificial intelligence-based decision support system (AI-DSS) for frequent insulin dose adjustment was demonstrated to be effective in improving glycemic control in our previous clinical trials. We aimed to assess the effectiveness of AI-DSS in real world diabetes clinics.
 Methods: Data was collected and evaluated from routine AI-DSS usage for individuals with type 1 diabetes treated at 21 diabetes clinics (in the USA and 2 in Israel) . Two analyses were made: (1) Insulin dose recommendations were compared for rate of agreement/disagreement between those suggested by the AI-DSS and the approved recommendations by health care providers (HCP) . (2) Glycemic outcomes were compared between baseline and after 3 months of AI-DSS use. Included in this analysis were individuals who had mean baseline glucose level ≥ 182 mg/dL and at least one AI-DSS recommendation within the 3-months.
 Results: A total of 8 recommendations, provided to 370 individuals, were evaluated. Full agreement on the direction of insulin dose adjustments was observed in 87%, 87%, and 83% of the basal rate, carbs ratio (CR) , and correction factor (CF) pump settings parameters, respectively. Full disagreement on the direction of dose change was observed in only 0.9%, 0.5%, and 1.2% for the basal rate, CR and CF, respectively. Glycemic outcome analysis included 1 eligible individuals (mean of 1.6 recommendations within a 3-month interval) . Average sensor percentage of time in range [70-180 mg/dL] increased by 4% (p=0.003) : 43% increased time in range by more than 5% and 33% increased by 10% or more. Time in hyperglycemia [> 180 mg/dL] was reduced by 5% (p<0.01) while time in severe hypoglycemia [< 54 mg/dL] remained below 0.5%.
 Conclusion: A high rate of agreement with automated insulin adjustments was observed among HCPs at academic centers who used the AI-DSS in their workflow. Glycemic control was significantly improved for sub-optimally controlled individuals already after 3-months of follow up.
 
 
 R.Nimri: Employee; DreaMed Diabetes, Ltd., Research Support; Abbott Diabetes, Dexcom, Inc., Insulet Corporation, Medtronic, Speaker's Bureau; Eli Lilly and Company, Novo Nordisk, Stock/Shareholder; DreaMed Diabetes, Ltd. A.Tirosh: Advisory Panel; Abbott Diagnostics, AstraZeneca, Boehringer Ingelheim International GmbH, Merck & Co., Inc., Novo Nordisk, Sanofi, Consultant; Bayer AG, DreaMed Diabetes, Ltd., Research Support; Medtronic, Speaker's Bureau; Eli Lilly and Company. I.Muller: Board Member; DreaMed Diabetes, Ltd. Y.Shtrit: Employee; DreaMed Diabetes, Ltd. M.Phillip: Advisory Panel; Dompé, Insulet Corporation, Medtronic, Pfizer Inc., Board Member; DreaMed Diabetes, Ltd., Consultant; QuLab Medical Ltd., Other Relationship; Dexcom, Inc., Dompé, DreaMed Diabetes, Ltd., Eli Lilly and Company, Medtronic, NG Solutions Ltd, Novo Nordisk, OPKO, Pfizer Inc., Sanofi, Stock/Shareholder; DreaMed Diabetes, Ltd., NG Solutions Ltd.
","Glycemic control was significantly improved for sub-optimally controlled individuals already after 3-months of follow up and a high rate of agreement with automated insulin adjustments was observed among HCPs at academic centers who used the AI-DSS in their workflow."
"AI-based diabetes care: risk prediction models and implementation concerns","https://scispace.com/paper/ai-based-diabetes-care-risk-prediction-models-and-53lx211ct4","2024","","npj digital medicine","Serena C Y Wang
Grace Nickel
Kaushik P. Venkatesh
Marium M. Raza
Joseph C. Kvedar","10.1038/s41746-024-01034-7","","The utilization of artificial intelligence (AI) in diabetes care has focused on early intervention and treatment management. Notably, usage has expanded to predict an individual’s risk for developing type 2 diabetes. A scoping review of 40 studies by Mohsen et al. shows that while most studies used unimodal AI models, multimodal approaches were superior because they integrate multiple types of data. However, creating multimodal models and determining model performance are challenging tasks given the multi-factored nature of diabetes. For both unimodal and multimodal models, there are also concerns of bias with the lack of external validations and representation of race, age, and gender in training data. The barriers in data quality and evaluation standardization are ripe areas for developing new technologies, especially for entrepreneurs and innovators. Collaboration amongst providers, entrepreneurs, and researchers must be prioritized to ensure that AI in diabetes care is providing quality and equitable patient care. ","Collaboration amongst providers, entrepreneurs, and researchers must be prioritized to ensure that AI in diabetes care is providing quality and equitable patient care, especially for entrepreneurs and innovators."
"Clinical Decision Support for Diabetes Care in the Hospital: A Time for Change Toward Improvement of Management and Outcomes:","https://scispace.com/paper/clinical-decision-support-for-diabetes-care-in-the-hospital-43k3iz7nyj","2021","Journal Article","Journal of diabetes science and technology","Ariana Pichardo-Lowden","10.1177/1932296820982661","","The increasing prevalence of diabetes permeates hospitals and dysglycemia is associated with poor clinical and economic outcomes. Despite endorsed guidelines, barriers to optimal management and gaps in care prevail. Providers' limitations on knowledge, attitudes, and decision-making about hospital diabetes management are common. This adds to the complexity of dispersed glucose and insulin dosing data within medical records. This creates a dichotomy as safe and effective care are key objectives of healthcare organizations. This perspective highlights evidence of the benefits of clinical decision support (CDS) in hospital glycemic management. It elaborates on barriers CDS can help resolve, and factors driving its success. CDS represents a resource to individualize care and improve outcomes. It can help overcome a multifactorial problem impacting patients' lives on a daily basis.","In this article, the authors highlight evidence of the benefits of clinical decision support (CDS) in hospital glycemic management, and elaborates on barriers CDS can help resolve, and factors driving its success."
"Applications of Clinical Decision Support Systems (CDSSs) in Diabetes Care: A Scoping Review (Preprint)","https://scispace.com/paper/applications-of-clinical-decision-support-systems-cdsss-in-qretvu1h2d","2023","Journal Article","Journal of Medical Internet Research","Shan Huang
Yuzhen Liang
Jiarui Li
Xueqing Li","10.2196/51024","","Background Providing comprehensive and individualized diabetes care remains a significant challenge in the face of the increasing complexity of diabetes management and a lack of specialized endocrinologists to support diabetes care. Clinical decision support systems (CDSSs) are progressively being used to improve diabetes care, while many health care providers lack awareness and knowledge about CDSSs in diabetes care. A comprehensive analysis of the applications of CDSSs in diabetes care is still lacking. Objective This review aimed to summarize the research landscape, clinical applications, and impact on both patients and physicians of CDSSs in diabetes care. Methods We conducted a scoping review following the Arksey and O’Malley framework. A search was conducted in 7 electronic databases to identify the clinical applications of CDSSs in diabetes care up to June 30, 2022. Additional searches were conducted for conference abstracts from the period of 2021-2022. Two researchers independently performed the screening and data charting processes. Results Of 11,569 retrieved studies, 85 (0.7%) were included for analysis. Research interest is growing in this field, with 45 (53%) of the 85 studies published in the past 5 years. Among the 58 (68%) out of 85 studies disclosing the underlying decision-making mechanism, most CDSSs (44/58, 76%) were knowledge based, while the number of non-knowledge-based systems has been increasing in recent years. Among the 81 (95%) out of 85 studies disclosing application scenarios, the majority of CDSSs were used for treatment recommendation (63/81, 78%). Among the 39 (46%) out of 85 studies disclosing physician user types, primary care physicians (20/39, 51%) were the most common, followed by endocrinologists (15/39, 39%) and nonendocrinology specialists (8/39, 21%). CDSSs significantly improved patients’ blood glucose, blood pressure, and lipid profiles in 71% (45/63), 67% (12/18), and 38% (8/21) of the studies, respectively, with no increase in the risk of hypoglycemia. Conclusions CDSSs are both effective and safe in improving diabetes care, implying that they could be a potentially reliable assistant in diabetes care, especially for physicians with limited experience and patients with limited access to medical resources. International Registered Report Identifier (IRRID) RR2-10.37766/inplasy2022.9.0061","CDSSs are both effective and safe in improving diabetes care, implying that they could be a potentially reliable assistant in diabetes care, especially for physicians with limited experience and patients with limited access to medical resources."
"Explainable AI for Medical Data: Current Methods, Limitations, and Future Directions","https://scispace.com/paper/explainable-ai-for-medical-data-current-methods-limitations-4nb52ev149","2023","Journal Article","ACM Computing Surveys","Md Imran Hossain
Ghada Zamzmi
Peter R. Mouton
Md. Sirajus Salekin
Yu Sun
Dmitry Goldgof","10.1145/3637487","","With the power of parallel processing, large datasets,and fast computational resources, deep neural networks (DNNs) have outperformed highly trained and experienced human experts in medical applications. However, the large global community of healthcare professionals, many of whom routinely face potentially life-or-death outcomes with complex medicolegal consequences, have yet to embrace this powerful technology. The major problem is that most current AI solutions function as a metaphorical black-box positioned between input data and output decisions without a rigorous explanation for their internal processes. With the goal of enhancing trust and improving acceptance of AI-based technology in clinical medicine, there is a large and growing effort to address this challenge using eXplainable AI (XAI), a set of techniques, strategies, and algorithms with an explicit focus on explaining the “hows and whys” of DNNs. Here, we provide a comprehensive review of the state-of-the-art XAI techniques concerning healthcare applications and discuss current challenges and future directions. We emphasize the strengths and limitations of each category, including image, tabular, and textual explanations, and explore a range of evaluation metrics for assessing the effectiveness of XAI solutions. Finally, we highlight promising opportunities for XAI research to enhance the acceptance of DNNs by the healthcare community.","A comprehensive review of the state-of-the-art XAI techniques concerning healthcare applications, including image, tabular, and textual explanations, and a range of evaluation metrics for assessing the effectiveness of XAI solutions are provided."
"Examining explainable clinical decision support systems with think aloud protocols","https://scispace.com/paper/examining-explainable-clinical-decision-support-systems-with-ji07nobr8m","2023","Journal Article","PLOS ONE","Sabrina G Anjara
Adrianna Janik
Amy Dunford-Stenger
Kenneth Mc Kenzie
Ana Collazo-Lorduy
Maria Torrente
Luca Costabello
Mariano Provencio","10.1371/journal.pone.0291443","","Machine learning tools are increasingly used to improve the quality of care and the soundness of a treatment plan. Explainable AI (XAI) helps users in understanding the inner mechanisms of opaque machine learning models and is a driver of trust and adoption. Explanation methods for black-box models exist, but there is a lack of user studies on the interpretability of the provided explanations. We used a Think Aloud Protocol (TAP) to explore oncologists’ assessment of a lung cancer relapse prediction system with the aim of refining the purpose-built explanation model for better credibility and utility. Novel to this context, TAP is used as a neutral methodology to elicit experts’ thought processes and judgements of the AI system, without explicit prompts. TAP aims to elicit the factors which influenced clinicians’ perception of credibility and usefulness of the system. Ten oncologists took part in the study. We conducted a thematic analysis of their verbalized responses, generating five themes that help us to understand the context within which oncologists’ may (or may not) integrate an explainable AI system into their working day.","A Think Aloud Protocol is used as a neutral methodology to elicit experts’ thought processes and judgements of the AI system, without explicit prompts, to elicit the factors which influenced clinicians’ perception of credibility and usefulness of the system."
"Designing Interpretable ML System to Enhance Trust in Healthcare: A Systematic Review to Proposed Responsible Clinician-AI-Collaboration Framework","https://scispace.com/paper/designing-interpretable-ml-system-to-enhance-trust-in-3mhu4488g2","2024","Journal Article","Information Fusion","Elham Nasarian
Roohallah Alizadehsani
U. Acharya
Kwok-Leung Tsui","10.1016/j.inffus.2024.102412","","Artificial intelligence (AI)-based medical devices and digital health technologies, including medical sensors, wearable health trackers, telemedicine, mobile health (mHealth), large language models (LLMs), and digital care twins (DCTs), significantly influence the process of clinical decision support systems (CDSS) in healthcare and medical applications. However, given the complexity of medical decisions, it is crucial that results generated by AI tools not only be correct but also carefully evaluated, understandable, and explainable to end-users, especially clinicians. The lack of interpretability in communicating AI clinical decisions can lead to mistrust among decision-makers and a reluctance to use these technologies. This paper systematically reviews the processes and challenges associated with interpretable machine learning (IML) and explainable artificial intelligence (XAI) within the healthcare and medical domains. Its main goals are to examine the processes of IML and XAI, their related methods, applications, and the implementation challenges they pose in digital health interventions (DHIs), particularly from a quality control perspective, to help understand and improve communication between AI systems and clinicians. The IML process is categorized into pre-processing interpretability, interpretable modeling, and post-processing interpretability. This paper aims to foster a comprehensive understanding of the significance of a robust interpretability approach in clinical decision support systems (CDSS) by reviewing related experimental results. The goal is to provide future researchers with insights for creating clinician-AI tools that are more communicable in healthcare decision support systems and offer a deeper understanding of their challenges. Our research questions, eligibility criteria, and primary goals were proved using the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) guideline and the PICO (population, intervention, control, and outcomes) method. We systematically searched PubMed, Scopus, and Web of Science databases using sensitive and specific search strings. Subsequently, duplicate papers were removed using EndNote and Covidence. A two-phase selection process was then carried out on Covidence, starting with screening by title and abstract, followed by a full-text appraisal. The Meta Quality Appraisal Tool (MetaQAT) was used to assess the quality and risk of bias. Finally, a standardized data extraction tool was employed for reliable data mining. The searches yielded 2,241 records, from which 555 duplicate papers were removed. During the title and abstract screening step, 958 papers were excluded, and the full-text review step excluded 482 studies. Subsequently, in quality and risk of bias assessment, 172 papers were removed. 74 publications were selected for data extraction, which formed 10 insightful reviews and 64 related experimental studies. The paper provides general definitions of explainable artificial intelligence (XAI) in the medical domain and introduces a framework for interpretability in clinical decision support systems structured across three levels. It explores XAI-related health applications within each tier of this framework, underpinned by a review of related experimental findings. Furthermore, the paper engages in a detailed discussion of quality assessment tools for evaluating XAI in intelligent health systems. It also presents a step-by-step roadmap for implementing XAI in clinical settings. To direct future research toward bridging current gaps, the paper examines the importance of XAI models from various angles and acknowledges their limitations. ","This systematic review proposes a responsible clinician-AI-collaboration framework to enhance trust in healthcare, emphasizing the need for interpretable machine learning and explainable artificial intelligence in clinical decision support systems to improve communication and quality control."
"Artificial intelligence for diabetes care: current and future prospects","https://scispace.com/paper/artificial-intelligence-for-diabetes-care-current-and-future-14t86giivx","2024","Journal Article","The Lancet Diabetes & Endocrinology","Bin Sheng
Krithi Pushpanathan
Zhouyu Guan
QH Lim
Zhi Wei Lim
Samantha Min Er Yew
Jocelyn Hui Lin Goh
Yong Mong Bee
Charumathi Sabanayagam
Nick Sevdalis
Cynthia Ciwei Lim
Chwee Teck Lim
Jonathan Shaw
Weiping Jia
Elif I. Ekinci
Rafael Simó
Lee‐Ling Lim
Huating Li
Yih‐Chung Tham","10.1016/s2213-8587(24)00154-2","","Artificial intelligence (AI) use in diabetes care is increasingly being explored to personalise care for people with diabetes and adapt treatments for complex presentations. However, the rapid advancement of AI also introduces challenges such as potential biases, ethical considerations, and implementation challenges in ensuring that its deployment is equitable. Ensuring inclusive and ethical developments of AI technology can empower both health-care providers and people with diabetes in managing the condition. In this Review, we explore and summarise the current and future prospects of AI across the diabetes care continuum, from enhancing screening and diagnosis to optimising treatment and predicting and managing complications. ","The current and future prospects of AI across the diabetes care continuum are explored, from enhancing screening and diagnosis to optimising treatment and predicting and managing complications."
"EXplainable AI for Decision Support to Obesity Comorbidities Diagnosis","https://scispace.com/paper/explainable-ai-for-decision-support-to-obesity-comorbidities-5fe0nibz42","","Journal Article","IEEE Access","G. V. Aiosa
Maurizio Palesi
Francesca Sapuppo","10.1109/access.2023.3320057","","This paper describes the implementation of a comprehensive clinical decision support system (CDSS) for the risk factors prediction of comorbidities related to obesity and for the characterization of indirect connections between such comorbidities and non-communicable diseases. In particular, the direct correlation between obesity, diabetes, cardiovascular, and heart disease is analyzed by using machine learning (ML) predictive models, while the connection of the co-occurring disorders to the numerous additional non-communicable diseases is analyzed via a graph-based user interface. The CDSS here proposed is, therefore, structured with three main components: ML predictive models based on publicly available datasets, explainable artificial intelligence (XAI) local and global model interpretation, and graph-based representation of non-communicable disease connections. Multiple ML models are presented for risk assessment and a comparison is carried out based on performance key performance indicators. The best-performing model for each disease was proved to be: the multi-layer perceptron for diabetes and heart disease, and extreme gradient boosting for cardiovascular disease. Comorbidities risk factor prediction and a XAI local model explanation is performed on significant case studies. In addition, XAI global model interpretation is given for the entire dataset providing insights on the features’ contribution to the models’ implementation. Moreover, the graph-based visualization of indirect disease co-occurrence is performed by filtering connections according to different relative risk factor thresholds. This interface can be exploited by healthcare professionals to obtain, according to the needs and the clinical approach, a global perspective on obesity and its associated pathologies prevention as well as long-term treatment and care provision. ","The CDSS here proposed is structured with three main components: ML predictive models based on publicly available datasets, explainable artificial intelligence (XAI) local and global model interpretation, and graph-based representation of non-communicable disease connections."
"The Role of Explainability in Assuring Safety of Machine Learning in Healthcare","https://scispace.com/paper/the-role-of-explainability-in-assuring-safety-of-machine-3sgug62v","2022","Journal Article","IEEE Transactions on Emerging Topics in Computing","","10.1109/tetc.2022.3171314","","Established approaches to assuring safety-critical systems and software are difficult to apply to systems employing ML where there is no clear, pre-defined specification against which to assess validity. This problem is exacerbated by the “opaque” nature of ML where the learnt model is not amenable to human scrutiny. Explainable AI (XAI) methods have been proposed to tackle this issue by producing human-interpretable representations of ML models which can help users to gain confidence and build trust in the ML system. However, little work explicitly investigates the role of explainability for safety assurance in the context of ML development. This paper identifies ways in which XAI methods can contribute to safety assurance of ML-based systems. It then uses a concrete ML-based clinical decision support system, concerning weaning of patients from mechanical ventilation, to demonstrate how XAI methods can be employed to produce evidence to support safety assurance. The results are also represented in a safety argument to show where, and in what way, XAI methods can contribute to a safety case. Overall, we conclude that XAI methods have a valuable role in safety assurance of ML-based systems in healthcare but that they are not sufficient in themselves to assure safety. ","In this paper , explainable AI (XAI) methods have been proposed to produce human-interpretable representations of ML models which can help users to gain confidence and build trust in the ML system."
"Diabetes and artificial intelligence beyond the closed loop: a review of the landscape, promise and challenges.","https://scispace.com/paper/diabetes-and-artificial-intelligence-beyond-the-closed-loop-1yju2pdvuu","2023","","Diabetologia","Scott C. Mackenzie
Chris A R Sainsbury
Deborah J Wake","10.1007/s00125-023-06038-8","","Abstract The discourse amongst diabetes specialists and academics regarding technology and artificial intelligence (AI) typically centres around the 10% of people with diabetes who have type 1 diabetes, focusing on glucose sensors, insulin pumps and, increasingly, closed-loop systems. This focus is reflected in conference topics, strategy documents, technology appraisals and funding streams. What is often overlooked is the wider application of data and AI, as demonstrated through published literature and emerging marketplace products, that offers promising avenues for enhanced clinical care, health-service efficiency and cost-effectiveness. This review provides an overview of AI techniques and explores the use and potential of AI and data-driven systems in a broad context, covering all diabetes types, encompassing: (1) patient education and self-management; (2) clinical decision support systems and predictive analytics, including diagnostic support, treatment and screening advice, complications prediction; and (3) the use of multimodal data, such as imaging or genetic data. The review provides a perspective on how data- and AI-driven systems could transform diabetes care in the coming years and how they could be integrated into daily clinical practice. We discuss evidence for benefits and potential harms, and consider existing barriers to scalable adoption, including challenges related to data availability and exchange, health inequality, clinician hesitancy and regulation. Stakeholders, including clinicians, academics, commissioners, policymakers and those with lived experience, must proactively collaborate to realise the potential benefits that AI-supported diabetes care could bring, whilst mitigating risk and navigating the challenges along the way. Graphical Abstract ","This review provides an overview of AI techniques and explores the use and potential of AI and data-driven systems in a broad context, covering all diabetes types, encompassing: patient education and self-management; clinical decision support systems and predictive analytics; and the use of multimodal data."
"Leveraging explainable artificial intelligence to optimize clinical decision support.","https://scispace.com/paper/leveraging-explainable-artificial-intelligence-to-optimize-4mvfab9ggl","2024","Journal Article","Journal of the American Medical Informatics Association","Siru Liu
Allison B. McCoy
Josh F Peterson
Thomas A. Lasko
Dean F. Sittig
Scott D Nelson
Jennifer Andrews
Lorraine Patterson
Cheryl M Cobb
David Mulherin
Colleen T Morton
A. Wright","10.1093/jamia/ocae019","","OBJECTIVE
To develop and evaluate a data-driven process to generate suggestions for improving alert criteria using explainable artificial intelligence (XAI) approaches.


METHODS
We extracted data on alerts generated from January 1, 2019 to December 31, 2020, at Vanderbilt University Medical Center. We developed machine learning models to predict user responses to alerts. We applied XAI techniques to generate global explanations and local explanations. We evaluated the generated suggestions by comparing with alert's historical change logs and stakeholder interviews. Suggestions that either matched (or partially matched) changes already made to the alert or were considered clinically correct were classified as helpful.


RESULTS
The final dataset included 2 991 823 firings with 2689 features. Among the 5 machine learning models, the LightGBM model achieved the highest Area under the ROC Curve: 0.919 [0.918, 0.920]. We identified 96 helpful suggestions. A total of 278 807 firings (9.3%) could have been eliminated. Some of the suggestions also revealed workflow and education issues.


CONCLUSION
We developed a data-driven process to generate suggestions for improving alert criteria using XAI techniques. Our approach could identify improvements regarding clinical decision support (CDS) that might be overlooked or delayed in manual reviews. It also unveils a secondary purpose for the XAI: to improve quality by discovering scenarios where CDS alerts are not accepted due to workflow, education, or staffing issues.","The approach could identify improvements regarding clinical decision support (CDS) that might be overlooked or delayed in manual reviews and unveils a secondary purpose for the XAI: to improve quality by discovering scenarios where CDS alerts are not accepted due to workflow, education, or staffing issues."
"Designing explainable AI to improve human-AI team performance: A medical stakeholder-driven scoping review","https://scispace.com/paper/designing-explainable-ai-to-improve-human-ai-team-18y89kpem5","2024","Journal Article","Artificial Intelligence in Medicine","H. V. Subramanian
Casey Canfield
Daniel B. Shank","10.1016/j.artmed.2024.102780","","The rise of complex AI systems in healthcare and other sectors has led to a growing area of research called Explainable AI (XAI) designed to increase transparency. In this area, empirical and qualitative studies focus on improving user trust and task performance by providing system- and prediction-level XAI features. We analyze stakeholder engagement events (interviews and workshops) on the use of AI for kidney transplantation. From this we identify themes which we use to frame a scoping literature review on current XAI features. The stakeholder engagement process, including workshops and interviews, lasted over nine months covering three stakeholder group's workflows, determining where AI could intervene and assessing a mock XAI decision support system. Based on the stakeholder engagement, we identify four major themes relevant to designing XAI systems – 1) use of AI predictions, 2) information included in AI predictions, 3) personalization of AI predictions for individual differences, and 4) customizing AI predictions for specific cases. Using these themes, our scoping literature review finds that providing AI predictions before, during, or after decision-making could be beneficial depending on the complexity of the stakeholder's task. Additionally, expert stakeholders like surgeons prefer minimal to no XAI features, AI prediction, and uncertainty estimates for easy use cases. However, almost all stakeholders prefer to have optional XAI features to review when needed, especially in hard-to-predict cases. The literature also suggests that providing both system- and prediction-level information is necessary to build the user's mental model of the system appropriately. Although XAI features improve users' trust in the system, human-AI team performance is not always enhanced. Overall, stakeholders prefer to have agency over the XAI interface to control the level of information based on their needs and task complexity. We conclude with suggestions for future research, especially on customizing XAI features based on preferences and tasks. ","A medical stakeholder-driven scoping review identifies four themes for designing Explainable AI (XAI) systems: AI predictions, information inclusion, personalization, and customization. Stakeholders prefer optional XAI features and agency over the interface to enhance human-AI team performance."
"Intelligent Data-Driven Model for Diabetes Diurnal Patterns Analysis","https://scispace.com/paper/intelligent-data-driven-model-for-diabetes-diurnal-patterns-3tejua7lgt","2020","Journal Article","IEEE Journal of Biomedical and Health Informatics","Mohammad R. Eissa
Tim Good
Jackie Elliott
Mohammed Benaissa","10.1109/JBHI.2020.2975927","https://scispace.com/pdf/intelligent-data-driven-model-for-diabetes-diurnal-patterns-3tejua7lgt.pdf","In type 1 diabetes, diurnal activity routines are influential factors in insulin dose calculations. Bolus advisors have been developed to more accurately suggest doses of meal-related insulin based on carbohydrate intake, according to pre-set insulin to carbohydrate levels and insulin sensitivity factors. These parameters can be varied according to the time of day and their optimal setting relies on identifying the daily time periods of routines accurately. The main issues with reporting and adjustments of daily activity routines are the reliance on self-reporting which is prone to inaccuracy and within bolus calculators, the keeping of default settings for daily time periods, such as within insulin pumps, glucose meters, and mobile applications. Moreover, daily routines are subject to change over periods of time which could go unnoticed. Hence, forgetting to change the daily time periods in the bolus calculator could contribute to sub-optimal self-management. In this paper, these issues are addressed by proposing a data-driven model for identification of diabetes diurnal patterns based on self-monitoring data. The model uses time-series clustering to achieve a meaningful separation of the patterns which is then used to identify the daily time periods and to advise of any time changes required. Further improvements in bolus advisor settings are proposed to include week/weekend or even modifiable daily time settings. The proposed model provides a quick, granular, more accurate, and personalized daily time setting profile while providing a more contextual perspective to glycemic pattern identification to both patients and clinicians.","The proposed model provides a quick, granular, more accurate, and personalized daily time setting profile while providing a more contextual perspective to glycemic pattern identification to both patients and clinicians."
"Bringing Machine Learning Systems into Clinical Practice: A Design Science Approach to Explainable Machine Learning-Based Clinical Decision Support Systems","https://scispace.com/paper/bringing-machine-learning-systems-into-clinical-practice-a-jed8q2e8","","Journal Article","Journal of the Association for Information Systems","Luisa Pumplun
Felix Peters
Joshua Gawlitza
Peter Buxmann","10.17705/1jais.00820","","Clinical decision support systems (CDSSs) based on machine learning (ML) hold great promise for
improving medical care. Technically, such CDSSs are already feasible but physicians have been
skeptical about their application. In particular, their opacity is a major concern, as it may lead physicians
to overlook erroneous outputs from ML-based CDSSs, potentially causing serious consequences for
patients. Research on explainable AI (XAI) offers methods with the potential to increase the
explainability of black-box ML systems. This could significantly accelerate the application of MLbased CDSSs in medicine. However, XAI research to date has mainly been technically driven and
largely neglects the needs of end users. To better engage the users of ML-based CDSSs, we applied a
design science approach to develop a design for explainable ML-based CDSSs that incorporates
insights from XAI literature while simultaneously addressing physicians’ needs. This design comprises
five design principles that designers of ML-based CDSSs can apply to implement user-centered
explanations, which are instantiated in a prototype of an explainable ML-based CDSS for lung nodule
classification. We rooted the design principles and the derived prototype in a body of justificatory
knowledge consisting of XAI literature, the concept of usability, and an online survey study involving
57 physicians. We refined the design principles and their instantiation by conducting walk-throughs
with six radiologists. A final experiment with 45 radiologists demonstrated that our design resulted in
physicians perceiving the ML-based CDSS as more explainable and usable in terms of the required
cognitive effort than a system without explanations.","In this article , a design for explainable clinical decision support systems (CDSSs) based on machine learning (ML) is proposed to better engage the users of ML-based CDSS."
"Interpretable Prediction of Diabetes from Tabular Health Screening Records Using an Attentional Neural Network","https://scispace.com/paper/interpretable-prediction-of-diabetes-from-tabular-health-1aud29427c","2021","Proceedings Article","IEEE International Conference on Data Science and Advanced Analytics","Yuki Oba
Taro Tezuka
Masaru Sanuki
Yukiko Wagatsuma","10.1109/DSAA53316.2021.9564151","","Health screening is conducted in numerous countries to observe general health conditions. Machine learning has been applied to health screening records to predict asymptomatic patients' future medical states. However, for medical researchers and physicians, it is crucial to know why machine learning methods made such predictions to understand the underlying mechanism of the disease and prescribe treatments; therefore, predictions must be interpretable. We investigated the ability of an attentional neural network that processes tabular data, namely TabNet, to determine attributes that contribute to making predictions of the aggravation of type 2 diabetes. We used both model-agnostic and model-specific interpretation methods. For the former, we tested SHapley Additive exPlanations (SHAP). For the latter, we used model-specific feature importance and the mask in the attentive transformer of TabNet. We found that this mask provides useful information regarding which items in a biochemical analysis affect the aggravation of type 2 diabetes. The results from model-agnostic and model-specific methods were consistent.","In this paper, the ability of an attentional neural network that processes tabular data, namely TabNet, to determine attributes that contribute to making predictions of the aggravation of type 2 diabetes was investigated."
"A historical perspective of biomedical explainable AI research","https://scispace.com/paper/a-historical-perspective-of-biomedical-explainable-ai-5dk523dey8","2023","Journal Article","Patterns","","10.1016/j.patter.2023.100830","","The black-box nature of most artificial intelligence (AI) models encourages the development of explainability methods to engender trust into the AI decision-making process. Such methods can be broadly categorized into two main types: post hoc explanations and inherently interpretable algorithms. We aimed at analyzing the possible associations between COVID-19 and the push of explainable AI (XAI) to the forefront of biomedical research. We automatically extracted from the PubMed database biomedical XAI studies related to concepts of causality or explainability and manually labeled 1,603 papers with respect to XAI categories. To compare the trends pre- and post-COVID-19, we fit a change point detection model and evaluated significant changes in publication rates. We show that the advent of COVID-19 in the beginning of 2020 could be the driving factor behind an increased focus concerning XAI, playing a crucial role in accelerating an already evolving trend. Finally, we present a discussion with future societal use and impact of XAI technologies and potential future directions for those who pursue fostering clinical trust with interpretable machine learning models. ","It is shown that the advent of COVID-19 in the beginning of 2020 could be the driving factor behind an increased focus concerning XAI, playing a crucial role in accelerating an already evolving trend."
"Closing the Loop: Optimizing Diabetes Care in the Hospital by Addressing Dispersed Information in Electronic Health Records and Using Clinical Decision Support","https://scispace.com/paper/closing-the-loop-optimizing-diabetes-care-in-the-hospital-by-1o2jxfpfie","2019","Journal Article","Journal of diabetes science and technology","Ariana Pichardo-Lowden
Paul Haidet","10.1177/1932296818817005","https://journals.sagepub.com/doi/pdf/10.1177/1932296818817005","Multiple factors hinder the management of diabetes in hospitals. Amid the demands of practice, health care providers must collect, collate, and analyze multiple data points to optimally interpret g...","A conceptual, systems-based design is proposed for closing the loop between data gathering, analysis and decision making in the management of inpatient diabetes, which capitalizes on attributes of the EHR that can enable automated recognition of cases and provision of clinical recommendations."
"Subgoal-Based Explanations for Unreliable Intelligent Decision Support Systems","https://scispace.com/paper/subgoal-based-explanations-for-unreliable-intelligent-1evoybbt","2022","Proceedings Article","International Conference on Intelligent User Interfaces","Devleena Das
Been Kim
Sonia Chernova","10.1145/3581641.3584055","https://scispace.com/pdf/subgoal-based-explanations-for-unreliable-intelligent-1evoybbt.pdf","Intelligent decision support (IDS) systems leverage artificial intelligence techniques to generate recommendations that guide human users through the decision making phases of a task. However, a key challenge is that IDS systems are not perfect, and in complex real-world scenarios may produce suboptimal output or fail to work altogether. The field of explainable AI (XAI) has sought to develop techniques that improve the interpretability of black-box systems. While most XAI work has focused on single-classification tasks, the subfield of explainable AI planning (XAIP) has sought to develop techniques that make sequential decision making AI systems explainable to domain experts. Critically, prior work in applying XAIP techniques to IDS systems has assumed that the plan being proposed by the planner is always optimal, and therefore the action or plan being recommended as decision support to the user is always optimal. In this work, we examine novice user interactions with a non-robust IDS system – one that occasionally recommends suboptimal actions, and one that may become unavailable after users have become accustomed to its guidance. We introduce a new explanation type, subgoal-based explanations, for plan-based IDS systems, that supplements traditional IDS output with information about the subgoal toward which the recommended action would contribute. We demonstrate that subgoal-based explanations lead to improved user task performance in the presence of IDS recommendations, improve user ability to distinguish optimal and suboptimal IDS recommendations, and are preferred by users. Additionally, we demonstrate that subgoal-based explanations enable more robust user performance in the case of IDS failure, showing the significant benefit of training users for an underlying task with subgoal-based explanations.","It is demonstrated that subgoal-based explanations for plan-based IDS systems lead to improved user task performance, improve user ability to distinguish optimal and suboptimal IDS recommendations, are preferred by users, and enable more robust user performance in the case of IDS failure."
"The Methods and Approaches of Explainable Artificial Intelligence","https://scispace.com/paper/the-methods-and-approaches-of-explainable-artificial-3ib3raot1h","2021","Book Chapter","International Conference on Computational Science","Mateusz Szczepanski
Michał Choraś
Marek Pawlicki
Aleksandra Pawlicka","10.1007/978-3-030-77970-2_1","","Artificial Intelligence has found innumerable applications, becoming ubiquitous in the contemporary society. From making unnoticeable, minor choices to determining people’s fates (the case of predictive policing). This fact raises serious concerns about the lack of explainability of those systems. Finding ways to enable humans to comprehend the results provided by AI is a blooming area of research right now. This paper explores the current findings in the field of Explainable Artificial Intelligence (xAI), along with xAI methods and solutions that realise them. The paper provides an umbrella perspective on available xAI options, sorting them into a range of levels of abstraction, starting from community-developed code snippets implementing facets of xAI research all the way up to comprehensive solutions utilising state-of-the-art achievements in the domain.","In this paper, the authors explore the current findings in the field of explainable Artificial Intelligence (xAI), along with xAI methods and solutions that realize them. But they do not discuss how to find ways to enable humans to comprehend the results provided by AI."
"VBridge: Connecting the Dots Between Features and Data to Explain Healthcare Models","https://scispace.com/paper/vbridge-connecting-the-dots-between-features-and-data-to-3tbo5dw9","2022","Journal Article","IEEE Transactions on Visualization and Computer Graphics","","10.1109/tvcg.2021.3114836","https://scispace.com/pdf/vbridge-connecting-the-dots-between-features-and-data-to-3tbo5dw9.pdf","Machine learning (ML) is increasingly applied to Electronic Health Records (EHRs) to solve clinical prediction tasks. Although many ML models perform promisingly, issues with model transparency and interpretability limit their adoption in clinical practice. Directly using existing explainable ML techniques in clinical settings can be challenging. Through literature surveys and collaborations with six clinicians with an average of 17 years of clinical experience, we identified three key challenges, including clinicians' unfamiliarity with ML features, lack of contextual information, and the need for cohort-level evidence. Following an iterative design process, we further designed and developed VBridge, a visual analytics tool that seamlessly incorporates ML explanations into clinicians' decision-making workflow. The system includes a novel hierarchical display of contribution-based feature explanations and enriched interactions that connect the dots between ML features, explanations, and data. We demonstrated the effectiveness of VBridge through two case studies and expert interviews with four clinicians, showing that visually associating model explanations with patients' situational records can help clinicians better interpret and use model predictions when making clinician decisions. We further derived a list of design implications for developing future explainable ML tools to support clinical decision-making. ","VBridge as mentioned in this paper is a visual analytics tool that seamlessly incorporates ML explanations into clinicians' decision-making workflow, including a hierarchical display of contribution-based feature explanations and enriched interactions that connect the dots between ML features, explanations, and data."
"Explainable AI-driven IoMT fusion: Unravelling techniques, opportunities, and challenges with Explainable AI in healthcare","https://scispace.com/paper/explainable-ai-driven-iomt-fusion-unravelling-techniques-3fieocbo3e","2024","article","Information Fusion","","10.1016/j.inffus.2024.102472","","Artificial Intelligence (AI) has shown significant advancements across several industries, including healthcare, using better fusion methodologies, improved data accessibility, and heightened processing capabilities. Although deep learning algorithms excel in challenging scenarios, their widespread adoption presents a challenge due to their need for more transparency. The advent of Explainable AI (XAI) has addressed this limitation by offering AI-driven systems with attributes such as transparency, interpretability, and reliability. The need for transparency in critical sectors like healthcare has prompted an increased academic focus on investigating and understanding these frameworks. The present paper thoroughly examines the latest research and advancements in Explainable Artificial Intelligence (XAI), specifically focusing on its integration into the Internet of Medical Things (IoMT) in healthcare settings. The last section tackles outstanding issues concerning XAI and IoMT and outlines prospective directions for further research. A thorough investigation was carried out across many scholarly databases and online digital libraries, such as IEEE Xplore, ACM Digital Library, Wiley Interscience, Taylor and Francis, ScienceDirect, Scopus, Springer, Google Scholar, Citeseer Library, Semantic Scholar, and other relevant sources. An analysis was conducted on articles published from March 2004 to February 2024, specifically focusing on AI models that explain different healthcare issues. The search query included ""Explainable AI"" in conjunction with ""Open Black Box, Healthcare, Transparent Models, Interpretable Deep Learning, Machine Learning, Medical Information System, Accountability, Smart Healthcare, and Responsible AI"", indicating the field's dynamic nature. The authors also examined various techniques and datasets to elucidate healthcare difficulties, including incorporating the Internet of Medical Things (IoMT). The evaluation included more than 105 published models that used clinical data input to diagnose diverse disorders, focusing on incorporating Internet of Medical Things (IoMT) components. In addition to illness diagnosis, these models also elucidate the decisions made. The models were classified according to the input data, used methodologies, and integration of the Internet of Medical Things (IoMT). The categorization included machine learning and deep learning methodologies, particularly on explainability models connected to the Internet of Medical Things (IoMT). These models were classified as model-agnostic, model-specific, global, local, ante-hoc, and post-hoc. This extensive study examines machine learning, and deep learning models developed based on clinical data and associated with the Internet of Medical Things (IoMT) for illness detection. These models improve efficiency and accuracy and provide essential assistance to medical personnel. Explaining each choice made by individuals contributes to the acceleration of sickness detection, resulting in decreased medical expenses for patients and healthcare systems. The integration of Explainable Artificial Intelligence (XAI) and the Internet of Medical Things (IoMT) highlights a fundamental change in healthcare towards more transparent and linked systems, leading to significant breakthroughs in medical decision-making. ","This study examines Explainable AI (XAI) integration with Internet of Medical Things (IoMT) in healthcare, analyzing 105 models that diagnose diverse disorders using clinical data, and categorizing them based on methodologies and IoMT integration for improved efficiency and transparency."
"Artificial intelligence and machine learning for improving glycemic control in diabetes: best practices, pitfalls and opportunities","https://scispace.com/paper/artificial-intelligence-and-machine-learning-for-improving-52no2s7yey","2023","Journal Article","IEEE Reviews in Biomedical Engineering","Peter G. Jacobs
Pau Herrero
Andrea Facchinetti
Josep Vehı́
Boris Kovatchev
Marc D. Breton
Ali Çιnar
Konstantina S. Nikita
Francis J. Doyle
Jorge Bondía
Tadej Battelino
Jessica R. Castle
Konstantia Zarkogianni
Rahul Narayan
Clara Mosquera-Lopez","10.1109/rbme.2023.3331297","","<italic xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">Objective:</i> Artificial intelligence and machine learning are transforming many fields including medicine. In diabetes, robust biosensing technologies and automated insulin delivery therapies have created a substantial opportunity to improve health. While the number of manuscripts addressing the topic of applying machine learning to diabetes has grown in recent years, there has been a lack of consistency in the methods, metrics, and data used to train and evaluate these algorithms. This manuscript provides consensus guidelines for machine learning practitioners in the field of diabetes, including best practice recommended approaches and warnings about pitfalls to avoid. <italic xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">Methods:</i> Algorithmic approaches are reviewed and benefits of different algorithms are discussed including importance of clinical accuracy, explainability, interpretability, and personalization. We review the most common features used in machine learning applications in diabetes glucose control and provide an open-source library of functions for calculating features, as well as a framework for specifying data sets using data sheets. A review of current data sets available for training algorithms is provided as well as an online repository of data sources. <italic xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">Significance:</i> These consensus guidelines are designed to improve performance and translatability of new machine learning algorithms developed in the field of diabetes for engineers and data scientists. ","Machine learning and diabetes: consensus guidelines for improving glycemic control. Best practices, pitfalls and opportunities."
"Directive Explanations for Monitoring the Risk of Diabetes Onset: Introducing Directive Data-Centric Explanations and Combinations to Support What-If Explorations","https://scispace.com/paper/directive-explanations-for-monitoring-the-risk-of-diabetes-e2d8vzs2","2023","Proceedings Article","International Conference on Intelligent User Interfaces","Aditya Bhattacharya
Jeroen Ooge
Gregor Stiglic
Katrien Verbert","10.1145/3581641.3584075","https://scispace.compdf/directive-explanations-for-monitoring-the-risk-of-diabetes-e2d8vzs2.pdf","Explainable artificial intelligence is increasingly used in machine learning (ML) based decision-making systems in healthcare. However, little research has compared the utility of different explanation methods in guiding healthcare experts for patient care. Moreover, it is unclear how useful, understandable, actionable and trustworthy these methods are for healthcare experts, as they often require technical ML knowledge. This paper presents an explanation dashboard that predicts the risk of diabetes onset and explains those predictions with data-centric, feature-importance, and example-based explanations. We designed an interactive dashboard to assist healthcare experts, such as nurses and physicians, in monitoring the risk of diabetes onset and recommending measures to minimize risk. We conducted a qualitative study with 11 healthcare experts and a mixed-methods study with 45 healthcare experts and 51 diabetic patients to compare the different explanation methods in our dashboard in terms of understandability, usefulness, actionability, and trust. Results indicate that our participants preferred our representation of data-centric explanations that provide local explanations with a global overview over other methods. Therefore, this paper highlights the importance of visually directive data-centric explanation method for assisting healthcare experts to gain actionable insights from patient health records. Furthermore, we share our design implications for tailoring the visual representation of different explanation methods for healthcare experts.","In this paper , the authors present an explanation dashboard that predicts the risk of diabetes onset and explains those predictions with data-centric, feature-importance, and example-based explanations."
"Current Status and Future Directions for Electronic Point-of-Care Clinical Decision Support to Improve Diabetes Management in Primary Care.","https://scispace.com/paper/current-status-and-future-directions-for-electronic-point-of-4pz9cz2227","2019","Journal Article","Diabetes Technology & Therapeutics","Patrick J. O'Connor
JoAnn M. Sperl-Hillen","10.1089/DIA.2019.0070","","In the past decade there have been major improvements in the design, use, and effectiveness of point-of-care clinical decision support (CDS) systems to improve quality of care for patients with diabetes and related conditions. Advances in data exchange, data security, and human factors research have driven these improvements. Current diabetes CDS systems have high use rates, high clinician/user satisfaction rates, and have measurably improved glucose control, blood pressure control, and cardiovascular risk trajectories in adults with diabetes. As diabetes care increasingly relies on complex biomarker-driven risk prediction methods to optimize care goals and prioritize treatment options based on potential benefit to an individual patient, CDS systems will become indispensable tools to guide clinician and patient decision-making. In this study we describe specific challenges that must be addressed further to improve the design, implementation, and effectiveness of primary care diabetes CDS systems in coming years.","The design, use, and effectiveness of point-of-care clinical decision support systems to improve quality of care for patients and reduce unnecessary hospital admissions are major improvements."
"Explanatory pragmatism: a context-sensitive framework for explainable medical AI","https://scispace.com/paper/explanatory-pragmatism-a-context-sensitive-framework-for-1xd7qkzj","2022","Journal Article","Ethics and Information Technology","Rune Nyrup
Diana Robinson","10.1007/s10676-022-09632-3","https://scispace.com/pdf/explanatory-pragmatism-a-context-sensitive-framework-for-1xd7qkzj.pdf","Explainable artificial intelligence (XAI) is an emerging, multidisciplinary field of research that seeks to develop methods and tools for making AI systems more explainable or interpretable. XAI researchers increasingly recognise explainability as a context-, audience- and purpose-sensitive phenomenon, rather than a single well-defined property that can be directly measured and optimised. However, since there is currently no overarching definition of explainability, this poses a risk of miscommunication between the many different researchers within this multidisciplinary space. This is the problem we seek to address in this paper. We outline a framework, called Explanatory Pragmatism, which we argue has two attractive features. First, it allows us to conceptualise explainability in explicitly context-, audience- and purpose-relative terms, while retaining a unified underlying definition of explainability. Second, it makes visible any normative disagreements that may underpin conflicting claims about explainability regarding the purposes for which explanations are sought. Third, it allows us to distinguish several dimensions of AI explainability. We illustrate this framework by applying it to a case study involving a machine learning model for predicting whether patients suffering disorders of consciousness were likely to recover consciousness. ","The Explanatory Pragmatism framework as discussed by the authors proposes to conceptualise explainability in explicitly context-, audience- and purpose-relative terms, while retaining a unified underlying definition of explainability."
"Digital Diabetes Data and Artificial Intelligence: A Time for Humility Not Hubris:","https://scispace.com/paper/digital-diabetes-data-and-artificial-intelligence-a-time-for-3s7fo9al78","2019","Journal Article","Journal of diabetes science and technology","David Kerr
David C. Klonoff","10.1177/1932296818796508","https://journals.sagepub.com/doi/pdf/10.1177/1932296818796508","In the future artificial intelligence (AI) will have the potential to improve outcomes diabetes care. With the creation of new sensors for physiological monitoring sensors and the introduction of s...","In the future artificial intelligence (AI) will have the potential to improve outcomes diabetes care but decision-making processes based exclusively on quantitative metrics that ignore qualitative factors could create a quantitative fallacy."
"Healthcare AI Treatment Decision Support: Design Principles to Enhance Clinician Adoption and Trust","https://scispace.com/paper/healthcare-ai-treatment-decision-support-design-principles-2sr2daq0","2023","Proceedings Article","International Conference on Human Factors in Computing Systems","Eleanor R. Burgess
Ivana Jankovic
J. Marc Overhage
Erika Shehan Poole
Joseph 'Jofish' Kaye","10.1145/3544548.3581251","https://scispace.com/pdf/healthcare-ai-treatment-decision-support-design-principles-2sr2daq0.pdf","Artificial intelligence (AI) supported clinical decision support (CDS) technologies can parse vast quantities of patient data into meaningful insights for healthcare providers. Much work is underway to determine the technical feasibility and the accuracy of AI-driven insights. Much less is known about what insights are considered useful and actionable by healthcare providers, their trust in the insights, and clinical workflow integration challenges. Our research team used a conceptual prototype based on AI-generated treatment insights for type 2 diabetes medications to elicit feedback from 41 U.S.-based clinicians, including primary care and internal medicine physicians, endocrinologists, nurse practitioners, physician assistants, and pharmacists. We contribute to the human-computer interaction (HCI) community by describing decision optimization and design objective tensions between population-level and personalized insights, and patterns of use and trust of AI systems. We also contribute a set of 6 design principles for AI-supported CDS.","In this paper , the authors used a conceptual prototype based on AI-generated treatment insights for type 2 diabetes medications to elicit feedback from 41 U.S.-based clinicians, including primary care and internal medicine physicians, endocrinologists, nurse practitioners, physician assistants and pharmacists."
"Policy Implications of Artificial Intelligence and Machine Learning in Diabetes Management.","https://scispace.com/paper/policy-implications-of-artificial-intelligence-and-machine-zf5moulbkz","2020","Journal Article","Current Diabetes Reports","David T. Broome
C Beau Hilton
Neil Mehta","10.1007/S11892-020-1287-2","","Machine learning (ML) is increasingly being studied for the screening, diagnosis, and management of diabetes and its complications. Although various models of ML have been developed, most have not led to practical solutions for real-world problems. There has been a disconnect between ML developers, regulatory bodies, health services researchers, clinicians, and patients in their efforts. Our aim is to review the current status of ML in various aspects of diabetes care and identify key challenges that must be overcome to leverage ML to its full potential. ML has led to impressive progress in development of automated insulin delivery systems and diabetic retinopathy screening tools. Compared with these, use of ML in other aspects of diabetes is still at an early stage. The Food & Drug Administration (FDA) is adopting some innovative models to help bring technologies to the market in an expeditious and safe manner. ML has great potential in managing diabetes and the future is in furthering the partnership of regulatory bodies with health service researchers, clinicians, developers, and patients to improve the outcomes of populations and individual patients with diabetes.","Machine learning has great potential in managing diabetes and the future is in furthering the partnership of regulatory bodies with health service researchers, clinicians, developers, and patients to improve the outcomes of populations and individual patients with diabetes."
"A Provable Algorithm for Learning Interpretable Scoring Systems","https://scispace.com/paper/a-provable-algorithm-for-learning-interpretable-scoring-1a7fov78nb","2018","Proceedings Article","International Conference on Artificial Intelligence and Statistics","Nataliya Sokolovska
Yann Chevaleyre
Jean-Daniel Zucker","","https://scispace.com/pdf/a-provable-algorithm-for-learning-interpretable-scoring-1a7fov78nb.pdf","Score learning aims at taking advantage of supervised learning to produce interpretable models which facilitate decision making. Scoring systems are simple classification models that let users quickly perform stratification. Ideally, a scoring system is based on simple arithmetic operations, is sparse, and can be easily explained by human experts. In this contribution, we introduce an original methodology to simultaneously learn interpretable binning mapped to a class variable, and the weights associated with these bins contributing to the score. We develop and show the theoretical guarantees for the proposed method. We demonstrate by numerical experiments on benchmark data sets that our approach is competitive compared to the state-of-the-art methods. We illustrate by a real medical problem of type 2 diabetes remission prediction that a scoring system learned automatically purely from data is comparable to one manually constructed by clinicians.","This work introduces an original methodology to simultaneously learn interpretable binning mapped to a class variable, and the weights associated with these bins contributing to the score, and develops and shows the theoretical guarantees for this method."
"Explainability and Interpretability: Keys to Deep Medicine","https://scispace.com/paper/explainability-and-interpretability-keys-to-deep-medicine-1nlq6r7n00","2021","Book Chapter","National Conference on Artificial Intelligence","Arash Shaban-Nejad
Martin Michalowski
David L. Buckeridge","10.1007/978-3-030-53352-6_1","","Deep medicine, which aims to push the boundaries of artificial intelligence to reshape the health and medical intelligence and decision making, is a promising concept that is gaining attention over traditional EMR-based medical information management systems. The success of intelligent solutions in health and medicine depends on the degree to which they support interoperability, to allow consistent integration of different systems and data sources, and explainability, to make their decisions understandable, interpretable, and justifiable by humans.","The success of intelligent solutions in health and medicine depends on the degree to which they support interoperability, to allow consistent integration of different systems and data sources, and explainability to make their decisions understandable, interpretable, and justifiable by humans as discussed by the authors."
"A Blood Glucose Control Framework Based on Reinforcement Learning With Safety and Interpretability: In Silico Validation","https://scispace.com/paper/a-blood-glucose-control-framework-based-on-reinforcement-x6cqh76avu","2021","Journal Article","IEEE Access","Min Hyuk Lim
Woo Hyung Lee
Byoungjun Jeon
Sungwan Kim","10.1109/ACCESS.2021.3100007","https://scispace.com/pdf/a-blood-glucose-control-framework-based-on-reinforcement-x6cqh76avu.pdf","Controlling blood glucose levels in diabetic patients is important for managing their health and quality of life. Several algorithms based on model predictive control and reinforcement learning (RL) have been proposed so far, most of which use prior knowledge of physiological systems, the mathematical structure of blood glucose dynamics, and many episodes including failures for training the policy network in RL. To be smoothly adopted in clinical settings, we propose a fast online learning method underlining safety and interpretability. A random forest regressor and a dual attention network were exploited for glucose prediction and extension of state variables. The soft actor-critic network to determine insulin dosing was guided by proportional-integral-derivative (PID) control in the early phase, and an adaptive safe actor with suspension and additional insulin dosing was incorporated. The performance of the models was validated using an FDA-approved type 1 diabetes simulator. The results showed comparable outcomes with PID control. Using this system, glucose dynamics could be captured despite minimal prior knowledge. The extended state variables were correlated with basic states such as glucose, insulin, and meal intake, their derivatives, and their integrals, which can be fundamental elements of mathematical modeling of physiological responses. Attention scores and attribution scores in the prediction and control models represented the focused features and the internal operation of the models with interpretability. We expect this study to provide some insights on how RL can be practically adopted in clinical environments and how interpretability can provide hints of machines’ thoughts for clinical applications.","In this paper, a soft actor-critic network was used to determine insulin dosing in the early phase, and an adaptive safe actor with suspension and additional insulin dose was incorporated."
"A Review on Explainable Artificial Intelligence for Healthcare: Why, How, and When?","https://scispace.com/paper/a-review-on-explainable-artificial-intelligence-for-13qpycks","2023","Journal Article","IEEE transactions on artificial intelligence","Subrato Bharati
M. Rubaiyat Hossain Mondal
Prajoy Podder","10.1109/TAI.2023.3266418","https://scispace.compdf/a-review-on-explainable-artificial-intelligence-for-13qpycks.pdf","Artificial intelligence (AI) models are increasingly finding applications in the field of medicine. Concerns have been raised about the explainability of the decisions that are made by these AI models. In this article, we give a systematic analysis of explainable artificial intelligence (XAI), with a primary focus on models that are currently being used in the field of healthcare. The literature search is conducted following the preferred reporting items for systematic reviews and meta-analyses (PRISMA) standards for relevant work published from 1 January 2012 to 02 February 2022. The review analyzes the prevailing trends in XAI and lays out the major directions in which research is headed. We investigate the why, how, and when of the uses of these XAI models and their implications. We present a comprehensive examination of XAI methodologies as well as an explanation of how a trustworthy AI can be derived from describing AI models for healthcare fields. The discussion of this work will contribute to the formalization of the XAI field.","In this paper , a systematic analysis of explainable artificial intelligence (XAI), with a primary focus on models that are currently being used in the field of healthcare, is presented."
"Artificial intelligence and diabetes technology: A review.","https://scispace.com/paper/artificial-intelligence-and-diabetes-technology-a-review-3bqhlym2uw","2021","Journal Article","Metabolism-clinical and Experimental","Thibault Gautier
Leah B. Ziegler
Matthew S. Gerber
Enrique Campos-Náñez
Stephen D. Patek","10.1016/J.METABOL.2021.154872","","Artificial intelligence (AI) is widely discussed in the popular literature and is portrayed as impacting many aspects of human life, both in and out of the workplace. The potential for revolutionizing healthcare is significant because of the availability of increasingly powerful computational platforms and methods, along with increasingly informative sources of patient data, both in and out of clinical settings. This review aims to provide a realistic assessment of the potential for AI in understanding and managing diabetes, accounting for the state of the art in the methodology and medical devices that collect data, process data, and act accordingly. Acknowledging that many conflicting definitions of AI have been put forth, this article attempts to characterize the main elements of the field as they relate to diabetes, identifying the main perspectives and methods that can (i) affect basic understanding of the disease, (ii) affect understanding of risk factors (genetic, clinical, and behavioral) of diabetes development, (iii) improve diagnosis, (iv) improve understanding of the arc of disease (progression and personal/societal impact), and finally (v) improve treatment.","A review of the potential for AI in understanding and managing diabetes, accounting for the state of the art in the methodology and medical devices that collect data, process data, and act accordingly, is presented in this article."
"Leveraging Artificial Intelligence to Improve Chronic Disease Care: Methods and Application to Pharmacotherapy Decision Support for Type-2 Diabetes Mellitus","https://scispace.com/paper/leveraging-artificial-intelligence-to-improve-chronic-375fs485yw","2021","Journal Article","Methods of Information in Medicine","Shinji Tarumi
Takeuchi Wataru
George Chalkidis
Salvador Rodriguez-Loya
Junichi Kuwata
Michael Flynn
Kyle Turner
Farrant Sakaguchi
Charlene R. Weir
Heidi Kramer
David Shields
Phillip B. Warner
Polina V. Kukhareva
Hideyuki Ban
Kensaku Kawamoto","10.1055/S-0041-1728757","https://scispace.com/pdf/leveraging-artificial-intelligence-to-improve-chronic-375fs485yw.pdf","Objectives Artificial intelligence (AI), including predictive analytics, has great potential to improve the care of common chronic conditions with high morbidity and mortality. However, there are still many challenges to achieving this vision. The goal of this project was to develop and apply methods for enhancing chronic disease care using AI. Methods Using a dataset of 27,904 patients with diabetes, an analytical method was developed and validated for generating a treatment pathway graph which consists of models that predict the likelihood of alternate treatment strategies achieving care goals. An AI-driven clinical decision support system (CDSS) integrated with the electronic health record (EHR) was developed by encapsulating the prediction models in an OpenCDS Web service module and delivering the model outputs through a SMART on FHIR (Substitutable Medical Applications and Reusable Technologies on Fast Healthcare Interoperability Resources) web-based dashboard. This CDSS enables clinicians and patients to review relevant patient parameters, select treatment goals, and review alternate treatment strategies based on prediction results. Results The proposed analytical method outperformed previous machine-learning algorithms on prediction accuracy. The CDSS was successfully integrated with the Epic EHR at the University of Utah. Conclusion A predictive analytics-based CDSS was developed and successfully integrated with the EHR through standards-based interoperability frameworks. The approach used could potentially be applied to many other chronic conditions to bring AI-driven CDSS to the point of care.","In this article, an AI-driven clinical decision support system (CDSS) integrated with the electronic health record (EHR) was developed by encapsulating the prediction models in an OpenCDS Web service module and delivering the model outputs through a SMART on FHIR (Substitutable Medical Applications and Reusable Technologies on Fast Healthcare Interoperability Resources) web-based dashboard."
"Decision Support in Diabetes Care: The Challenge of Supporting Patients in Their Daily Living Using a Mobile Glucose Predictor.","https://scispace.com/paper/decision-support-in-diabetes-care-the-challenge-of-3r9aouy1vz","2018","Journal Article","Journal of diabetes science and technology","Carmen Pérez-Gandía
Gema García-Sáez
David Subias
Agustín Rodríguez-Herrero
Enrique J. Gómez
Mercedes Rigla
M. Elena Hernando","10.1177/1932296818761457","https://journals.sagepub.com/doi/pdf/10.1177/1932296818761457","Background:In type 1 diabetes mellitus (T1DM), patients play an active role in their own care and need to have the knowledge to adapt decisions to their daily living conditions. Artificial intelligence applications can help people with type 1 diabetes in decision making and allow them to react at time scales shorter than the scheduled face-to-face visits. This work presents a decision support system (DSS), based on glucose prediction, to assist patients in a mobile environment.Methods:The system’s impact on therapeutic corrective actions has been evaluated in a randomized crossover pilot study focused on interprandial periods. Twelve people with type 1 diabetes treated with insulin pump participated in two phases: In the experimental phase (EP) patients used the DSS to modify initial corrective decisions in presence of hypoglycemia or hyperglycemia events. In the control phase (CP) patients were asked to follow decisions without knowing the glucose prediction. A telemedicine platform allowed participants ...","The decision support system (DSS), based on glucose prediction, had a relevant impact in the participants’ decision making while dealing with T1DM and showed a high confidence of patients in the use of glucose prediction."
"Artificial intelligence in diabetes care","https://scispace.com/paper/artificial-intelligence-in-diabetes-care-1ea09vyi3i","2018","Journal Article","Diabetic Medicine","V Buch
G Varughese
M Maruthappu","10.1111/DME.13587","https://onlinelibrary.wiley.com/doi/pdf/10.1111/dme.13587","Medical artificial intelligence (AI) is moving forward at considerable pace. Promising research ideas are surfacing in clinical waters; AI is automating the national 111 triage service [1] and has exhibited dermatologist-level performance at identifying suspicious skin lesions, a task where experts frequently disagree [2]. The present article explores how machine learning, a prominent branch of AI, may be set to transform diabetes care.","How machine learning, a prominent branch of AI, may be set to transform diabetes care is explored."
"A Survey on Medical Explainable AI (XAI): Recent Progress, Explainability Approach, Human Interaction and Scoring System","https://scispace.com/paper/a-survey-on-medical-explainable-ai-xai-recent-progress-16320cex","2022","Journal Article","Sensors","Mayuresh Sunil Pardeshi","10.3390/s22208068","https://scispace.com/pdf/a-survey-on-medical-explainable-ai-xai-recent-progress-16320cex.pdf","The emerging field of eXplainable AI (XAI) in the medical domain is considered to be of utmost importance. Meanwhile, incorporating explanations in the medical domain with respect to legal and ethical AI is necessary to understand detailed decisions, results, and current status of the patient's conditions. Successively, we will be presenting a detailed survey for the medical XAI with the model enhancements, evaluation methods, significant overview of case studies with open box architecture, medical open datasets, and future improvements. Potential differences in AI and XAI methods are provided with the recent XAI methods stated as (i) local and global methods for preprocessing, (ii) knowledge base and distillation algorithms, and (iii) interpretable machine learning. XAI characteristics details with future healthcare explainability is included prominently, whereas the pre-requisite provides insights for the brainstorming sessions before beginning a medical XAI project. Practical case study determines the recent XAI progress leading to the advance developments within the medical field. Ultimately, this survey proposes critical ideas surrounding a user-in-the-loop approach, with an emphasis on human-machine collaboration, to better produce explainable solutions. The surrounding details of the XAI feedback system for human rating-based evaluation provides intelligible insights into a constructive method to produce human enforced explanation feedback. For a long time, XAI limitations of the ratings, scores and grading are present. Therefore, a novel XAI recommendation system and XAI scoring system are designed and approached from this work. Additionally, this paper encourages the importance of implementing explainable solutions into the high impact medical field. ","In this paper , a detailed survey of the medical XAI with model enhancements, evaluation methods, significant overview of case studies with open box architecture, medical open datasets, and future improvements is presented."
"Guest Editorial Explainable AI: Towards Fairness, Accountability, Transparency and Trust in Healthcare","https://scispace.com/paper/guest-editorial-explainable-ai-towards-fairness-3ex190ktaj","2021","Journal Article","IEEE Journal of Biomedical and Health Informatics","Arash Shaban-Nejad
Martin Michalowski
John S. Brownstein
David L. Buckeridge","10.1109/JBHI.2021.3088832","https://scispace.com/pdf/guest-editorial-explainable-ai-towards-fairness-3ex190ktaj.pdf","The papers in this special section focus on explainable artificial intelligence (AI) in healthcare services. Recent advances in AI, precision health, and medicine have paved the way for the accelerated adaptation and use of intelligent tools and systems in decision-making processes across the healthcare spectrum. Insights and knowledge derived from complex analytics are used to implement diagnostic and therapeutic solutions and targeted interventions in individuals and communities across the globe. Given the complexity of the current multi-dimensional clinical and public health data landscape, providing explainability in the context of socio-environmental and technical systems is a key to revealing pathways from socio-economic disadvantages to health disparities and implementing equitable interventions. As the complexity of the underlying data sets and AI-based algorithms increases, the explainability and justifiability of the insights generated decrease. Humans need to understand the underlying mechanism behind these insights to know whether they are sound, correct, trustable, and justifiable to make informed decisions. Lack of understandability and explainability in the biomedical domain often leads to poor transparency and accountability and ultimately lower quality of care and suboptimal and unfair health policies. Explainability is considered one of the prerequisites for deep medicine, where AI is meant to provide composite, panoramic views of individuals’ medical data.","In this paper, explainability is considered one of the prerequisites for deep medicine, where AI is meant to provide composite, panoramic views of individuals' medical data, and humans need to understand the underlying mechanism behind these insights to know whether they are sound, correct, trustable and justifiable to make informed decisions."
"Artificial intelligence in diabetes management: Advancements, opportunities, and challenges.","https://scispace.com/paper/artificial-intelligence-in-diabetes-management-advancements-35se3f17gj","2023","","Cell reports medicine","Zhouyu Guan
Huating Li
Ruhan Liu
Chun Cai
Yuexing Liu
Jiajia Li
Xiangning Wang
Shan Huang
Liang Wu
Dan Liu
Shujie Yu
Zheyuan Wang
Jia Shu
Xuhong Hou
Xiaokang Yang
Weiping Jia
Bin Sheng","10.1016/j.xcrm.2023.101213","","The increasing prevalence of diabetes, high avoidable morbidity and mortality due to diabetes and diabetic complications, and related substantial economic burden make diabetes a significant health challenge worldwide. A shortage of diabetes specialists, uneven distribution of medical resources, low adherence to medications, and improper self-management contribute to poor glycemic control in patients with diabetes. Recent advancements in digital health technologies, especially artificial intelligence (AI), provide a significant opportunity to achieve better efficiency in diabetes care, which may diminish the increase in diabetes-related health-care expenditures. Here, we review the recent progress in the application of AI in the management of diabetes and then discuss the opportunities and challenges of AI application in clinical practice. Furthermore, we explore the possibility of combining and expanding upon existing digital health technologies to develop an AI-assisted digital health-care ecosystem that includes the prevention and management of diabetes. ","Recent progress in the application of AI in the management of diabetes is reviewed and the opportunities and challenges of AI application in clinical practice are discussed and the possibility of combining and expanding upon existing digital health technologies to develop an AI-assisted digital health-care ecosystem that includes the prevention and management ofabetes is explored."
"A Review on Explainable Artificial Intelligence for Healthcare: Why, How, and When?","https://scispace.com/paper/a-review-on-explainable-artificial-intelligence-for-2ggw4exo","2023","Journal Article","IEEE transactions on artificial intelligence","","10.1109/tai.2023.3266418","https://scispace.com/pdf/a-review-on-explainable-artificial-intelligence-for-2ggw4exo.pdf","Artificial intelligence (AI) models are increasingly finding applications in the field of medicine. Concerns have been raised about the explainability of the decisions that are made by these AI models. In this article, we give a systematic analysis of explainable artificial intelligence (XAI), with a primary focus on models that are currently being used in the field of healthcare. The literature search is conducted following the preferred reporting items for systematic reviews and meta-analyses (PRISMA) standards for relevant work published from 1 January 2012 to 02 February 2022. The review analyzes the prevailing trends in XAI and lays out the major directions in which research is headed. We investigate the why, how, and when of the uses of these XAI models and their implications. We present a comprehensive examination of XAI methodologies as well as an explanation of how a trustworthy AI can be derived from describing AI models for healthcare fields. The discussion of this work will contribute to the formalization of the XAI field. ","In this paper , a systematic analysis of explainable artificial intelligence (XAI), with a primary focus on models that are currently being used in the field of healthcare, is presented."
"FairLens: Auditing black-box clinical decision support systems","https://scispace.com/paper/fairlens-auditing-black-box-clinical-decision-support-2pm5vowtot","2021","Journal Article","Information Processing and Management","Cecilia Panigutti
Alan Perotti
André Panisson
Paolo Bajardi
Dino Pedreschi","10.1016/J.IPM.2021.102657","https://arxiv.org/pdf/2011.04049","The pervasive application of algorithmic decision-making is raising concerns on the risk of unintended bias in AI systems deployed in critical settings such as healthcare. The detection and mitigation of model bias is a very delicate task that should be tackled with care and involving domain experts in the loop. In this paper we introduce FairLens, a methodology for discovering and explaining biases. We show how this tool can audit a fictional commercial black-box model acting as a clinical decision support system (DSS). In this scenario, the healthcare facility experts can use FairLens on their historical data to discover the biases of the model before incorporating it into the clinical decision flow. FairLens first stratifies the available patient data according to demographic attributes such as age, ethnicity, gender and healthcare insurance; it then assesses the model performance on such groups highlighting the most common misclassifications. Finally, FairLens allows the expert to examine one misclassification of interest by explaining which elements of the affected patients’ clinical history drive the model error in the problematic group. We validate FairLens’ ability to highlight bias in multilabel clinical DSSs introducing a multilabel-appropriate metric of disparity and proving its efficacy against other standard metrics.","In this paper, the authors introduce FairLens, a methodology for discovering and explaining biases in a clinical decision support system (DSS), which can audit a fictional commercial black-box model acting as a clinical DSS."
"Application of Artificial Intelligence in Diabetes Education and Management: Present Status and Promising Prospect","https://scispace.com/paper/application-of-artificial-intelligence-in-diabetes-education-zs7f48uc0s","2020","Journal Article","Frontiers in Public Health","Juan Li
Jin Huang
Lanbo Zheng
Xia Li","10.3389/FPUBH.2020.00173","https://scispace.com/pdf/application-of-artificial-intelligence-in-diabetes-education-zs7f48uc0s.pdf","Despite the rapid development of science and technology in healthcare, diabetes remains an incurable lifelong illness. Diabetes education aiming to improve the self-management skills is an essential way to help patients enhance their metabolic control and quality of life. Artificial intelligence (AI) technologies have made significant progress in transforming available genetic data and clinical information into valuable knowledge. The application of AI tech in disease education would be extremely beneficial considering their advantages in promoting individualization and full-course education intervention according to the unique pictures of different individuals. This paper reviews and discusses the most recent applications of AI techniques to various aspects of diabetes education. With the information and evidence collected, this review attempts to provide insight and guidance for the development of prospective, data-driven decision support platforms for diabetes management, with a focus on individualized patient management and lifelong educational interventions.","This paper reviews and discusses the most recent applications of AI techniques to various aspects of diabetes education and attempts to provide insight and guidance for the development of prospective, data-driven decision support platforms for diabetes management, with a focus on individualized patient management and lifelong educational interventions."
"Assessing the communication gap between AI models and healthcare professionals: explainability, utility and trust in AI-driven clinical decision-making","https://scispace.com/paper/assessing-the-communication-gap-between-ai-models-and-2ludyioj","2022","Journal Article","Artificial intelligence","Oskar Wysocki
Jessica Davies
Markel Vigo
Anne Caroline Armstrong
Donal Landers
Rebecca Lee
André Freitas","10.1016/j.artint.2022.103839","","This paper contributes with a pragmatic evaluation framework for explainable Machine Learning (ML) models for clinical decision support. The study revealed a more nuanced role for ML explanation models, when these are pragmatically embedded in the clinical context. Despite the general positive attitude of healthcare professionals (HCPs) towards explanations as a safety and trust mechanism, for a significant set of participants there were negative effects associated with confirmation bias, accentuating model over-reliance and increased effort to interact with the model. Also, contradicting one of its main intended functions, standard explanatory models showed limited ability to support a critical understanding of the limitations of the model. However, we found new significant positive effects which repositions the role of explanations within a clinical context: these include reduction of automation bias, addressing ambiguous clinical cases (cases where HCPs were not certain about their decision) and support of less experienced HCPs in the acquisition of new domain knowledge. ","In this paper , a pragmatic evaluation framework for explainable Machine Learning (ML) models for clinical decision support is presented, and the authors reveal a more nuanced role for ML explanation models, when these are pragmatically embedded in the clinical context."
"The enlightening role of explainable artificial intelligence in medical & healthcare domains: A systematic literature review.","https://scispace.com/paper/the-enlightening-role-of-explainable-artificial-intelligence-1uwfgcao72","2023","","Computers in Biology and Medicine","Subhan Ali
Filza Akhlaq
Ali Imran
Zenun Kastrati
Sher Muhammad Daudpota
Muhammad Moosa","10.1016/j.compbiomed.2023.107555","","In domains such as medical and healthcare, the interpretability and explainability of machine learning and artificial intelligence systems are crucial for building trust in their results. Errors caused by these systems, such as incorrect diagnoses or treatments, can have severe and even life-threatening consequences for patients. To address this issue, Explainable Artificial Intelligence (XAI) has emerged as a popular area of research, focused on understanding the black-box nature of complex and hard-to-interpret machine learning models. While humans can increase the accuracy of these models through technical expertise, understanding how these models actually function during training can be difficult or even impossible. XAI algorithms such as Local Interpretable Model-Agnostic Explanations (LIME) and SHapley Additive exPlanations (SHAP) can provide explanations for these models, improving trust in their predictions by providing feature importance and increasing confidence in the systems. Many articles have been published that propose solutions to medical problems by using machine learning models alongside XAI algorithms to provide interpretability and explainability. In our study, we identified 454 articles published from 2018-2022 and analyzed 93 of them to explore the use of these techniques in the medical domain. ","This study identified 454 articles published from 2018-2022 and analyzed 93 of them to explore the use of XAI techniques in the medical domain, finding that many articles have been published that propose solutions to medical problems by using machine learning models alongside XAI algorithms to provide interpretability and explainability."
"Re-focusing explainability in medicine","https://scispace.com/paper/re-focusing-explainability-in-medicine-kxs2dmqn","2022","Journal Article","Digital health","Laura Arbelaez Ossa
Georg Starke
Giorgia Lorenzini
Julia E. Vogt
David Shaw
Bernice Simone Elger","10.1177/20552076221074488","https://journals.sagepub.com/doi/pdf/10.1177/20552076221074488","Using artificial intelligence to improve patient care is a cutting-edge methodology, but its implementation in clinical routine has been limited due to significant concerns about understanding its behavior. One major barrier is the explainability dilemma and how much explanation is required to use artificial intelligence safely in healthcare. A key issue is the lack of consensus on the definition of explainability by experts, regulators, and healthcare professionals, resulting in a wide variety of terminology and expectations. This paper aims to fill the gap by defining minimal explainability standards to serve the views and needs of essential stakeholders in healthcare. In that sense, we propose to define minimal explainability criteria that can support doctors’ understanding, meet patients’ needs, and fulfill legal requirements. Therefore, explainability need not to be exhaustive but sufficient for doctors and patients to comprehend the artificial intelligence models’ clinical implications and be integrated safely into clinical practice. Thus, minimally acceptable standards for explainability are context-dependent and should respond to the specific need and potential risks of each clinical scenario for a responsible and ethical implementation of artificial intelligence.","The proposed minimal explainability criteria can support doctors’ understanding, meet patients’ needs, and fulfill legal requirements and should respond to the specific need and potential risks of each clinical scenario for a responsible and ethical implementation of artificial intelligence."
"Ada-WHIPS: explaining AdaBoost classification with applications in the health sciences","https://scispace.com/paper/ada-whips-explaining-adaboost-classification-with-4mknlki58m","2020","Journal Article","BMC Medical Informatics and Decision Making","Julian Hatwell
Mohamed Medhat Gaber
R. Muhammad Atif Azad","10.1186/S12911-020-01201-2","https://scispace.com/pdf/ada-whips-explaining-adaboost-classification-with-4mknlki58m.pdf","Computer Aided Diagnostics (CAD) can support medical practitioners to make critical decisions about their patients’ disease conditions. Practitioners require access to the chain of reasoning behind CAD to build trust in the CAD advice and to supplement their own expertise. Yet, CAD systems might be based on black box machine learning models and high dimensional data sources such as electronic health records, magnetic resonance imaging scans, cardiotocograms, etc. These foundations make interpretation and explanation of the CAD advice very challenging. This challenge is recognised throughout the machine learning research community. eXplainable Artificial Intelligence (XAI) is emerging as one of the most important research areas of recent years because it addresses the interpretability and trust concerns of critical decision makers, including those in clinical and medical practice. In this work, we focus on AdaBoost, a black box model that has been widely adopted in the CAD literature. We address the challenge – to explain AdaBoost classification – with a novel algorithm that extracts simple, logical rules from AdaBoost models. Our algorithm, Adaptive-Weighted High Importance Path Snippets (Ada-WHIPS), makes use of AdaBoost’s adaptive classifier weights. Using a novel formulation, Ada-WHIPS uniquely redistributes the weights among individual decision nodes of the internal decision trees of the AdaBoost model. Then, a simple heuristic search of the weighted nodes finds a single rule that dominated the model’s decision. We compare the explanations generated by our novel approach with the state of the art in an experimental study. We evaluate the derived explanations with simple statistical tests of well-known quality measures, precision and coverage, and a novel measure stability that is better suited to the XAI setting. Experiments on 9 CAD-related data sets showed that Ada-WHIPS explanations consistently generalise better (mean coverage 15%-68%) than the state of the art while remaining competitive for specificity (mean precision 80%-99%). A very small trade-off in specificity is shown to guard against over-fitting which is a known problem in the state of the art methods. The experimental results demonstrate the benefits of using our novel algorithm for explaining CAD AdaBoost classifiers widely found in the literature. Our tightly coupled, AdaBoost-specific approach outperforms model-agnostic explanation methods and should be considered by practitioners looking for an XAI solution for this class of models.","This work addresses the challenge to explain AdaBoost classification with a novel algorithm that extracts simple, logical rules from AdaBoost models with results that demonstrate the benefits of using this novel algorithm for explaining CAD AdaBoost classifiers widely found in the literature."
"A survey on the interpretability of deep learning in medical diagnosis","https://scispace.com/paper/a-survey-on-the-interpretability-of-deep-learning-in-medical-3i349qpy","2022","Journal Article","Multimedia Systems","Qiaoying Teng
Zhe Liu
Yuqing Song
Kyungdo Han
Yang Lu","10.1007/s00530-022-00960-4","https://scispace.com/pdf/a-survey-on-the-interpretability-of-deep-learning-in-medical-3i349qpy.pdf","Deep learning has demonstrated remarkable performance in the medical domain, with accuracy that rivals or even exceeds that of human experts. However, it has a significant problem that these models are “black-box” structures, which means they are opaque, non-intuitive, and difficult for people to understand. This creates a barrier to the application of deep learning models in clinical practice due to lack of interpretability, trust, and transparency. To overcome this problem, several studies on interpretability have been proposed. Therefore, in this paper, we comprehensively review the interpretability of deep learning in medical diagnosis based on the current literature, including some common interpretability methods used in the medical domain, various applications with interpretability for disease diagnosis, prevalent evaluation metrics, and several disease datasets. In addition, the challenges of interpretability and future research directions are also discussed here. To the best of our knowledge, this is the first time that various applications of interpretability methods for disease diagnosis have been summarized. ","In this paper , the authors comprehensively review the interpretability of deep learning in medical diagnosis based on the current literature, including some common interpretability methods used in the medical domain, various applications with interpretability for disease diagnosis, prevalent evaluation metrics, and several disease datasets."
"Explainable Artificial Intelligence Methods in Combating Pandemics: A Systematic Review","https://scispace.com/paper/explainable-artificial-intelligence-methods-in-combating-gt9ot33c","2023","Journal Article","IEEE Reviews in Biomedical Engineering","","10.1109/rbme.2022.3185953","","Despite the myriad peer-reviewed papers demonstrating novel Artificial Intelligence (AI)-based solutions to COVID-19 challenges during the pandemic, few have made a significant clinical impact, especially in diagnosis and disease precision staging. One major cause for such low impact is the lack of model transparency, significantly limiting the AI adoption in real clinical practice. To solve this problem, AI models need to be explained to users. Thus, we have conducted a comprehensive study of Explainable Artificial Intelligence (XAI) using PRISMA technology. Our findings suggest that XAI can improve model performance, instill trust in the users, and assist users in decision-making. In this systematic review, we introduce common XAI techniques and their utility with specific examples of their application. We discuss the evaluation of XAI results because it is an important step for maximizing the value of AI-based clinical decision support systems. Additionally, we present the traditional, modern, and advanced XAI models to demonstrate the evolution of novel techniques. Finally, we provide a best practice guideline that developers can refer to during the model experimentation. We also offer potential solutions with specific examples for common challenges in AI model experimentation. This comprehensive review, hopefully, can promote AI adoption in biomedicine and healthcare. ","In this paper , explainable artificial intelligence (XAI) is used to improve model performance, instill trust in the users, and assist users in decision-making in clinical decision support systems."
"Artificial Intelligence in Decision Support Systems for Type 1 Diabetes.","https://scispace.com/paper/artificial-intelligence-in-decision-support-systems-for-type-ckzu3slsqy","2020","Journal Article","Sensors","Nichole S. Tyler
Peter G. Jacobs","10.3390/S20113214","","Type 1 diabetes (T1D) is a chronic health condition resulting from pancreatic beta cell dysfunction and insulin depletion. While automated insulin delivery systems are now available, many people choose to manage insulin delivery manually through insulin pumps or through multiple daily injections. Frequent insulin titrations are needed to adequately manage glucose, however, provider adjustments are typically made every several months. Recent automated decision support systems incorporate artificial intelligence algorithms to deliver personalized recommendations regarding insulin doses and daily behaviors. This paper presents a comprehensive review of computational and artificial intelligence-based decision support systems to manage T1D. Articles were obtained from PubMed, IEEE Xplore, and ScienceDirect databases. No time period restrictions were imposed on the search. After removing off-topic articles and duplicates, 562 articles were left to review. Of those articles, we identified 61 articles for comprehensive review based on algorithm evaluation using real-world human data, in silico trials, or clinical studies. We grouped decision support systems into general categories of (1) those which recommend adjustments to insulin and (2) those which predict and help avoid hypoglycemia. We review the artificial intelligence methods used for each type of decision support system, and discuss the performance and potential applications of these systems.","A comprehensive review of computational and artificial intelligence-based decision support systems to manage T1D is presented, and the artificial intelligence methods used for each type of decision support system are reviewed."
"An Ontology-Based Interpretable Fuzzy Decision Support System for Diabetes Diagnosis","https://scispace.com/paper/an-ontology-based-interpretable-fuzzy-decision-support-490p3ahw7v","2018","Journal Article","IEEE Access","Shaker El-Sappagh
Jose M. Alonso
Farman Ali
Amjad Ali
Jun-Hyeog Jang
Kyung Sup Kwak","10.1109/ACCESS.2018.2852004","","Diabetes is a serious chronic disease. The importance of clinical decision support systems (CDSSs) to diagnose diabetes has led to extensive research efforts to improve the accuracy, applicability, interpretability, and interoperability of these systems. However, this problem continues to require optimization. Fuzzy rule-based systems are suitable for the medical domain, where interpretability is a main concern. The medical domain is data-intensive, and using electronic health record data to build the FRBS knowledge base and fuzzy sets is critical. Multiple variables are frequently required to determine a correct and personalized diagnosis, which usually makes it difficult to arrive at accurate and timely decisions. In this paper, we propose and implement a new semantically interpretable FRBS framework for diabetes diagnosis. The framework uses multiple aspects of knowledge-fuzzy inference, ontology reasoning, and a fuzzy analytical hierarchy process (FAHP) to provide a more intuitive and accurate design. First, we build a two-layered hierarchical and interpretable FRBS; then, we improve this by integrating an ontology reasoning process based on SNOMED CT standard ontology. We incorporate FAHP to determine the relative medical importance of each sub-FRBS. The proposed system offers numerous unique and critical improvements regarding the implementation of an accurate, dynamic, semantically intelligent, and interpretable CDSS. The designed system considers the ontology semantic similarity of diabetes complications and symptoms concepts in the fuzzy rules’ evaluation process. The framework was tested using a real data set, and the results indicate how the proposed system helps physicians and patients to accurately diagnose diabetes mellitus.","A new semantically interpretable FRBS framework for diabetes diagnosis using multiple aspects of knowledge-fuzzy inference, ontology reasoning, and a fuzzy analytical hierarchy process (FAHP) to provide a more intuitive and accurate design."
"Explainable AI for clinical and remote health applications: a survey on tabular and time series data","https://scispace.com/paper/explainable-ai-for-clinical-and-remote-health-applications-a-2kbwxdzm","2022","Journal Article","Artificial Intelligence Review","Flavio Di Martino
Franca Delmastro","10.1007/s10462-022-10304-3","https://scispace.com/pdf/explainable-ai-for-clinical-and-remote-health-applications-a-2kbwxdzm.pdf","Nowadays Artificial Intelligence (AI) has become a fundamental component of healthcare applications, both clinical and remote, but the best performing AI systems are often too complex to be self-explaining. Explainable AI (XAI) techniques are defined to unveil the reasoning behind the system's predictions and decisions, and they become even more critical when dealing with sensitive and personal health data. It is worth noting that XAI has not gathered the same attention across different research areas and data types, especially in healthcare. In particular, many clinical and remote health applications are based on tabular and time series data, respectively, and XAI is not commonly analysed on these data types, while computer vision and Natural Language Processing (NLP) are the reference applications. To provide an overview of XAI methods that are most suitable for tabular and time series data in the healthcare domain, this paper provides a review of the literature in the last 5 years, illustrating the type of generated explanations and the efforts provided to evaluate their relevance and quality. Specifically, we identify clinical validation, consistency assessment, objective and standardised quality evaluation, and human-centered quality assessment as key features to ensure effective explanations for the end users. Finally, we highlight the main research challenges in the field as well as the limitations of existing XAI methods. ","A review of the literature in the last 5 years, illustrating the type of generated explanations and the efforts provided to evaluate their relevance and quality, is provided in this paper , where the authors identify clinical validation, consistency assessment, objective and standardised quality evaluation, and human-centered quality assessment as key features to ensure effective explanations for the end users."
"Advanced Diabetes Management Using Artificial Intelligence and Continuous Glucose Monitoring Sensors.","https://scispace.com/paper/advanced-diabetes-management-using-artificial-intelligence-2stag99m7u","2020","Journal Article","Sensors","Martina Vettoretti
Giacomo Cappon
Andrea Facchinetti
Giovanni Sparacino","10.3390/S20143870","https://scispace.com/pdf/advanced-diabetes-management-using-artificial-intelligence-2stag99m7u.pdf","Wearable continuous glucose monitoring (CGM) sensors are revolutionizing the treatment of type 1 diabetes (T1D). These sensors provide in real-time, every 1–5 min, the current blood glucose concentration and its rate-of-change, two key pieces of information for improving the determination of exogenous insulin administration and the prediction of forthcoming adverse events, such as hypo-/hyper-glycemia. The current research in diabetes technology is putting considerable effort into developing decision support systems for patient use, which automatically analyze the patient’s data collected by CGM sensors and other portable devices, as well as providing personalized recommendations about therapy adjustments to patients. Due to the large amount of data collected by patients with T1D and their variety, artificial intelligence (AI) techniques are increasingly being adopted in these decision support systems. In this paper, we review the state-of-the-art methodologies using AI and CGM sensors for decision support in advanced T1D management, including techniques for personalized insulin bolus calculation, adaptive tuning of bolus calculator parameters and glucose prediction.","The state-of-the-art methodologies using AI and CGM sensors for decision support in advanced T1D management are reviewed, including techniques for personalized insulin bolus calculation, adaptive tuning of bolus calculator parameters and glucose prediction."
"Artificial Intelligence Methodologies and Their Application to Diabetes.","https://scispace.com/paper/artificial-intelligence-methodologies-and-their-application-3h7tspju4p","2018","Journal Article","Journal of diabetes science and technology","Mercedes Rigla
Gema García-Sáez
Belén Pons
María Hernando","10.1177/1932296817710475","https://journals.sagepub.com/doi/pdf/10.1177/1932296817710475","In the past decade diabetes management has been transformed by the addition of continuous glucose monitoring and insulin pump data More recently, a wide variety of functions and physiologic variables, such as heart rate, hours of sleep, number of steps walked and movement, have been available through wristbands or watches New data, hydration, geolocation, and barometric pressure, among others, will be incorporated in the future All these parameters, when analyzed, can be helpful for patients and doctors' decision support Similar new scenarios have appeared in most medical fields, in such a way that in recent years, there has been an increased interest in the development and application of the methods of artificial intelligence (AI) to decision support and knowledge acquisition Multidisciplinary research teams integrated by computer engineers and doctors are more and more frequent, mirroring the need of cooperation in this new topic AI, as a science, can be defined as the ability to make computers do things that would require intelligence if done by humans Increasingly, diabetes-related journals have been incorporating publications focused on AI tools applied to diabetes In summary, diabetes management scenarios have suffered a deep transformation that forces diabetologists to incorporate skills from new areas This recently needed knowledge includes AI tools, which have become part of the diabetes health care The aim of this article is to explain in an easy and plane way the most used AI methodologies to promote the implication of health care providers-doctors and nurses-in this field","The aim of this article is to explain in an easy and plane way the most used AI methodologies to promote the implication of health care providers—doctors and nurses—in this field."
"Opening the black box of AI-Medicine.","https://scispace.com/paper/opening-the-black-box-of-ai-medicine-27i5578bcb","2021","Journal Article","Journal of Gastroenterology and Hepatology","Aaron I F Poon
Joseph J.Y. Sung","10.1111/JGH.15384","","One of the biggest challenges of utilizing artificial intelligence (AI) in medicine is that physicians are reluctant to trust and adopt something that they do not fully understand and regarded as a ""black box."" Machine Learning (ML) can assist in reading radiological, endoscopic and histological pictures, suggesting diagnosis and predict disease outcome, and even recommending therapy and surgical decisions. However, clinical adoption of these AI tools has been slow because of a lack of trust. Besides clinician's doubt, patients lacking confidence with AI-powered technologies also hamper development. While they may accept the reality that human errors can occur, little tolerance of machine error is anticipated. In order to implement AI medicine successfully, interpretability of ML algorithm needs to improve. Opening the black box in AI medicine needs to take a stepwise approach. Small steps of biological explanation and clinical experience in ML algorithm can help to build trust and acceptance. AI software developers will have to clearly demonstrate that when the ML technologies are integrated into the clinical decision-making process, they can actually help to improve clinical outcome. Enhancing interpretability of ML algorithm is a crucial step in adopting AI in medicine.","In order to implement AI medicine successfully, interpretability of ML algorithm needs to improve as mentioned in this paper, and small steps of biological explanation and clinical experience in ML algorithm can help to build trust and acceptance."
"Transforming Diabetes Care Through Artificial Intelligence: The Future Is Here.","https://scispace.com/paper/transforming-diabetes-care-through-artificial-intelligence-4lgr1lurba","2019","Journal Article","Population Health Management","Irene Dankwa-Mullan
Marc Rivo
Marisol Sepulveda
Yoonyoung Park
Jane L. Snowdon
Kyu Rhee","10.1089/POP.2018.0129","","An estimated 425 million people globally have diabetes, accounting for 12% of the world's health expenditures, and yet 1 in 2 persons remain undiagnosed and untreated. Applications of artificial intelligence (AI) and cognitive computing offer promise in diabetes care. The purpose of this article is to better understand what AI advances may be relevant today to persons with diabetes (PWDs), their clinicians, family, and caregivers. The authors conducted a predefined, online PubMed search of publicly available sources of information from 2009 onward using the search terms ""diabetes"" and ""artificial intelligence."" The study included clinically-relevant, high-impact articles, and excluded articles whose purpose was technical in nature. A total of 450 published diabetes and AI articles met the inclusion criteria. The studies represent a diverse and complex set of innovative approaches that aim to transform diabetes care in 4 main areas: automated retinal screening, clinical decision support, predictive population risk stratification, and patient self-management tools. Many of these new AI-powered retinal imaging systems, predictive modeling programs, glucose sensors, insulin pumps, smartphone applications, and other decision-support aids are on the market today with more on the way. AI applications have the potential to transform diabetes care and help millions of PWDs to achieve better blood glucose control, reduce hypoglycemic episodes, and reduce diabetes comorbidities and complications. AI applications offer greater accuracy, efficiency, ease of use, and satisfaction for PWDs, their clinicians, family, and caregivers.","A predefined, online PubMed search of publicly available sources of information from 2009 onward using the search terms “diabetes” and “artificial intelligence” concluded that 450 published diabetes and AI articles met the inclusion criteria."
"Explainable AI for Healthcare 5.0: Opportunities and Challenges","https://scispace.com/paper/explainable-ai-for-healthcare-5-0-opportunities-and-1n7he1lb","2022","Journal Article","IEEE Access","Deepti Saraswat
Pronaya Bhattacharya
Ashwin Verma
Vivek Kumar Prasad
Sudeep Tanwar
Gulshan Sharma
Pitshou Bokoro
Ravi Sharma","10.1109/access.2022.3197671","https://scispace.com/pdf/explainable-ai-for-healthcare-5-0-opportunities-and-1n7he1lb.pdf","In the healthcare domain, a transformative shift is envisioned towards Healthcare 5.0. It expands the operational boundaries of Healthcare 4.0 and leverages patient-centric digital wellness. Healthcare 5.0 focuses on real-time patient monitoring, ambient control and wellness, and privacy compliance through assisted technologies like artificial intelligence (AI), Internet-of-Things (IoT), big data, and assisted networking channels. However, healthcare operational procedures, verifiability of prediction models, resilience, and lack of ethical and regulatory frameworks are potential hindrances to the realization of Healthcare 5.0. Recently, explainable AI (EXAI) has been a disruptive trend in AI that focuses on the explainability of traditional AI models by leveraging the decision-making of the models and prediction outputs. The explainability factor opens new opportunities to the black-box models and brings confidence in healthcare stakeholders to interpret the machine learning (ML) and deep learning (DL) models. EXAI is focused on improving clinical health practices and brings transparency to the predictive analysis, which is crucial in the healthcare domain. Recent surveys on EXAI in healthcare have not significantly focused on the data analysis and interpretation of models, which lowers its practical deployment opportunities. Owing to the gap, the proposed survey explicitly details the requirements of EXAI in Healthcare 5.0, the operational and data collection process. Based on the review method and presented research questions, systematically, the article unfolds a proposed architecture that presents an EXAI ensemble on the computerized tomography (CT) image classification and segmentation process. A solution taxonomy of EXAI in Healthcare 5.0 is proposed, and operational challenges are presented. A supported case study on electrocardiogram (ECG) monitoring is presented that preserves the privacy of local models via federated learning (FL) and EXAI for metric validation. The case-study is supported through experimental validation. The analysis proves the efficacy of EXAI in health setups that envisions real-life model deployments in a wide range of clinical applications.","A proposed architecture that presents an EXAI ensemble on the computerized tomography (CT) image classification and segmentation process is unfolded that proves the efficacy of EXAI in health setups that envisions real-life model deployments in a wide range of clinical applications."
"Survey of Explainable AI Techniques in Healthcare","https://scispace.com/paper/survey-of-explainable-ai-techniques-in-healthcare-3qntgd4a","2023","Journal Article","Sensors","Ahmad Chaddad
Jihao Peng
Jian Xu
Ahmed Bouridane","10.3390/s23020634","https://scispace.com/pdf/survey-of-explainable-ai-techniques-in-healthcare-3qntgd4a.pdf","Artificial intelligence (AI) with deep learning models has been widely applied in numerous domains, including medical imaging and healthcare tasks. In the medical field, any judgment or decision is fraught with risk. A doctor will carefully judge whether a patient is sick before forming a reasonable explanation based on the patient’s symptoms and/or an examination. Therefore, to be a viable and accepted tool, AI needs to mimic human judgment and interpretation skills. Specifically, explainable AI (XAI) aims to explain the information behind the black-box model of deep learning that reveals how the decisions are made. This paper provides a survey of the most recent XAI techniques used in healthcare and related medical imaging applications. We summarize and categorize the XAI types, and highlight the algorithms used to increase interpretability in medical imaging topics. In addition, we focus on the challenging XAI problems in medical applications and provide guidelines to develop better interpretations of deep learning models using XAI concepts in medical image and text analysis. Furthermore, this survey provides future directions to guide developers and researchers for future prospective investigations on clinical topics, particularly on applications with medical imaging.","A survey of explainable AI techniques used in healthcare and related medical imaging applications can be found in this paper , where the authors provide guidelines to develop better interpretations of deep learning models using XAI concepts in medical image and text analysis."
"Explainable AI (XAI): A systematic meta-survey of current challenges and future opportunities","https://scispace.com/paper/explainable-ai-xai-a-systematic-meta-survey-of-current-zghnjjxd","2023","Journal Article","Knowledge Based Systems","","10.1016/j.knosys.2023.110273","","The past decade has seen significant progress in artificial intelligence (AI), which has resulted in algorithms being adopted for resolving a variety of problems. However, this success has been met by increasing model complexity and employing black-box AI models that lack transparency. In response to this need, Explainable AI (XAI) has been proposed to make AI more transparent and thus advance the adoption of AI in critical domains. Although there are several reviews of XAI topics in the literature that have identified challenges and potential research directions of XAI, these challenges and research directions are scattered. This study, hence, presents a systematic meta-survey of challenges and future research directions in XAI organized in two themes: (1) general challenges and research directions of XAI and (2) challenges and research directions of XAI based on machine learning life cycle’s phases: design, development, and deployment. We believe that our meta-survey contributes to XAI literature by providing a guide for future exploration in the XAI area. ","In this paper , a meta-survey of challenges and future research directions in explainable AI is presented, which is organized in two themes: (1) general challenges and research directions of XAI and (2) challenge and research direction of XA based on machine learning life cycle's phases: design, development and deployment."
"Explainable artificial intelligence: a comprehensive review","https://scispace.com/paper/explainable-artificial-intelligence-a-comprehensive-review-3bp0mxoj18","2021","Journal Article","Artificial Intelligence Review","Dang Minh
H. Xiang Wang
Y. Fen Li
Tan N. Nguyen","10.1007/S10462-021-10088-Y","","Thanks to the exponential growth in computing power and vast amounts of data, artificial intelligence (AI) has witnessed remarkable developments in recent years, enabling it to be ubiquitously adopted in our daily lives. Even though AI-powered systems have brought competitive advantages, the black-box nature makes them lack transparency and prevents them from explaining their decisions. This issue has motivated the introduction of explainable artificial intelligence (XAI), which promotes AI algorithms that can show their internal process and explain how they made decisions. The number of XAI research has increased significantly in recent years, but there lacks a unified and comprehensive review of the latest XAI progress. This review aims to bridge the gap by discovering the critical perspectives of the rapidly growing body of research associated with XAI. After offering the readers a solid XAI background, we analyze and review various XAI methods, which are grouped into (i) pre-modeling explainability, (ii) interpretable model, and (iii) post-modeling explainability. We also pay attention to the current methods that dedicate to interpret and analyze deep learning methods. In addition, we systematically discuss various XAI challenges, such as the trade-off between the performance and the explainability, evaluation methods, security, and policy. Finally, we show the standard approaches that are leveraged to deal with the mentioned challenges.","A review of explainable artificial intelligence (XAI) can be found in this article, where the authors analyze and review various XAI methods, which are grouped into (i) pre-modeling explainability, (ii) interpretable model, and (iii) post-model explainability."
"Faithful and Customizable Explanations of Black Box Models","https://scispace.com/paper/faithful-and-customizable-explanations-of-black-box-models-qev41j2cxf","2019","Proceedings Article","National Conference on Artificial Intelligence","Himabindu Lakkaraju
Ece Kamar
Rich Caruana
Jure Leskovec","10.1145/3306618.3314229","https://dl.acm.org/doi/pdf/10.1145/3306618.3314229","As predictive models increasingly assist human experts (e.g., doctors) in day-to-day decision making, it is crucial for experts to be able to explore and understand how such models behave in different feature subspaces in order to know if and when to trust them. To this end, we propose Model Understanding through Subspace Explanations (MUSE), a novel model agnostic framework which facilitates understanding of a given black box model by explaining how it behaves in subspaces characterized by certain features of interest. Our framework provides end users (e.g., doctors) with the flexibility of customizing the model explanations by allowing them to input the features of interest. The construction of explanations is guided by a novel objective function that we propose to simultaneously optimize for fidelity to the original model, unambiguity and interpretability of the explanation. More specifically, our objective allows us to learn, with optimality guarantees, a small number of compact decision sets each of which captures the behavior of a given black box model in unambiguous, well-defined regions of the feature space. Experimental evaluation with real-world datasets and user studies demonstrate that our approach can generate customizable, highly compact, easy-to-understand, yet accurate explanations of various kinds of predictive models compared to state-of-the-art baselines.","Model Understanding through Subspace Explanations (MUSE), a novel model agnostic framework which facilitates understanding of a given black box model by explaining how it behaves in subspaces characterized by certain features of interest, is proposed."
"Explainable AI (XAI): Core Ideas, Techniques, and Solutions","https://scispace.com/paper/explainable-ai-xai-core-ideas-techniques-and-solutions-1xrl1xvj","2022","Journal Article","ACM Computing Surveys","Rudresh Dwivedi
Devam Dave
Heta Naik
Smiti Singhal
Omer Rana
Pankesh Patel
Bin Qian
Zhenyu Wen
Tejal Shah
Graham Morgan
Rajiv Ranjan","10.1145/3561048","https://scispace.com/pdf/explainable-ai-xai-core-ideas-techniques-and-solutions-1xrl1xvj.pdf","As our dependence on intelligent machines continues to grow, so does the demand for more transparent and interpretable models. In addition, the ability to explain the model generally is now the gold standard for building trust and deployment of artificial intelligence systems in critical domains. Explainable artificial intelligence (XAI) aims to provide a suite of machine learning techniques that enable human users to understand, appropriately trust, and produce more explainable models. Selecting an appropriate approach for building an XAI-enabled application requires a clear understanding of the core ideas within XAI and the associated programming frameworks. We survey state-of-the-art programming techniques for XAI and present the different phases of XAI in a typical machine learning development process. We classify the various XAI approaches and, using this taxonomy, discuss the key differences among the existing XAI techniques. Furthermore, concrete examples are used to describe these techniques that are mapped to programming frameworks and software toolkits. It is the intention that this survey will help stakeholders in selecting the appropriate approaches, programming frameworks, and software toolkits by comparing them through the lens of the presented taxonomy.","This survey surveys state-of-the-art programming techniques for XAI and presents the different phases of XAI in a typical ML development process, and classify the various XAI approaches and using this taxonomy, discusses the key differences among the existing XAI techniques."
"Interpretability of machine learning‐based prediction models in healthcare","https://scispace.com/paper/interpretability-of-machine-learning-based-prediction-models-3lv7zarxj6","2020","Journal Article","Wiley Interdisciplinary Reviews-Data Mining and Knowledge Discovery","Gregor Stiglic
Primoz Kocbek
Nino Fijačko
Marinka Zitnik
Katrien Verbert
Leona Cilar","10.1002/WIDM.1379","https://arxiv.org/pdf/2002.08596","There is a need of ensuring machine learning models that are interpretable. Higher interpretability of the model means easier comprehension and explanation of future predictions for end-users. Further, interpretable machine learning models allow healthcare experts to make reasonable and data-driven decisions to provide personalized decisions that can ultimately lead to higher quality of service in healthcare. Generally, we can classify interpretability approaches in two groups where the first focuses on personalized interpretation (local interpretability) while the second summarizes prediction models on a population level (global interpretability). Alternatively, we can group interpretability methods into model-specific techniques, which are designed to interpret predictions generated by a specific model, such as a neural network, and model-agnostic approaches, which provide easy-to-understand explanations of predictions made by any machine learning model. Here, we give an overview of interpretability approaches and provide examples of practical interpretability of machine learning in different areas of healthcare, including prediction of health-related outcomes, optimizing treatments or improving the efficiency of screening for specific conditions. Further, we outline future directions for interpretable machine learning and highlight the importance of developing algorithmic solutions that can enable machine-learning driven decision making in high-stakes healthcare problems.","In this article, the authors give an overview of interpretability approaches and provide examples of practical interpretability of machine learning in different areas of healthcare, including prediction of health-related outcomes, optimizing treatments or improving the efficiency of screening for specific conditions."
"Artificial Intelligence for Diabetes Management and Decision Support: Literature Review","https://scispace.com/paper/artificial-intelligence-for-diabetes-management-and-decision-qzn2ktfld1","2018","Journal Article","Journal of Medical Internet Research","Ivan Contreras
Josep Vehí","10.2196/10775","https://scispace.com/pdf/artificial-intelligence-for-diabetes-management-and-decision-qzn2ktfld1.pdf","Background: Artificial intelligence methods in combination with the latest technologies, including medical devices, mobile computing, and sensor technologies, have the potential to enable the creation and delivery of better management services to deal with chronic diseases. One of the most lethal and prevalent chronic diseases is diabetes mellitus, which is characterized by dysfunction of glucose homeostasis. Objective: The objective of this paper is to review recent efforts to use artificial intelligence techniques to assist in the management of diabetes, along with the associated challenges. Methods: A review of the literature was conducted using PubMed and related bibliographic resources. Analyses of the literature from 2010 to 2018 yielded 1849 pertinent articles, of which we selected 141 for detailed review. Results: We propose a functional taxonomy for diabetes management and artificial intelligence. Additionally, a detailed analysis of each subject category was performed using related key outcomes. This approach revealed that the experiments and studies reviewed yielded encouraging results. Conclusions: We obtained evidence of an acceleration of research activity aimed at developing artificial intelligence-powered tools for prediction and prevention of complications associated with diabetes. Our results indicate that artificial intelligence methods are being progressively established as suitable for use in clinical daily practice, as well as for the self-management of diabetes. Consequently, these methods provide powerful tools for improving patients’ quality of life.","Evidence of an acceleration of research activity aimed at developing artificial intelligence-powered tools for prediction and prevention of complications associated with diabetes is obtained, indicating that artificial intelligence methods are being progressively established as suitable for use in clinical daily practice, as well as for the self-management of diabetes."
"Explainability for artificial intelligence in healthcare: a multidisciplinary perspective.","https://scispace.com/paper/explainability-for-artificial-intelligence-in-healthcare-a-3taqlmxc0a","2020","Journal Article","BMC Medical Informatics and Decision Making","Julia Amann
Alessandro Blasimme
Effy Vayena
Dietmar Frey
Vince I. Madai
Vince I. Madai","10.1186/S12911-020-01332-6","https://scispace.com/pdf/explainability-for-artificial-intelligence-in-healthcare-a-3taqlmxc0a.pdf","Explainability is one of the most heavily debated topics when it comes to the application of artificial intelligence (AI) in healthcare. Even though AI-driven systems have been shown to outperform humans in certain analytical tasks, the lack of explainability continues to spark criticism. Yet, explainability is not a purely technological issue, instead it invokes a host of medical, legal, ethical, and societal questions that require thorough exploration. This paper provides a comprehensive assessment of the role of explainability in medical AI and makes an ethical evaluation of what explainability means for the adoption of AI-driven tools into clinical practice. Taking AI-based clinical decision support systems as a case in point, we adopted a multidisciplinary approach to analyze the relevance of explainability for medical AI from the technological, legal, medical, and patient perspectives. Drawing on the findings of this conceptual analysis, we then conducted an ethical assessment using the “Principles of Biomedical Ethics” by Beauchamp and Childress (autonomy, beneficence, nonmaleficence, and justice) as an analytical framework to determine the need for explainability in medical AI. Each of the domains highlights a different set of core considerations and values that are relevant for understanding the role of explainability in clinical practice. From the technological point of view, explainability has to be considered both in terms how it can be achieved and what is beneficial from a development perspective. When looking at the legal perspective we identified informed consent, certification and approval as medical devices, and liability as core touchpoints for explainability. Both the medical and patient perspectives emphasize the importance of considering the interplay between human actors and medical AI. We conclude that omitting explainability in clinical decision support systems poses a threat to core ethical values in medicine and may have detrimental consequences for individual and public health. To ensure that medical AI lives up to its promises, there is a need to sensitize developers, healthcare professionals, and legislators to the challenges and limitations of opaque algorithms in medical AI and to foster multidisciplinary collaboration moving forward.","There is a need to sensitize developers, healthcare professionals, and legislators to the challenges and limitations of opaque algorithms in medical AI and to foster multidisciplinary collaboration moving forward to ensure that medical AI lives up to its promises."
"Peeking Inside the Black-Box: A Survey on Explainable Artificial Intelligence (XAI)","https://scispace.com/paper/peeking-inside-the-black-box-a-survey-on-explainable-49qys2awbp","2018","Journal Article","IEEE Access","Amina Adadi
Mohammed Berrada","10.1109/ACCESS.2018.2870052","","At the dawn of the fourth industrial revolution, we are witnessing a fast and widespread adoption of artificial intelligence (AI) in our daily life, which contributes to accelerating the shift towards a more algorithmic society. However, even with such unprecedented advancements, a key impediment to the use of AI-based systems is that they often lack transparency. Indeed, the black-box nature of these systems allows powerful predictions, but it cannot be directly explained. This issue has triggered a new debate on explainable AI (XAI). A research field holds substantial promise for improving trust and transparency of AI-based systems. It is recognized as the sine qua non for AI to continue making steady progress without disruption. This survey provides an entry point for interested researchers and practitioners to learn key aspects of the young and rapidly growing body of research related to XAI. Through the lens of the literature, we review the existing approaches regarding the topic, discuss trends surrounding its sphere, and present major research trajectories.","This survey provides an entry point for interested researchers and practitioners to learn key aspects of the young and rapidly growing body of research related to XAI, and review the existing approaches regarding the topic, discuss trends surrounding its sphere, and present major research trajectories."
"Interpretable prediction model for assessing diabetes complication risks in Chinese sufferers.","https://scispace.com/paper/interpretable-prediction-model-for-assessing-diabetes-4iszr1omsl","2024","Journal Article","Diabetes Research and Clinical Practice","Shiren Ye
Jiangnan Ye
Xinhua Ye
Ni Xinye","10.1016/j.diabres.2024.111560","","With growing concerns over complications in diabetes sufferers, this study sought to develop an interpretable machine learning model to offer enhanced diagnostic and treatment recommendations.We assessed coronary heart disease, diabetic nephropathy, diabetic retinopathy, and fatty liver disease using logistic regression, decision tree, random forest, and CatBoost algorithms. The SHAP algorithm was employed to elucidate the model's predictions, offering a more in-depth understanding of influential features.The CatBoost model notably outperformed other algorithms in AUC, achieving an average AUC of 90.47 % for the four complications. Through SHAP analysis and visualization, we provided clear and actionable insights into risk factors, enabling better complication risk assessment.We introduced an innovative, interpretable complication risk model for people with diabetes. This not only offers a potent tool for healthcare professionals but also empowers sufferers with clearer self-assessment capabilities, encouraging earlier preventive actions. Further studies will underscore the model's clinical applicability. ","An innovative, interpretable complication risk model for people with diabetes is introduced that not only offers a potent tool for healthcare professionals but also empowers sufferers with clearer self-assessment capabilities, encouraging earlier preventive actions."
"Artificial Intelligence-Assisted Clinical Decision-Making: A Perspective on Advancing Personalized Precision Medicine for Elderly Diabetes Patients","https://scispace.com/paper/artificial-intelligence-assisted-clinical-decision-making-a-y7htvu6mr58l","2025","Journal Article","Journal of multidisciplinary healthcare","Jianfeng Hu
Lijun Ren
Tingwen Wang
Yao Peng","10.2147/jmdh.s529190","","The global aging population is expanding at an unprecedented rate and is projected to reach 2 billion by 2050, presenting significant medical challenges, particularly multimorbidity and heterogeneous responses to treatment. Using diabetes as an illustrative case, this study explores the transformative potential of artificial intelligence (AI)-assisted clinical decision-making to advance personalized precision medicine for older adults. Through systematic analysis of current healthcare practices and emerging AI technologies, we examined the integration of machine learning algorithms, natural language processing, and intelligent monitoring systems into diabetes care for elderly populations. Based on current evidence showing up to 25% reduction in hospitalization rates and 30% increase in treatment adherence, we argue that AI integration represents a transformative approach to improving clinical outcomes in elderly diabetes care. We contend that AI-driven clinical decision support systems (CDSS) offer superior performance in risk prediction and treatment optimization, with studies demonstrating diagnostic accuracy rates of up to 93.07%, supporting our argument for their widespread implementation. Furthermore, AI-enhanced monitoring systems improved medication adherence by 17.9% compared to conventional monitoring approaches. Nonetheless, several challenges persist, including issues related to data standardization, algorithm transparency, and patient privacy protection. These results underscore the necessity of adopting a balanced implementation strategy that addresses both technical limitations and ethical considerations, while upholding patient autonomy. This perspective emphasizes the critical importance of multidisciplinary collaboration among healthcare professionals, technology developers, and regulatory authorities in establishing a comprehensive framework for AI deployment in clinical settings. By demonstrating the capacity of AI-assisted clinical decision-making to enhance healthcare quality and efficiency for elderly patients with diabetes, this study makes a meaningful contribution to the evolving field of personalized medicine. ","This study explores AI-assisted clinical decision-making for elderly diabetes patients, demonstrating up to 25% reduced hospitalization rates and 30% increased treatment adherence, and highlighting the need for a balanced implementation strategy addressing technical and ethical challenges."
"A unified and practical user-centric framework for explainable artificial intelligence","https://scispace.com/paper/a-unified-and-practical-user-centric-framework-for-uxdousvzij","2023","Journal Article","Knowledge Based Systems","Sinan Kaplan
Hannu Uusitalo
Lasse Lensu","10.1016/j.knosys.2023.111107","","Adoption of artificial intelligence (AI) is causing a paradigm change in many fields. Its practical utilization, however, especially in safety-critical applications like medicine, remains limited, mainly due to the black-box nature of most advanced AI models, which creates difficulties understanding why and how a model produces a particular output or decision. To overcome this issue, various methods and techniques have been proposed within the emerging field of explainable artificial intelligence (XAI). In this paper, we introduce a user-centric and interactive framework that enables a holistic understanding of AI systems. The proposed framework is designed to aid the development of more explainable AI systems by promoting transparency and trust in their use and allow different stakeholders to better understand and evaluate AI decisions. To illustrate the effectiveness of the framework, we implement a case study of an AI system analyzing optical coherence tomography (OCT) images. The development of this example case is reported using the proposed framework. ","This paper introduces a user-centric framework for explainable artificial intelligence, promoting transparency and trust in AI systems by enabling holistic understanding and evaluation of AI decisions, illustrated through a case study of AI analyzing OCT images."
"Explainable Artificial Intelligence: Importance, Use Domains, Stages, Output Shapes, and Challenges","https://scispace.com/paper/explainable-artificial-intelligence-importance-use-domains-2bj6w6l7qvjo","2024","Journal Article","ACM Computing Surveys","Naeem Ullah
Javed Ali Khan
I. De Falco
Giovanna Sannino","10.1145/3705724","","There is an urgent need in many application areas for eXplainable ArtificiaI Intelligence (XAI) approaches to boost people’s confidence and trust in Artificial Intelligence methods. Current works concentrate on specific aspects of XAI and avoid a comprehensive perspective. This study undertakes a systematic survey of importance, approaches, methods, and application domains to address this gap and provide a comprehensive understanding of the XAI domain. Applying the Systematic Literature Review approach has resulted in finding and discussing 155 papers, allowing a wide discussion on the strengths, limitations, and challenges of XAI methods and future research directions.","This study provides a comprehensive review of Explainable Artificial Intelligence (XAI), covering importance, approaches, methods, and application domains, discussing 155 papers to address the gap in current XAI research and identify future research directions."
"When time is of the essence: ethical reconsideration of XAI in time-sensitive environments.","https://scispace.com/paper/when-time-is-of-the-essence-ethical-reconsideration-of-xai-1pv28m3qtry0","2024","Journal Article","Journal of Medical Ethics","Andreas Wabro
Markus Herrmann
Eva C Winkler","10.1136/jme-2024-110046","","The objective of explainable artificial intelligence systems designed for clinical decision support (XAI-CDSS) is to enhance physicians' diagnostic performance, confidence and trust through the implementation of interpretable methods, thus providing for a superior epistemic positioning, a robust foundation for critical reflection and trustworthiness in times of heightened technological dependence. However, recent studies have revealed shortcomings in achieving these goals, questioning the widespread endorsement of XAI by medical professionals, ethicists and policy-makers alike. Based on a surgical use case, this article challenges generalising calls for XAI-CDSS and emphasises the significance of time-sensitive clinical environments which frequently preclude adequate consideration of system explanations. Therefore, XAI-CDSS may not be able to meet expectations of augmenting clinical decision-making in specific circumstances where time is of the essence. This article, by employing a principled ethical balancing methodology, highlights several fallacies associated with XAI deployment in time-sensitive clinical situations and recommends XAI endorsement only where scientific evidence or stakeholder assessments do not contradict such deployment in specific target settings.","This article challenges the widespread endorsement of XAI-CDSS in time-sensitive clinical environments, highlighting fallacies associated with XAI deployment and recommending cautious adoption based on scientific evidence and stakeholder assessments."
"Methodology for Safe and Secure AI in Diabetes Management","https://scispace.com/paper/methodology-for-safe-and-secure-ai-in-diabetes-management-3yd17w9glwoi","2024","Journal Article","Journal of diabetes science and technology","Remco Jan Geukes Foppen
Vincenzo La Gioia
Shreya Gupta
Curtis L. Johnson
John Giantsidis
Maria Papademetris","10.1177/19322968241304434","","The use of artificial intelligence (AI) in diabetes management is emerging as a promising solution to improve the monitoring and personalization of therapies. However, the integration of such technologies in the clinical setting poses significant challenges related to safety, security, and compliance with sensitive patient data, as well as the potential direct consequences on patient health. This article provides guidance for developers and researchers on identifying and addressing these safety, security, and compliance challenges in AI systems for diabetes management. We emphasize the role of explainable AI (xAI) systems as the foundational strategy for ensuring security and compliance, fostering user trust, and informed clinical decision-making which is paramount in diabetes care solutions. The article examines both the technical and regulatory dimensions essential for developing explainable applications in this field. Technically, we demonstrate how understanding the lifecycle phases of AI systems aids in constructing xAI frameworks while addressing security concerns and implementing risk mitigation strategies at each stage. In addition, from a regulatory perspective, we analyze key Governance, Risk, and Compliance (GRC) standards established by entities, such as the Food and Drug Administration (FDA), providing specific guidelines to ensure safety, efficacy, and ethical integrity in AI-enabled diabetes care applications. By addressing these interconnected aspects, this article aims to deliver actionable insights and methodologies for developing trustworthy AI-enabled diabetes care solutions while ensuring safety, efficacy, and compliance with ethical standards to enhance patient engagement and improve clinical outcomes. ","This article provides a methodology for safe and secure AI in diabetes management, emphasizing explainable AI (xAI) systems to ensure security, compliance, and informed clinical decision-making, while addressing technical and regulatory challenges to enhance patient engagement and improve clinical outcomes."
"<scp>GlucoNet</scp>‐<scp>MM</scp>: A multimodal attention‐based multi‐task learning framework with decision transformer for personalised and explainable blood glucose forecasting","https://scispace.com/paper/scp-gluconet-scp-scp-mm-scp-a-multimodal-attention-based-io3ap30t1q9h","2025","Journal Article","Diabetes, Obesity and Metabolism","Sarmad Maqsood
Muhammad Abdullah Sarwar
Eglė Belousovienė
Rytis Maskeliūnas","10.1111/dom.70147","","Abstract Aims Accurate and personalized blood glucose prediction is critical for proactive diabetes management. Conventional machine learning (ML) models often struggle to generalize across patients due to individual variability, nonlinear glycemic dynamics, and sparse multimodal input data. This study aims to develop an advanced, interpretable deep learning (DL) framework for patient‐specific, policy‐aware blood glucose forecasting. Materials and Methods We propose GlucoNet‐MM, a novel multimodal DL framework that combines attention‐based multi‐task learning (MTL) with a Decision Transformer (DT), a reinforcement learning paradigm that frames policy learning as sequence modeling. The model integrates heterogeneous physiological and behavioral data, continuous glucose monitoring (CGM), insulin dosage, carbohydrate intake, and physical activity, to capture complex temporal dependencies. The MTL backbone learns shared representations across multiple prediction horizons, while the DT module conditions future glucose predictions on desired glycemic outcomes. Temporal attention visualizations and integrated gradient‐based attribution methods are used to provide interpretability, and Monte Carlo dropout is employed for uncertainty quantification. Results GlucoNet‐MM was evaluated on two publicly available datasets, BrisT1D and OhioT1DM. The model achieved R 2 scores of 0.94 and 0.96 and mean absolute error (MAE) values of 0.031 and 0.027, respectively. These results outperform single‐modality and conventional non‐adaptive baseline models, demonstrating superior predictive accuracy and generalizability. Conclusion GlucoNet‐MM represents a promising step toward intelligent, personalized clinical decision support for diabetes care. Its multimodal design, policy‐aware forecasting, and interpretability features enhance both prediction accuracy and clinical trust, enabling proactive glycemic management tailored to individual patient needs. ","This study presents GlucoNet-MM, a multimodal deep learning framework that integrates heterogeneous data for personalized blood glucose forecasting, achieving superior predictive accuracy and generalizability on two public datasets with R² scores of 0.94-0.96 and MAE of 0.027-0.031."
"Explainable Machine-Learning Models to Predict Weekly Risk of Hyperglycemia, Hypoglycemia, and Glycemic Variability in Patients With Type 1 Diabetes Based on Continuous Glucose Monitoring.","https://scispace.com/paper/explainable-machine-learning-models-to-predict-weekly-risk-6bd33sdkcmdb","2024","Journal Article","Journal of diabetes science and technology","Simon Lebech Cichosz
S. S. Olesen
M. H. Jensen","10.1177/19322968241286907","","BACKGROUND AND OBJECTIVE
The aim of this study was to develop and validate explainable prediction models based on continuous glucose monitoring (CGM) and baseline data to identify a week-to-week risk of CGM key metrics (hyperglycemia, hypoglycemia, glycemic variability). By having a weekly prediction of CGM key metrics, it is possible for the patient or health care personnel to take immediate preemptive action.


METHODS
We analyzed, trained, and internally tested three prediction models (Logistic regression, XGBoost, and TabNet) using CGM data from 187 type 1 diabetes patients with long-term CGM monitoring. A binary classification approach combined with feature engineering deployed on the CGM signals was used to predict hyperglycemia, hypoglycemia, and glycemic variability based on consensus targets (time above range ≥5%, time below range ≥4%, coefficient of variation ≥36%). The models were validated in two independent cohorts with a total of 223 additional patients of varying ages.


RESULTS
A total of 46 593 weeks of CGM data were included in the analysis. For the best model (XGBoost), the area under the receiver operating characteristic curve (ROC-AUC) was 0.9 [95% confidence interval (CI) = 0.89-0.91], 0.89 [95% CI = 0.88-0.9], and 0.8 [95% CI = 0.79-0.81] for predicting hyperglycemia, hypoglycemia, and glycemic variability in the interval validation, respectively. The validation test showed good generalizability of the models with ROC-AUC of 0.88 to 0.95, 0.84 to 0.89, and 0.80 to 0.82 for predicting the glycemic outcomes.


CONCLUSION
Prediction models based on real-world CGM data can be used to predict the risk of unstable glycemic control in the forthcoming week. The models showed good performance in both internal and external validation cohorts.","This study develops and validates explainable machine-learning models to predict weekly risk of hyperglycemia, hypoglycemia, and glycemic variability in type 1 diabetes patients using continuous glucose monitoring data, achieving high accuracy in internal and external validation cohorts."
"The ethical requirement of explainability for AI-DSS in healthcare: a systematic review of reasons","https://scispace.com/paper/the-ethical-requirement-of-explainability-for-ai-dss-in-s6g843bsxeun","2024","Journal Article","BMC Medical Ethics","Nils Freyer
Dominik Groß
Myriam Lipprandt","10.1186/s12910-024-01103-2","","Abstract Background Despite continuous performance improvements, especially in clinical contexts, a major challenge of Artificial Intelligence based Decision Support Systems (AI-DSS) remains their degree of epistemic opacity. The conditions of and the solutions for the justified use of the occasionally unexplainable technology in healthcare are an active field of research. In March 2024, the European Union agreed upon the Artificial Intelligence Act (AIA), requiring medical AI-DSS to be ad-hoc explainable or to use post-hoc explainability methods. The ethical debate does not seem to settle on this requirement yet. This systematic review aims to outline and categorize the positions and arguments in the ethical debate. Methods We conducted a literature search on PubMed, BASE, and Scopus for English-speaking scientific peer-reviewed publications from 2016 to 2024. The inclusion criterion was to give explicit requirements of explainability for AI-DSS in healthcare and reason for it. Non-domain-specific documents, as well as surveys, reviews, and meta-analyses were excluded. The ethical requirements for explainability outlined in the documents were qualitatively analyzed with respect to arguments for the requirement of explainability and the required level of explainability. Results The literature search resulted in 1662 documents; 44 documents were included in the review after eligibility screening of the remaining full texts. Our analysis showed that 17 records argue in favor of the requirement of explainable AI methods (xAI) or ad-hoc explainable models, providing 9 categories of arguments. The other 27 records argued against a general requirement, providing 11 categories of arguments. Also, we found that 14 works advocate the need for context-dependent levels of explainability, as opposed to 30 documents, arguing for context-independent, absolute standards. Conclusions The systematic review of reasons shows no clear agreement on the requirement of post-hoc explainability methods or ad-hoc explainable models for AI-DSS in healthcare. The arguments found in the debate were referenced and responded to from different perspectives, demonstrating an interactive discourse. Policymakers and researchers should watch the development of the debate closely. Conversely, ethicists should be well informed by empirical and technical research, given the frequency of advancements in the field. ","This systematic review of 44 healthcare AI-DSS studies (2016-2024) reveals no consensus on explainability requirements, with 17 arguing for explainable AI methods and 27 against, highlighting the need for context-dependent levels of explainability."
"XAI Unveiled: Revealing the Potential of Explainable AI in Medicine - A Systematic Review","https://scispace.com/paper/xai-unveiled-revealing-the-potential-of-explainable-ai-in-2hb3yggbcga7","2024","Journal Article","IEEE Access","Noemi Scarpato
Patrizia Ferroni
Fiorella Guadagni","10.1109/access.2024.3514197","","Nowadays, artificial intelligence in medicine plays a leading role. This necessitates the need to ensure that artificial intelligence systems are not only high-performing but also comprehensible to all stakeholders involved, including doctors, patients, healthcare providers, etc. As a result, the explainability of artificial intelligence systems has become a widely discussed subject in recent times, leading to the publication of numerous approaches and solutions. In this paper, we aimed to provide a systematic review of these approaches in order to analyze their role in making artificial intelligence interpretable for everyone. The conducted review was carried out in accordance with the PRISMA statement. We conducted a BIAS analysis, identifying 87 scientific papers from those retrieved as having a low risk of BIAS. Subsequently, we defined a classification framework based on the classification taxonomy and applied it to analyze these papers. The results show that, although most AI approaches in medicine currently incorporate explainability methods, the evaluation of these systems is not always performed. When evaluation does occur, it is most often focused on improving the system itself rather than assessing users’ perception of the system’s effectiveness. To address these limitations, we propose a framework for evaluating explainability approaches in medicine, aimed at guiding developers in designing effective human-centered methods.","This systematic review examines the role of explainable AI (XAI) in medicine, analyzing 87 papers and proposing a framework for evaluating XAI approaches to improve human-centered methods and ensure AI systems are comprehensible to stakeholders."
"Comparison of SHAP and clinician friendly explanations reveals effects on clinical decision behaviour","https://scispace.com/paper/comparison-of-shap-and-clinician-friendly-explanations-8er3rnhsgvxa","2025","Journal Article","npj digital medicine","Sujeong Hur
Yura Lee
J. S. Park
Yeong Jeong Jeon
Jong Ho Cho
Duck Cho
David M. Lim
Wonil Hwang
Won Chul
Junsang Yoo","10.1038/s41746-025-01958-8","https://scispace.compdf/comparison-of-shap-and-clinician-friendly-explanations-8er3rnhsgvxa.pdf","Clinical decision-making substantially impacts patients’ lives and their quality of life. However, the black-box nature of AI-powered clinical decision support systems (CDSSs) complicates the interpretation of how decisions are derived. Explainable AI (XAI) improves acceptance and trust with explanations, but the effectiveness of different methods remains uncertain. We compared the acceptance, trust, satisfaction and usability of various explanatory methods among clinicians. We also explored the factors associated with acceptance levels for each item using trust, satisfaction and usability score questionnaires. Surgeons and physicians (N = 63), who had prescribed blood products before surgery, made decisions before and after receiving one of three CDSS explanation methods, each comprising six vignettes, in a counterbalanced design. We found empirical evidence, which indicates that providing a clinical explanation enhances clinicians’ acceptance than presenting ‘results only’ or ‘results with SHapley Additive exPlanations (SHAP)’. Additionally, trust, satisfaction and usability were correlated with acceptance. This study suggests best practices for the strategic application of the XAI–CDSS in the medical field.","This study compares SHAP and clinician-friendly explanations in AI-powered CDSSs, finding that clinical explanations enhance acceptance more than 'results only' or SHAP, and that trust, satisfaction, and usability correlate with acceptance among surgeons and physicians."
"Towards an Interpretable Continuous Glucose Monitoring Data Modeling","https://scispace.com/paper/towards-an-interpretable-continuous-glucose-monitoring-data-1xgarewth7","2024","Journal Article","IEEE Internet of Things Journal","Juan Guerrero
José Luis López Ruiz
Macarena Espinilla
Carmen Martínez-Cruz","10.1109/jiot.2024.3419260","","The ongoing global health challenge posed by diabetes necessitates a critical understanding of all generated data streamed from sensors. To address this, our study presents a robust fuzzy-logic-based descriptive analysis of glucose sensor data. This analysis is embedded within the context of an innovative architecture designed to support multipatient monitoring, with the goal of assisting healthcare professionals in their daily tasks and providing essential decision-making tools. Our novel approach captures and interprets complex data patterns from glucose sensors, and also introduces the capability of creating high-quality linguistic summaries, to highlight the most relevant phenomena through the use of natural language (NL). These descriptions facilitate clear communication between healthcare professionals and people with diabetes, enhancing a deeper understanding of intricate data patterns and promoting collaboration in diabetes care. A comparative evaluation between our proposal and the one obtained using GPT-4 underscores the sustainability, effectiveness, and efficiency of our methodology, positioning it as a new standard for empowering diabetic patients in terms of care and prevention, contributing to their progress and well-being.","This study presents a fuzzy-logic-based analysis of continuous glucose monitoring data, embedded in a multipatient monitoring architecture, to provide interpretable data patterns and linguistic summaries, enhancing collaboration and decision-making in diabetes care."
"1087-P: AI as a Clinical Advisor—Benchmarking AI’s Recommendations against Clinicians’ Practices in Type 2 Diabetes Management","https://scispace.com/paper/1087-p-ai-as-a-clinical-advisor-benchmarking-ais-3bt1lljd7n","2024","Journal Article","Diabetes","CLIPPER F. YOUNG
SHIRLEY WONG","10.2337/db24-1087-p","","Objective: To compare clinical recommendations on type 2 diabetes management based on the ADA’s Standards of Medical Care - 2021 generated by two generative artificial intelligent (AI) platforms for internal consistency (vertically within a platform) and external consensus (horizontally between platforms and with clinicians’ clinical decisions). Methods: A complex clinical case was chosen because the patient has developed comorbid conditions related to type 2 diabetes (e.g., myocardial infarction, stroke, chronic kidney disease). Before entering the case into each AI platform for analysis, the clinical note - chief complaint, subjective, and objective information - was edited and organized to maximize clarity and standardize the language. Following the analyses by the two AI platforms, the outcomes was analyzed qualitatively by comparing the assessment and plan written by a clinician. Results: Conclusion: The two generative AI platforms achieved a similar internal consistency. Regarding external consensus, both platforms focused on key points in the effectiveness theme for secondary cardiovascular prevention. Although this abstract shows one pilot case, this qualitative approach sets a foundation for future analyses as the clinical recommendations from the two generative AI platforms exhibited alignment with the notes written by clinicians (diabetes specialists). Disclosure C.F. Young: Other Relationship; Abbott. S. Wong: None. ","The two generative AI platforms achieved similar internal consistency and focused on key points in the effectiveness theme for secondary cardiovascular prevention in type 2 diabetes management."
"Human-centered explainability evaluation in clinical decision-making: a critical review of the literature","https://scispace.com/paper/human-centered-explainability-evaluation-in-clinical-eemib5ha9s9l","2025","Journal Article","Journal of the American Medical Informatics Association","Jenny M. Bauer
Martin Michalowski","10.1093/jamia/ocaf110","","Abstract Objectives This review paper comprehensively summarizes healthcare provider (HCP) evaluation of explanations produced by explainable artificial intelligence methods to support point-of-care, patient-specific, clinical decision-making (CDM) within medical settings. It highlights the critical need to incorporate human-centered (HCP) evaluation approaches based on their CDM needs, processes, and goals. Materials and Methods The review was conducted in Ovid Medline and Scopus databases, following the Institute of Medicine’s methodological standards and PRISMA guidelines. An individual study appraisal was conducted using design-specific appraisal tools. MaxQDA software was used for data extraction and evidence table procedures. Results Of the 2673 unique records retrieved, 25 records were included in the final sample. Studies were excluded if they did not meet this review’s definitions of HCP evaluation (1156), healthcare use (995), explainable AI (211), and primary research (285), and if they were not available in English (1). The sample focused primarily on physicians and diagnostic imaging use cases and revealed wide-ranging evaluation measures. Discussion The synthesis of sampled studies suggests a potential common measure of clinical explainability with 3 indicators of interpretability, fidelity, and clinical value. There is an opportunity to extend the current model-centered evaluation approaches to incorporate human-centered metrics, supporting the transition into practice. Conclusion Future research should aim to clarify and expand key concepts in HCP evaluation, propose a comprehensive evaluation model positioned in current theoretical knowledge, and develop a valid instrument to support comparisons. ","This review synthesizes 25 studies on human-centered evaluation of explainable AI in clinical decision-making, highlighting the need for HCP-centric approaches and proposing a potential common measure with 3 indicators: interpretability, fidelity, and clinical value."
"Improving Explainability and Integrability of Medical Artificial Intelligence to promote healthcare professional acceptance and usage: A mixed systematic review (Preprint)","https://scispace.com/paper/improving-explainability-and-integrability-of-medical-4rzbqlhoktg2","2025","Journal Article","Journal of Medical Internet Research","Yushu Liu
Chenxi Liu
Jianing Zheng
Chang Xu
Dan Wang","10.2196/73374","","BACKGROUND
The integration of artificial intelligence (AI) in health care has significant potential, yet its acceptance by health care professionals (HCPs) is essential for successful implementation. Understanding HCPs' perspectives on the explainability and integrability of medical AI is crucial, as these factors influence their willingness to adopt and effectively use such technologies.


OBJECTIVE
This study aims to improve the acceptance and use of medical AI. From a user perspective, it explores HCPs' understanding of the explainability and integrability of medical AI.


METHODS
We performed a mixed systematic review by conducting a comprehensive search in the PubMed, Web of Science, Scopus, IEEE Xplore, and ACM Digital Library and arXiv databases for studies published between 2014 and 2024. Studies concerning an explanation or the integrability of medical AI were included. Study quality was assessed using the Joanna Briggs Institute critical appraisal checklist and Mixed Methods Appraisal Tool, with only medium- or high-quality studies included. Qualitative data were analyzed via thematic analysis, while quantitative findings were synthesized narratively.


RESULTS
Out of 11,888 records initially retrieved, 22 (0.19%) studies met the inclusion criteria. All selected studies were published from 2020 onward, reflecting the recency and relevance of the topic. The majority (18/22, 82%) originated from high-income countries, and most (17/22, 77%) adopted qualitative methodologies, with the remainder (5/22, 23%) using quantitative or mixed method approaches. From the included studies, a conceptual framework was developed that delineates HCPs' perceptions of explainability and integrability. Regarding explainability, HCPs predominantly emphasized postprocessing explanations, particularly aspects of local explainability such as feature relevance and case-specific outputs. Visual tools that enhance the explainability of AI decisions (eg, heat maps and feature attribution) were frequently mentioned as important enablers of trust and acceptance. For integrability, key concerns included workflow adaptation, system compatibility with electronic health records, and overall ease of use. These aspects were consistently identified as primary conditions for real-world adoption.


CONCLUSIONS
To foster wider adoption of AI in clinical settings, future system designs must center on the needs of HCPs. Enhancing post hoc explainability and ensuring seamless integration into existing workflows are critical to building trust and promoting sustained use. The proposed conceptual framework can serve as a practical guide for developers, researchers, and policy makers in aligning AI solutions with frontline user expectations.


TRIAL REGISTRATION
PROSPERO CRD420250652253; https://www.crd.york.ac.uk/PROSPERO/view/CRD420250652253.","This mixed systematic review (n=22 studies) identifies key factors influencing healthcare professionals' acceptance and usage of medical AI, including postprocessing explanations, visual tools, and seamless integration into existing workflows, to promote trust and sustained use."
"Personalized health monitoring using explainable AI: bridging trust in predictive healthcare","https://scispace.com/paper/personalized-health-monitoring-using-explainable-ai-bridging-e7qhqiaag9vw","2025","Journal Article","Dental science reports","M. Sree Vani
R. Sudhakar
A. Mahendar
Sukanya Ledalla
Mustafa Radha
M. Sunitha","10.1038/s41598-025-15867-z","","AI has propelled the potential for moving toward personalized health and early prediction of diseases. Unfortunately, a significant limitation of many of these deep learning models is that they are not interpretable, restricting their clinical utility and undermining trust by clinicians. However, all existing methods are non-informative because they report generic or post-hoc explanations, and few or none support patient-specific, accurate, individualized patient-level explanations. Furthermore, existing approaches are often restricted to static, limited-domain datasets and are not generalizable across various healthcare scenarios. To tackle these problems, we propose a new deep learning approach called PersonalCareNet for personalized health monitoring based on the MIMIC-III clinical dataset. Our system jointly models convolutional neural networks with attention (CHARMS) and employs SHAP (Shapley Additive exPlanations) to obtain global and patient-specific model interpretability. We believe the model, enabled to leverage many clinical features, would offer clinically interpretable insights into the contribution of features while supporting real-time risk prediction, thus increasing transparency and instilling clinically-oriented trust in the model. We provide an extensive evaluation that shows PersonalCareNet achieves 97.86% accuracy, exceeding multiple notable SoTA healthcare risk prediction models. Explainability at Both Local and Global Level The framework offers explainability at local (using various matrix heatmaps for diagnosing models, such as force plots, SHAP summary visualizations, and confusion matrix-based diagnostics) and also at a global level through feature importance plots and Top-N list visualizations. As a result, we show quantitative results, demonstrating that much of the improvement can be achieved without paying a high price for interpretability. We have proposed a cost-effective and systematic approach as an AI-based platform that is scalable, accurate, transparent, and interpretable for critical care and personalized diagnostics. PersonalCareNet, by filling the void between performance and interpretability, promises a significant advancement in the field of reliable and clinically validated predictive healthcare AI. The design allows for additional extension to multiple data types and real-time deployment at the edge, creating a broader impact and adaptability. ","This study proposes PersonalCareNet, a deep learning approach for personalized health monitoring, leveraging MIMIC-III data and SHAP explanations to provide global and patient-specific model interpretability, achieving 97.86% accuracy and increasing transparency in predictive healthcare AI."
"Towards Personalized AI-Based Diabetes Therapy: A Review","https://scispace.com/paper/towards-personalized-ai-based-diabetes-therapy-a-review-45t9not5wti5","2024","Journal Article","IEEE Journal of Biomedical and Health Informatics","Sara Campanella
Giovanni Paragliola
Valentino Cherubini
Paola Pierleoni
Lorenzo Palma","10.1109/jbhi.2024.3443137","","Insulin pumps and other smart devices have recently made significant advancements in the treatment of diabetes, a disorder that affects people all over the world. The development of medical AI has been influenced by AI methods designed to help physicians make diagnoses, choose a course of therapy, and predict outcomes. In this article, we thoroughly analyse how AI is being used to enhance and personalize diabetes treatment. The search turned up 77 original research papers, from which we've selected the most crucial information regarding the learning models employed, the data typology, the deployment stage, and the application domains. We identified two key trends, enabled mostly by AI: patient-based therapy personalization and therapeutic algorithm optimization. In the meanwhile, we point out various shortcomings in the existing literature, like a lack of multimodal database analysis or a lack of interpretability. The rapid improvements in AI and the expansion of the amount of data already available offer the possibility to overcome these difficulties shortly and enable a wider deployment of this technology in clinical settings. ","This review examines AI-based advancements in diabetes treatment, analyzing 77 papers on personalized therapy, learning models, and data typology, highlighting trends and limitations in AI-assisted diabetes care and its potential for clinical deployment."
"Prediction of insulin requirements by explainable machine learning for individuals with type 1 diabetes","https://scispace.com/paper/prediction-of-insulin-requirements-by-explainable-machine-30fblxcfckgr","2024","Journal Article","The Journal of Clinical Endocrinology and Metabolism","Kai Yoshimura
Yushi Hirota
Shuichiro Saito
Seiji Nishikage
Akane Yamamoto
Tomofumi Takayoshi
Shin Urai
Wataru Ogawa","10.1210/clinem/dgae863","","Adverse events related to insulin therapy remain common in individuals with type 1 diabetes. Optimization of insulin dose can reduce the frequency of these events and help to prevent macrovascular and microvascular complications. The aim of the present study was to develop machine learning models to predict the total daily dose (TDD) of insulin on the basis of data available in routine clinical practice, to evaluate the performance of the models, and to interpret the relation between its predictions and features. ","This study develops and evaluates explainable machine learning models to predict insulin requirements for individuals with type 1 diabetes, aiming to optimize insulin dose and prevent complications, using routine clinical data to inform predictions and feature interpretation."
"Glucose interpretation meaning and action: enhancing type 1 diabetes decision-making with textual descriptions","https://scispace.com/paper/glucose-interpretation-meaning-and-action-enhancing-type-1-44zmycfjes7j","2025","Journal Article","Therapeutic Advances in Endocrinology and Metabolism","Rujiravee Kongdee
Bijan Parsia
Hood Thabit
Simon Harper","10.1177/20420188251362089","","Findings from our previous study indicate that people with type 1 diabetes mellitus (T1DM) unknowingly misinterpret data displayed on glucose monitoring systems and make inaccurate treatment decisions, which increases the risk of hospitalisation. This study aims to assess the effectiveness of incorporating textual descriptions in glucose monitoring systems compared to existing systems. The main goal is to minimise the effort required in glucose data interpretation, facilitating better self-management and ultimately improving haemoglobin A1C levels. A two-arm and mixed-methods evaluation was conducted. Participants were randomly allocated to the control arm (existing systems) or the experimental arm (newly developed systems incorporating textual descriptions). In the first part, a task-based usability assessment was conducted to compare performance between the two arms. The second part evaluated participant preferences, agreement with textual descriptions and perceptions of the new systems. A total of 86 participants were recruited. The experimental arm achieved an 85.15% total correctness score, compared to 74.38% in the control arm (p < 0.001). The experimental arm particularly outperformed the control arm in the ambiguous tasks, such as compression low. However, despite a higher performance and greater agreement with the textual descriptions, the experimental group exhibited a less favourable perception compared to the control group. Incorporating textual description into glucose monitoring systems enhances treatment decision-making for people with T1DM. It suggests that we are on the right path to helping them better understand their glucose data and assist their self-management. Extensive research is required to focus more on the patient-centred approach in information presentation and prioritise it in parallel with other advancements in glucose monitoring technologies. ","This study evaluates the effectiveness of incorporating textual descriptions into glucose monitoring systems for people with type 1 diabetes, finding improved accuracy and agreement with descriptions, but mixed perceptions, suggesting a patient-centred approach is needed for optimal information presentation."
"Leveraging EHR Data and Up-to-Date Clinical Guidelines for Highly Accurate and Practical Clinical Diabetes Drug and Dosage Recommendation System","https://scispace.com/paper/leveraging-ehr-data-and-up-to-date-clinical-guidelines-for-imkuw2lb7vhg","2025","Journal Article","Methods of Information in Medicine","Jhing-Fa Wang
Mei‐Lin Wei
Tzu‐Chun Yeh
T.‐C. Chiang
Hong‐I Chen
Yuan‐Teh Lee
Eric C. K. Cheng","10.1055/a-2707-2862","","Background: Existing drug recommendation systems lack integration with up-to-date clinical guidelines (the latest diabetes association standards of care and clinical guidelines that align with local government healthcare regulations) and lack high-precision drug interaction processing, explainability, and dynamic dosage adjustment. As a result, the recommendations generated by these systems are often inaccurate and do not align with local standards, greatly limiting their practicality. Objective: To develop a personalized drug recommendation and dosage optimization system named Diabetes Drug Recommendation System (DDRs), integrating FHIR-standardized EHR data and up-to-date clinical guidelines for accurate and practical recommendations. Methods: We analyzed patients' EHR and ICD-10 codes and integrated them with a drug interaction database to reduce adverse reactions. ADA guidelines and Taiwan’s NHI chronic disease guidelines served as data sources. Bio-GPT and RAG were used to build the clinical guideline database and ensure recommendations align with the latest standards, with references provided for interpretability. Finally, optimal dosage was dynamically calculated by integrating patient disease progression trends from the EHR. Result: DDRs achieved superior drug recommendation accuracy (PRAUC = 0.7951, Jaccard = 0.5632, F1-score = 0.7158), with a low DDI rate (4.73%) and dosage error (±6.21%). Faithfulness of recommendations reached 0.850. Field validation with three physicians showed that the system reduced literature review time by 30–40% and delivered clinically actionable recommendations. Conclusion: DDRs is the first system to integrate EHR data, LLMs, RAG, ADA guidelines, and Taiwan NHI policies for diabetes treatment. The system demonstrates high accuracy, safety, and interpretability, offering practical decision support in routine clinical settings. ","A novel Diabetes Drug Recommendation System (DDRs) integrates EHR data, up-to-date clinical guidelines, and AI models to provide accurate, safe, and interpretable recommendations, outperforming existing systems in accuracy and reducing literature review time by 30-40%."
"Designing Clinical Decision Support Systems (CDSS): A User-Centred Lens of Design Characteristics, Challenges, and Implications—A Systematic Review (Preprint)","https://scispace.com/paper/designing-clinical-decision-support-systems-cdss-a-user-wsr95u906o3o","2025","Journal Article","Journal of Medical Internet Research","Andrew A. Bayor
Jane Li
Ian A. Yang
Marlien Varnfield","10.2196/63733","","Abstract Background Clinical decision support systems (CDSS) have the potential to play a crucial role in enhancing health care quality by providing evidence-based information to clinicians at the point of care. Despite their increasing popularity, there is a lack of comprehensive research exploring their design characterization and trends. This limits our understanding and ability to optimize their functionality, usability, and adoption in health care settings. Objective This systematic review examined the design characteristics of CDSS from a user-centered perspective, focusing on user-centered design (UCD), user experience (UX), and usability, to identify related design challenges and provide insights into the implications for future design of CDSS. Methods This review followed the PRISMA (Preferred Reporting Items for Systematic Reviews and Meta-Analyses) recommendations and used a grounded theory analytical approach to guide the conduct, data analysis, and synthesis. A search of 4 major electronic databases (PubMed, Web of Science, Scopus, and IEEE Xplore) was conducted for papers published between 2013 and 2023, using predefined design-focused keywords (design, UX, implementation, evaluation, usability, and architecture). Papers were included if they focused on a designed CDSS for a health condition and discussed the design and UX aspects (eg, design approach, architecture, or integration). Papers were excluded if they solely covered technical implementation or architecture (eg, machine learning methods) or were editorials, reviews, books, conference abstracts, or study protocols. Results Out of 1905 initially identified papers, 40 passed screening and eligibility checks for a full review and analysis. Analysis of the studies revealed that UCD is the most widely adopted approach for designing CDSS, with all design processes incorporating functional or usability evaluation mechanisms. The CDSS reported were mainly clinician-facing and mostly stand-alone systems, with their design lacking consideration for integration with existing clinical information systems and workflows. Through a UCD lens, four key categories of challenges relevant to CDSS design were identified: (1) usability and UX, (2) validity and reliability, (3) data quality and assurance, and (4) design and integration complexities. Notably, a subset of studies incorporating Explainable artificial intelligence highlighted its emerging role in addressing key challenges related to validity and reliability by fostering explainability, transparency, and trust in CDSS recommendations, while also supporting collaborative validation with users. Conclusions While CDSS show promise in enhancing health care delivery, identified challenges have implications for their future design, efficacy, and utilization. Adopting pragmatic UCD design approaches that actively involve users is essential for enhancing usability and addressing identified UX challenges. Integrating with clinical systems is crucial for interoperability and presents opportunities for AI-enabled CDSS that rely on large patient data. Incorporating emerging technologies such as Explainable Artificial Intelligence can boost trust and acceptance. Enabling functionality for CDSS to support both clinicians and patients can create opportunities for effective use in virtual care. ","This systematic review examines user-centered design characteristics of Clinical Decision Support Systems (CDSS), identifying challenges and implications for future design, highlighting the importance of usability, integration, and emerging technologies like Explainable AI for enhancing healthcare delivery."
"Closed-Loop Control, Artificial Intelligence-Based Decision-Support Systems, and Data Science.","https://scispace.com/paper/closed-loop-control-artificial-intelligence-based-decision-f556siblzq","2024","Journal Article","Diabetes Technology & Therapeutics","Revital Nimri
Moshe Phillip
Mark A Clements
Boris Kovatchev","10.1089/dia.2024.2505","","Diabetes Technology & TherapeuticsVol. 26, No. S1 Original ArticlesFree AccessClosed-Loop Control, Artificial Intelligence–Based Decision-Support Systems, and Data ScienceRevital Nimri, Moshe Phillip, Mark A. Clements, and Boris KovatchevRevital NimriDiabetes Technology Center, Jesse Z and Sara Lea Shafer Institute for Endocrinology and Diabetes, Schneider Children's Medical Center of Israel, Petah Tikva, Israel.Faculty of Medicine, Tel Aviv University, Tel Aviv, Israel.Search for more papers by this author, Moshe PhillipDiabetes Technology Center, Jesse Z and Sara Lea Shafer Institute for Endocrinology and Diabetes, Schneider Children's Medical Center of Israel, Petah Tikva, Israel.Faculty of Medicine, Tel Aviv University, Tel Aviv, Israel.Search for more papers by this author, Mark A. ClementsDivision of Pediatric Endocrinology, Children's Mercy Hospitals and Clinics, Kansas City, MO, USA.Search for more papers by this author, and Boris KovatchevCenter for Diabetes Technology, School of Medicine, University of Virginia, Charlottesville, VA, USA.Search for more papers by this authorPublished Online:1 Mar 2024https://doi.org/10.1089/dia.2024.2505AboutSectionsPDF/EPUB Permissions & CitationsPermissionsDownload CitationsTrack CitationsAdd to favorites Back To Publication ShareShare onFacebookXLinked InRedditEmail IntroductionIncreasing academic and industrial effort is being focused on interfacing three distinct sources of health data: (1) electronic medical and laboratory records reflecting phenotype and health state, (2) wearable sensors and mobile devices monitoring health and lifestyle in real-time, and (3) genomic, transcriptomic, or metabolomic data. Genomic information is used to discover heritable risk factors or features applied in precision medicine to guide treatment decisions. This information is fixed in time for any individual. By contrast, periodically updated medical records and especially the use of real-time health-sensing devices provide accurate tracking of health and disease over time. The technological trend toward enabling consumer electronics with capabilities for sensing physiological signals, such as motion, heart rate, or blood sugar levels, is creating a vast data space. If properly processed, these real-time data can fundamentally change research paradigms and transform the clinical practice.However, more data are not always better. With the increased data volume and complexity, the challenge becomes how to extract information relevant to the condition of a particular patient at a particular time. The classic pathway of medical logic of data → information → decision becomes difficult to follow with the traditional methods of biostatistics. New approaches are needed and are fortunately available, ranging from modeling, simulation, and optimal control methods, to rapidly developing data science tools.Before continuing further, we should emphasize that, arguably, diabetes mellitus is the best quantified human condition. In the past 40 years, metabolic monitoring technologies have progressed from occasional assessment of average glycemia via glycated hemoglobin (HbA1c), to blood glucose monitoring a few times a day, to continuous glucose monitoring (CGM) producing data points every few minutes—time series tracking the dynamics of the metabolic system. The high temporal resolution of CGM data has enabled advanced treatments such as decision support assisting insulin injection or oral medication, or automated closed-loop control known as the ""artificial pancreas."" Sophisticated metabolic models and simulators are available as well.In this article, we review the progress of data technologies for diabetes from July 1, 2022, to June 30, 2023. We structured the results of this review in three sections: (1) closed-loop control, or automated insulin delivery (AID), which appears to be the term preferred recently; (2) decision-support systems (DSS), particularly those that use contemporary methods such as artificial intelligence, and (3) data acquisition, engineering, analytics, and visualization, which are all data science tools increasingly applied to the retrieval of electronic medical records (EMR) and real-time disease-tracking information.Key Articles ReviewedConsensus Recommendations for the Use of Automated Insulin Delivery Technologies in Clinical PracticePhillip M, Nimri R, Bergenstal RM, Barnard-Kelly K, Danne T, Hovorka R, Kovatchev BP, Messer LH, Parkin CG, Ambler-Osborn L, Amiel SA, Bally L, Beck RW, Biester S, Biester T, Blanchette JE, Bosi E, Boughton CK, Breton MD, Brown SA, Buckingham BA, Cai A, Carlson AL, Castle JR, Choudhary P, Close KL, Cobelli C, Criego AB, Davis E, de Beaufort C, de Bock MI, DeSalvo DJ, DeVries JH, Dovc K, Doyle FJ 3rd, Ekhlaspour L, Shvalb NF, Forlenza GP, Gallen G, Garg SK, Gershenoff DC, Gonder-Frederick LA, Haidar A, Hartnell S, Heinemann L, Heller S, Hirsch IB, Hood KK, Isaacs D, Klonoff DC, Kordonouri O, Kowalski A, Laffel L, Lawton J, Lal RA, Leelarathna L, Maahs DM, Murphy HR, Nørgaard K, O'Neal D, Oser S, Oser T, Renard E, Riddell MC, Rodbard D, Russell SJ, Schatz DA, Shah VN, Sherr JL, Simonson GD, Wadwa RP, Ward C, Weinzimer SA, Wilmot EG, Battelino TEndoc Rev2023; 44:254–280Open-Source Automated Insulin Delivery in Type 1 DiabetesBurnside MJ, Lewis DM, Crocket HR, Meier RA, Williman JA, Sanders OJ, Jefferies CA, Faherty AM, Paul RG, Lever CS, Price SKJ, Frewen CM, Jones SD, Gunn TC, Lampey C, Wheeler BJ, de Bock MIN Engl J Med2022; 387:869–881Trial of Hybrid Closed-Loop Control in Young Children with Type 1 DiabetesWadwa RP, Reed ZW, Buckingham BA, DeBoer MD, Ekhlaspour L, Forlenza GP, Schoelwer M, Lum J, Kollman C, Beck RW, Breton MD, for the PEDAP Trial Study GroupN Engl J Med2023; 388:991–1001Multicenter, Randomized Trial of a Bionic Pancreas in Type 1 DiabetesBionic Pancreas Research Group; Russell SJ, Beck RW, Damiano ER, El-Khatib FH, Ruedy KJ, Balliro CA, Li Z, Calhoun P, Wadwa RP, Buckingham B, Zhou K, Daniels M, Raskin P, White PC, Lynch J, Pettus J, Hirsch IB, Goland R, Buse JB, Kruger D, Mauras N, Muir A, McGill JB, Cogen F, Weissberg-Benchell J, Sherwood JS, Castellanos LE, Hillard MA, Tuffaha M, Putman MS, Sands MY, Forlenza G, Slover R, Messer LH, Cobry E, Shah VN, Polsky S, Lal R, Ekhlaspour L, Hughes MS, Basina M, Hatipoglu B, Olansky L, Bhangoo A, Forghani N, Kashmiri H, Sutton F, Choudhary A, Penn J, Jafri R, Rayas M, Escaname E, Kerr C, Favela-Prezas R, Boeder S, Trikudanathan S, Williams KM, Leibel N, Kirkman MS, Bergamo K, Klein KR, Dostou JM, Machineni S, Young LA, Diner JC, Bhan A, Jones JK, Benson M, Bird K, Englert K, Permuy J, Cossen K, Felner E, Salam M, Silverstein JM, Adamson S, Cedeno A, Meighan S, Dauber AN Engl J Med2022; 387:1161–1172Closed-Loop Therapy and Preservation of C-peptide Secretion in Type 1 DiabetesBoughton CK, Allen JM, Ware J, Wilinska ME, Hartnell S, Thankamony A, Randell T, Ghatak A, Besser REJ, Elleri D, Trevelyan N, Campbell FM, Sibayan J, Calhoun P, Bailey R, Dunseath G, Hovorka R; CLOuD ConsortiumN Engl J Med2022; 387:882–893Clinical Decision Support for Glycemic Management Reduces Hospital Length of StayPichardo-Lowden AR, Haidet P, Umpierrez GE, Lehman EB, Quigley FT, Wang L, Rafferty CM, DeFlitch CJ, Chinchilli VMDiabetes Care2022; 45:2526–2534Assessment of a Decision Support System for Adults with Type 1 Diabetes on Multiple Daily Insulin InjectionsCastle JR, Wilson LM, Tyler NS, Espinoza AZ, Mosquera-Lopez CM, Kushner T, Young GM, Pinsonault J, Dodier RH, Hilts WW, Oganessian SM, Branigan DL, Gabo VB, Eom JH, Ramsey K, Youssef JE, Cafazzo JA, Winters-Stone K, Jacobs PGDiabetes Technol Ther2022; 24:892–897Safety and Efficacy of an Adaptive Bolus Calculator for Type 1 Diabetes: A Randomized Controlled Crossover StudyUnsworth R, Armiger R, Jugnee N, Thomas M, Herrero P, Georgiou P, Oliver N, Reddy MDiabetes Technol Ther2023; 25:414–425Comparative Effectiveness of Team-based Care with and without a Clinical Decision Support System for Diabetes Management: A Cluster Randomized TrialShi X, He J, Lin M, Liu C, Yan B, Song H, Wang C, Xiao F, Huang P, Wang L, Li Z, Huang Y, Zhang M, Chen CS, Obst K, Shi L, Li W, Yang S, Yao G, Li XAnn Intern Med2023; 176:49–58Potential and Pitfalls of ChatGPT and Natural-Language Artificial Intelligence Models for Diabetes EducationSng GGR, Tung JYM, Lim DYZ, Bee YMDiabetes Care2023; 46:e103–e105Assessment of the Glucose Management Indicator Using Different Sampling DurationsBailey R, Calhoun P, Bergenstal RM, Beck RWDiabetes Technol Ther2023; 25:148–150Personalized Glycated Hemoglobin in Diabetes Management: Closing the Gap with Glucose Management IndicatorDunn TC, Xu Y, Bergenstal RM, Ogawa W, Ajjan RADiabetes Technol Ther2023; 25 (Suppl 3): S65–S74A New Index of Insulin Sensitivity from Glucose Sensor and Insulin Pump Data: In silico and In vivo Validation in Youths with Type 1 DiabetesSchiavon M, Galderisi A, Basu A, Kudva YC, Cengiz E, Dalla Man CDiabetes Technol Ther2023; 25:270–278The Launch of the iCoDE Standard ProjectXu NY, Nguyen KT, DuBord AY, Klonoff DC, Goldman JM, Shah SN, Spanakis EK, Madlock-Brown C, Sarlati S, Rafiq A, Wirth A, Kerr D, Khanna R, Weinstein S, Espinoza JJ Diabetes Sci Technol2022; 16:887–895Adding Glycemic and Physical Activity Metrics to a Multimodal Algorithm-enabled Decision Support Tool for Type 1 Diabetes Care: Keys to Implementation and OpportunitiesZaharieva DP, Senanayake R, Brown C, Watkins B, Loving G, Prahalad P, Ferstad JO, Guestrin C, Fox EB, Maahs DM, Scheinker D, the 4T Research TeamFront Endocrinol (Lausanne)2023; 13:1096325An ""All-Data-on-Hand"" Deep Learning Model to Predict Hospitalization for Diabetic Ketoacidosis in Youth with Type 1 Diabetes: Development and Validation StudyWilliams DD, Ferro D, Mullaney C, Skrabonja L, Barnes MS, Patton SR, Lockee B, Tallon EM, Vandervelden CA, Schweisberger C, Mehta S, McDonough R, Lind M, D'Avolio L, Clements MAJMIR Diabetes2023; 8:e47592Closed-Loop Control of Diabetes, or AIDA PubMed search on artificial pancreas, or AID, or closed loop in diabetes identified 397 results for the period of July 1, 2022, to June 30, 2023. From these, we selected 40 articles of interest (e.g., ∼10%) for inclusion in the reference list of this article. Five of these articles are reviewed in more detail in the following pages (1–5). It is worth noting that four of them are published in the prestigious New England Journal of Medicine (impact factor, 176.082) (2–5) and signify substantial clinical trials by different research groups. Publication of these studies in such high-ranking journals was rare until recently, and we believe the AID literature this past year has achieved a new record in terms of its impact on medical science.A number of other notable studies were published as well (6–15) that tested various AID systems in diverse conditions, such as the inpatient setting (7), with type 2 diabetes (T2D) (9,10), with long-standing type 1 diabetes (T1D) with hypoglycemia unawareness (12), or in very young children (14). The large randomized trials (> 100 participants) used traditional designs to reconfirm the advantages of hybrid closed loop over conventional therapy (6,15). Meta-analyses (16–18) and comprehensive reviews (19–23) solidified AID as an advanced, effective, and cost-effective (24) treatment of T1D and T2D. International consortiums published clinical guidelines for the use of AID in the clinical practice (1,25–27). And several studies reported real-life data for various AID systems (28–35), with one study reporting data for nearly 20,000 AID users (28).The investigation of using ultra-rapid-acting insulin has continued with clinical trials comparing faster insulin aspart with standard insulin aspart (36–38) and yielding mixed results—in some studies no effect was shown (36), in others ultra-rapid insulin contributed to better control (38). A new study using a sodium-glucose cotransporter 2 (SGLT-2) inhibitor in addition to insulin in people with T1D confirmed previous results and found that ""empagliflozin at 2.5 and 5 mg increased time in range during hybrid closed-loop therapy by 11–13 percentage points compared with placebo in those who otherwise were unable to attain glycemic targets"" (39). And last, but not least, a study on AID performance during swimming was published, which also presents interesting data about the interdevice (sensor–pump) communication in water (40).Consensus Recommendations for the Use of Automated Insulin Delivery Technologies in Clinical PracticePhillip M1,2*, Nimri R1,2*, Bergenstal RM3, Barnard-Kelly K4, Danne T5, Hovorka R6, Kovatchev BP7, Messer LH8, Parkin CG9, Ambler-Osborn L10, Amiel SA11, Bally L12, Beck RW13, Biester S5, Biester T5, Blanchette JE14,15, Bosi E16, Boughton CK17, Breton MD7, Brown SA7,18, Buckingham BA19, Cai A20, Carlson AL3, Castle JR21, Choudhary P22, Close KL20, Cobelli C23, Criego AB3, Davis E24, de Beaufort C25, de Bock MI26, DeSalvo DJ27, DeVries JH28, Dovc K29, Doyle FJ 3rd30, Ekhlaspour L31, Shvalb NF1, Forlenza GP8, Gallen G11, Garg SK8, Gershenoff DC3, Gonder-Frederick LA7, Haidar A32, Hartnell S33, Heinemann L34, Heller S35, Hirsch IB36, Hood KK37, Isaacs D38, Klonoff DC39, Kordonouri O5, Kowalski A40, Laffel L10, Lawton J41, Lal RA42, Leelarathna L43, Maahs DM19, Murphy HR44, Nørgaard K45, O'Neal D46, Oser S47, Oser T47, Renard E48, Riddell MC49, Rodbard D50, Russell SJ51, Schatz DA52, Shah VN8, Sherr JL53, Simonson GD3, Wadwa RP8, Ward C54, Weinzimer SA53, Wilmot EG55,56, Battelino T29*Contributed equally to the manuscript and are both corresponding authors 1The Jesse Z and Sara Lea Shafer Institute for Endocrinology and Diabetes, National Center for Childhood Diabetes, Schneider Children's Medical Center of Israel, Petah Tikva, Israel; 2Sacker Faculty of Medicine, Tel-Aviv University, Tel-Aviv, Israel; 3International Diabetes Center, HealthPartners Institute, Minneapolis, MN; 4Southern Health NHS Foundation Trust, Southampton, UK; 5AUF DER BULT, Diabetes-Center for Children and Adolescents, Endocrinology and General Paediatrics, Hannover, Germany; 6Wellcome Trust-MRC Institute of Metabolic Science, University of Cambridge, Cambridge, UK; 7Center for Diabetes Technology, School of Medicine, University of Virginia, Charlottesville, VA; 8Barbara Davis Center for Diabetes, University of Colorado Denver—Anschutz Medical Campus, Aurora, CO; 9CGParkin Communications, Inc., Henderson, NV; 10Joslin Diabetes Center, Harvard Medical School, Boston, MA; 11Department of Diabetes, King's College London, London, UK; 12Department of Diabetes, Endocrinology, Nutritional Medicine and Metabolism, Bern University Hospital and University of Bern, Bern, Switzerland; 13Jaeb Center for Health Research Foundation, Inc., Tampa, FL; 14College of Nursing, University of Utah, Salt Lake City, UT; 15Center for Diabetes and Obesity, University Hospitals Cleveland Medical Center, Cleveland, OH; 16Diabetes Research Institute, IRCCS San Raffaele Hospital and San Raffaele Vita Salute University, Milan, Italy; 17Wellcome Trust-MRC Institute of Metabolic Science, Addenbrooke's Hospital, University of Cambridge Metabolic Research Laboratories, Cambridge, UK; 18Division of Endocrinology, University of Virginia, Charlottesville, VA; 19Division of Endocrinology, Department of Pediatrics, Stanford University, School of Medicine, Stanford, CA; 20The diaTribe Foundation/Close Concerns, San Diego, CA; 21Harold Schnitzer Diabetes Health Center, Oregon Health & Science University, Portland, OR; 22Diabetes Research Centre, University of Leicester, Leicester, UK; 23Department of Woman and Child's Health, University of Padova, Padova, Italy; 24Telethon Kids Institute, University of Western Australia, Perth Children's Hospital, Perth, Australia; 25Diabetes & Endocrine Care Clinique Pédiatrique DECCP/Centre Hospitalier Luxembourg, and Faculty of Sciences, Technology and Medicine, University of Luxembourg, Esch sur Alzette, GD Luxembourg/Department of Paediatrics, UZ-VUB, Brussels, Belgium; 26Department of Paediatrics, University of Otago, Christchurch, New Zealand; 27Division of Pediatric Diabetes and Endocrinology, Baylor College of Medicine, Texas Children's Hospital, Houston, TX ; 28Amsterdam UMC, University of Amsterdam, Internal Medicine, Amsterdam, The Netherlands; 29Department of Pediatric Endocrinology, Diabetes and Metabolic Diseases, UMC - University Children's Hospital, Ljubljana, Slovenia, and Faculty of Medicine, University of Ljubljana, Ljubljana, Slovenia; 30Harvard John A. Paulson School of Engineering and Applied Sciences, Harvard University, Cambridge, MA; 31Lucile Packard Children's Hospital—Pediatric Endocrinology, Stanford University School of Medicine, Palo Alto, CA; 32Department of Biomedical Engineering, McGill University, Montreal, Canada; 33Wolfson Diabetes and Endocrine Clinic, Cambridge University Hospitals NHS Foundation Trust, Cambridge, UK; 34Science Consulting in Diabetes GmbH, Kaarst, Germany; 35Department of Oncology and Metabolism, University of Sheffield, Sheffield, UK; 36Department of Medicine, University of Washington Diabetes Institute, University of Washington, Seattle, WA; 37Stanford Diabetes Research Center, Stanford University School of Medicine, Stanford, CA; 38Cleveland Clinic, Endocrinology and Metabolism Institute, Cleveland, OH; 39Diabetes Research Institute, Mills-Peninsula Medical Center, San Mateo, CA; 40JDRF International, New York, NY; 41Usher Institute, University of Edinburgh, Edinburgh, UK; 42Division of Endocrinology, Department of Pediatrics, Stanford University School of Medicine, Stanford, CA; 43Manchester University Hospitals NHS Foundation Trust/University of Manchester, Manchester, UK; 44Norwich Medical School, University of East Anglia, Norwich, UK; 45Steno Diabetes Center Copenhagen and Department of Clinical Medicine, University of Copenhagen, Gentofte, Denmark; 46Department of Medicine and Department of Endocrinology, St Vincent's Hospital Melbourne, University of Melbourne, Melbourne, Australia; 47Department of Family Medicine, University of Colorado School of Medicine, Anschutz Medical Campus, Aurora, CO; 48Department of Endocrinology, Diabetes, Nutrition, Montpellier University Hospital, and Institute of Functional Genomics, University of Montpellier, CNRS, INSERM, Montpellier, France; 49School of Kinesiology & Health Science, Muscle Health Research Centre, York University, Toronto, Canada; 50Biomedical Informatics Consultants LLC, Potomac, MD; 51Massachusetts General Hospital and Harvard Medical School, Boston, MA; 52Department of Pediatrics, College of Medicine, Diabetes Institute, University of Florida, Gainesville, FL; 53Department of Pediatrics, Yale University School of Medicine, Pediatric Endocrinology, New Haven, CT; 54Institute of Metabolic Science, Cambridge University Hospitals NHS Foundation Trust, Cambridge, UK; 55Department of Diabetes & Endocrinology, University Hospitals of Derby and Burton NHS Trust, Derby, UK; 56Division of Medical Sciences and Graduate Entry Medicine, University of Nottingham, Nottingham, England, UKEndoc Rev 2023;44:254–280The past 6 years have produced tremendous advances in automated insulin delivery (AID) technologies. Numerous randomized controlled trials and real-world studies have shown that the use of AID systems is safe and effective in helping individuals with diabetes achieve long-term glycemic goals while reducing hypoglycemia risk. AID systems have thus become an integral part of diabetes management, but recommendations for using AID systems in clinical settings have been lacking.MethodsIn 2021, an international panel of clinicians, researchers, and patient advocates with expertise in AID was organized by the Advanced Technologies & Treatments for Diabetes (ATTD) Congress to develop clinical guidelines for initiating AID for individuals with type 1 diabetes (T1D). The nine working groups addressed evolution of AID, clinical evidence, determining the target population for AID use, initiation of AID, education and training, utilization of AID, AID data reporting, psychological issues/user perspective, and the future of AID. The full panel voted on the working group recommendations, which became the basis of the consensus recommendations.ResultsThe report provides needed guidance to clinicians who are interested in using AID and (2) serves as a comprehensive review of evidence for payers to consider when determining eligibility criteria for AID insurance coverage.ConclusionsGuided recommendations are critical for AID success and acceptance, and all clinicians who treat diabetes need to become familiar with the available systems to eliminate disparities in diabetes quality of care. A comprehensive listing of the evidence payers should consider when determining eligibility criteria for AID insurance coverage is also provided.CommentsThese consensus recommendations published in Endocrine Reviews present the collective opinion of a select group of experts in AID (aka closed-loop control of diabetes) regarding the use of AID systems in clinical practice. This article is a highly recommended reading for anyone who wants to be introduced to the current technological and clinical state of AID systems, with a peek into their future as well.Following a review of the closed-loop field and an account of the evidence-based advantages of AID systems over other T1D therapies, the guidelines continue with sections explaining the different types of AID algorithms and the difference between hybrid and fully automated AID systems (HCL and FCL), the latter being defined as an AID system that does not require user involvement to function.The following are major takeaways from these consensus recommendations. (1) Evidence suggests that AID systems should be considered as a treatment option for all people with T1D to improve glycemic control, regardless of age, hypoglycemia awareness, pregnancy, or certain comorbidities. (2) To date, all clinically available systems are ""hybrid,"" meaning they require specific diabetes managements skills such as carbohydrate counting, so training and support for users and health-care providers are essential. (3) Interoperability between system components (e.g., sensors, algorithms, and insulin pumps) is important for the proliferation of these new technologies, but to a large extent this depends on the industry. (4) Early initiation of diabetes technologies in newly diagnosed T1D has been shown to improve and sustain long-term glycemic control, so early AID system initiation is encouraged. (5) The transition to an AID system should be individualized, and the initial settings of an AID system should be selected according to personal glycemic targets, based on recently acquired continuous glucose monitor (CGM) metrics. (6) Unified reporting of AID data is suggested, using clinically important glucose (e.g., time-in-range) and insulin (e.g., total daily insulin) metrics, plus the ambulatory glucose profile (AGP) usually aggregated over 14 days, which has become a standardized way to visualize CGM data. (7) Psychological issues related to the use of AID systems should be discussed and documented in the context of discontinuation or noncompliance with AID treatment.In conclusion, the consensus recommendations map future directions for AID development, including FCL, use of data science methods to analyze and comprehend the vast amounts of CGM and insulin-delivery data collected by AID systems, and learning adaptive AID algorithms that continually tailor the treatment to the changing behavior and physiology of their users. The consensus strongly recommends that all payers (government and private) should reimburse for AID systems as well as for AID education and training to support the management of people with T1D.Open-Source Automated Insulin Delivery in Type 1 DiabetesBurnside MJ1,3, Lewis DM11, Crocket HR4, Meier RA1, Williman JA2, Sanders OJ1,3, Jefferies CA6,7, Faherty AM6, Paul RG5, Lever CS5, Price SKJ5, Frewen CM8, Jones SD8, Gunn TC10, Lampey C6, Wheeler BJ8,9, de Bock MI1,31Departments of Pediatrics, University of Otago, Dunedin, New Zealand; 2Department of Population Health, University of Otago, Dunedin, New Zealand; 3Department of Pediatrics, Canterbury District Health Board Christchurch, New Zealand; 4Te Huataki Waiora School of Health, Sport and Human Performance, University of Waikato, Hamilton, New Zealand; 5Waikato Regional Diabetes Service, Waikato District Health Board, Hamilton, New Zealand; 6Department of Pediatric Endocrinology, Starship Children's Health, Auckland District Health Board, Auckland, New Zealand; 7Liggins Institute, University of Auckland, Auckland, New Zealand; 8Department of Women's and Children's Health, Dunedin School of Medicine, University of Otago, Dunedin, New Zealand; 9Pediatric Department, Southern District Health Board, Dunedin, New Zealand; 10Nightscout New Zealand, Hamilton, New Zealand; 11OpenAPS, Seattle, WAN Engl J Med 2022;387:869–881This study is also discussed in DIA-2024-2508, page S-117.People with type 1 diabetes (T1D) use open-source automated insulin delivery (AID) systems, and more data are needed on the efficacy and safety of these system.MethodsIn this multicenter, open-label, randomized, controlled trial, patients with T1D were assigned in a 1:1 ratio to use an open-source AID system or a sensor-augmented insulin pump (control). The participants included both children (defined as 7–15 years of age) and adults (defined as 16–70 years of age). A modified version of AndroidAPS 2.8 (with a standard OpenAPS 0.7.0 algorithm) paired with a preproduction DANA-i insulin pump and Dexcom G6 CGM, which has an Android smartphone application as the user interface, was used as the AID system. The primary outcome was the percentage of time in the target glucose range of 70–180 mg/dL (3.9–10.0 mmol/L) between days 155 and 168, the final 2 weeks of the trial.ResultsA total of 97 patients (48 children and 49 adults) underwent randomization, 44 to open-source AID and 53 to the control group. At 24 weeks, the mean time in the target range increased from 61.2% ± 12.3% SD to 71.2% ± 12.1% SD in the AID group and decreased from 57.7% ± 14.3% SD to 54.5% ± 16.0% SD in the control group (adjusted difference, 14 percentage points [95% CI, 9.2–18.8], P < 0.001), with no treatment effect according to age (P = 0.56). Participants in the AID group spent 3 hours and 21 minutes more in the target range per day compared with in the control group. There were no episodes of severe hypoglycemia or diabetic ketoacidosis in either group. Two patients in the AID group withdrew from the trial owing to connectivity issues.ConclusionsThe use of an open-source AID system in children and adults with T1D resulted in a significantly higher percentage of time in the target glucose range compared with the use of a sensor-augmented insulin pump.CommentsOpen-source AID (or Do-it-yourself Loop) algorithms have been available since before commercial AID systems appeared on the market. Typically, the handling of these open-source algorithms required above-average engineering and computer skills. The lack of clinical trials of open-source AID was also a reason these systems to remain unregulated (i.e., not formally approved by the U.S. Food and Drug Administration [FDA] or other regulatory agencies). This article in the prestigious New England Journal of Medicine aims to counter these shortcomings by presenting a randomized-controlled trial of one open-source algorithm running on a smartphone.The trial achieved results that are very similar to those in trials published to date for commercial AID systems: for instance, a 14-percentage-point increase in time in range up to 71%, a reduction of HbA1c of 0.5 percentage points, and a relatively low risk for hypoglycemia. The editorial that accompanied the article labeled the trial a ""path toward expanding treatment options for diabetes"" (41). As expected there were opposing opinions that emphasized the lack of regulatory approval of open-source AID: ""Unapproved products and algorithms raise the undesirable prospect of clinical harm and litigation"" (42). After the publication of this article, the latter argument became less critical owing to the FDA approval in January 2023 of Tidepool Loop—an FDA-regulated version of Loop, to be available in the iOS App Store, which is intended to work with commercially available insulin pumps and CGMs.This ATTD Yearbook article takes a neutral view on the open-source AID controversy: evidence has shown that these systems are not superior to commercial devices, and their more flexible customization features do not necessarily result in improved glycemic control. However, as noted in the previously mentioned editorial (41), open-source AID offers one more path toward expanding the treatment options for insulin-requiring diabetes, which can be appealing to a perhaps select group of people who are confident in their diabetes-management skills.Trial of Hybrid Closed-Loop Control in Young Children with Type 1 DiabetesWadwa RP1, Reed ZW2, Buckingham BA3, DeBoer MD5, Ekhlaspour L4, Forlenza GP1, Schoelwer M5, Lum J2, Kollman C2, Beck RW2, Breton MD5, for the PEDAP Trial Study Group1Barbara Davis Center for Diabetes, University of Colorado, Anschutz Medical Campus, Aurora, CO; 2Jaeb Center for Health Research, Tampa, FL; 3Department of Pediatrics, Division of Pediatric Endocrinology and Diabetes, Stanford University School of Medicine, Stanford, CA; 4Division of Pediatric Endocrinology, University of California, San Francisco, CA; 5University of Virginia Center for Diabetes Technology, Charlottesville, VAN Engl J Med 2023;388:991–1001This study is also discussed in DIA-2024-2508, page S-117.Closed-loop control systems of insulin delivery may improve glycemic outcomes in young children with type 1 diabetes (T1D), but the efficacy and safety of initiating a closed-loop s ","This article reviews the progress of data technologies for diabetes from July 2022 to June 2023, focusing on closed-loop control, artificial intelligence-based decision-support systems, and data science. It highlights 40 key articles, including randomized trials and real-world studies, demonstrating the safety and efficacy of automated insulin delivery systems in improving glycemic control and reducing hypoglycemia risk in individuals with type 1 diabetes."
"Towards secure and trusted AI in healthcare: A systematic review of emerging innovations and ethical challenges","https://scispace.com/paper/towards-secure-and-trusted-ai-in-healthcare-a-systematic-7ni7g3l8xg4e","2024","Journal Article","International Journal of Medical Informatics","Muhammad Mohsin Khan
Nirav N. Shah
Nissar Shaikh
Abdulnasser Thabet
Talal Alrabayah
Sirajeddin Belkhair","10.1016/j.ijmedinf.2024.105780","","Artificial Intelligence is in the phase of health care, with transformative innovations in diagnostics, personalized treatment, and operational efficiency. While having potential, critical challenges are apparent in areas of safety, trust, security, and ethical governance. The development of these challenges is important for promoting the responsible adoption of AI technologies into healthcare systems. This systematic review of studies published between 2010 and 2023 addressed the applications of AI in healthcare and their implications for safety, transparency, and ethics. A comprehensive search was performed in PubMed, IEEE Xplore, Scopus, and Google Scholar. Those studies that met the inclusion criteria provided empirical evidence, theoretical insights, or systematic evaluations addressing trust, security, and ethical considerations. The analysis brought out both the innovative technologies and the continued challenges. Explainable AI (XAI) emerged as one of the significant developments. It made it possible for healthcare professionals to understand AI-driven recommendations, by this means increasing transparency and trust. Still, challenges in adversarial attacks, algorithmic bias, and variable regulatory frameworks remain strong. According to several studies, more than 60 % of healthcare professionals have expressed their hesitation in adopting AI systems due to a lack of transparency and fear of data insecurity. Moreover, the 2024 WotNot data breach uncovered weaknesses in AI technologies and highlighted the dire requirement for robust cybersecurity. Full understanding of the potential of AI will be possible only with putting into practice of ethical and technical maintains in healthcare systems. Effective strategies would include integrating bias mitigation methods, strengthening cybersecurity protocols to prevent breaches. Also by adopting interdisciplinary collaboration with the goal of forming transparent regulatory guidelines. These are very important steps toward earning trust and ensuring that AI systems are safe, reliable, and fair. AI can bring transformative opportunities to improve healthcare outcomes, but successful implementation will depend on overcoming the challenges of trust, security, and ethics. Future research should focus on testing these technologies in multiple real-world settings, enhance their scalability, and fine-tune regulations to facilitate accountability. Only by combining technological innovations with ethical principles and strong governance can AI reshape healthcare, ensuring at the same time safety and trustworthiness. ","This systematic review highlights the transformative potential of AI in healthcare, but notes significant challenges in safety, trust, security, and ethics, emphasizing the need for explainable AI, robust cybersecurity, and transparent regulatory guidelines to ensure trustworthy AI adoption."
"Developing an AI-Based clinical decision support system for basal insulin titration in type 2 diabetes in primary Care: A Mixed-Methods evaluation using heuristic Analysis, user Feedback, and eye tracking","https://scispace.com/paper/developing-an-ai-based-clinical-decision-support-system-for-1es6rl1m7skh","2025","Journal Article","International Journal of Medical Informatics","Camilla Heisel Nyholm Thomsen
Thomas Kronborg
Stine Hangaard
Peter Vestergaard
Morten Hasselstrøm Jensen","10.1016/j.ijmedinf.2024.105783","","The progressive nature of type 2 diabetes often, in time, necessitates basal insulin therapy to achieve glycemic targets. However, despite standardized titration algorithms, many people remain poorly controlled after initiating insulin therapy, leading to suboptimal glycemic control and complications. Both healthcare professionals and people with type 2 diabetes have expressed the need for novel tools to aid in this process. Traditional titration methods often lack the precision needed to address individual differences in glycemic response. Recent studies have highlighted the potential of AI-driven solutions, which can leverage large datasets to model patient-specific characteristics. Therefore, this study aims to develop a digital platform for an AI-based clinical decision support system to assist healthcare professionals in primary care with personalized and optimal basal insulin titration for people with type 2 diabetes. An iterative mixed-method approach was used for system development, incorporating usability engineering principles. Initial requirements were gathered from domain experts and followed by heuristic evaluation and user-based evaluation. Data from these evaluations guided successive iterations of the prototype. The initial prototype featured a retrospective graph of insulin doses and fasting glucose levels and a dose adjustment simulation environment. Heuristic evaluation identified 92 issues, primarily related to minimalistic and aesthetic design. The second prototype addressed these concerns, but user-based evaluation found 66 additional usability problems, notably with HbA1c presentation and the need for more glucose measures. The final prototype showed high usability, with a median System Usability Scale score of 93.8. Task completion rates were high (task 1: 87.5%, task 2: 75.0%, and task 3: 100%). Eye-tracking data showed minimal distractions. The AI-based Clinical Decision Support System shows promise in managing basal insulin titration for people with type 2 diabetes, addressing clinical inertia, and providing a user-friendly, efficient tool to improve glycemic control during insulin titration. ","This study develops an AI-based clinical decision support system for basal insulin titration in type 2 diabetes, using a mixed-methods approach, and evaluates its usability through heuristic analysis, user feedback, and eye-tracking, achieving high usability and task completion rates."
"The application of explainable artificial intelligence (XAI) in electronic health record research: A scoping review","https://scispace.com/paper/the-application-of-explainable-artificial-intelligence-xai-5v0qqqdbt38w","2024","Journal Article","Digital health","Jessica Caterson
Alex Lewin
Elizabeth Williamson","10.1177/20552076241272657","","Machine Learning (ML) and Deep Learning (DL) models show potential in surpassing traditional methods including generalised linear models for healthcare predictions, particularly with large, complex datasets. However, low interpretability hinders practical implementation. To address this, Explainable Artificial Intelligence (XAI) methods are proposed, but a comprehensive evaluation of their effectiveness is currently limited. The aim of this scoping review is to critically appraise the application of XAI methods in ML/DL models using Electronic Health Record (EHR) data. In accordance with PRISMA scoping review guidelines, the study searched PUBMED and OVID/MEDLINE (including EMBASE) for publications related to tabular EHR data that employed ML/DL models with XAI. Out of 3220 identified publications, 76 were included. The selected publications published between February 2017 and June 2023, demonstrated an exponential increase over time. Extreme Gradient Boosting and Random Forest models were the most frequently used ML/DL methods, with 51 and 50 publications, respectively. Among XAI methods, Shapley Additive Explanations (SHAP) was predominant in 63 out of 76 publications, followed by partial dependence plots (PDPs) in 11 publications, and Locally Interpretable Model-Agnostic Explanations (LIME) in 8 publications. Despite the growing adoption of XAI methods, their applications varied widely and lacked critical evaluation. This review identifies the increasing use of XAI in tabular EHR research and highlights a deficiency in the reporting of methods and a lack of critical appraisal of validity and robustness. The study emphasises the need for further evaluation of XAI methods and underscores the importance of cautious implementation and interpretation in healthcare settings. ","This scoping review of 76 studies (2017-2023) on Explainable Artificial Intelligence (XAI) in Electronic Health Record (EHR) research finds XAI adoption increasing, but applications varied and lacked critical evaluation, highlighting the need for further evaluation and cautious implementation in healthcare settings."
"Incorporating Uncertainty Estimation and Interpretability in Personalized Glucose Prediction Using the Temporal Fusion Transformer","https://scispace.com/paper/incorporating-uncertainty-estimation-and-interpretability-in-3homlu1a6bvd","2025","Journal Article","Sensors","Antonio J. Rodríguez-Almeida
Carmelo Betancort
Ana M. Wägner
Gustavo M. Callicó
Himar Fabelo","10.3390/s25154647","","More than 14% of the world’s population suffered from diabetes mellitus in 2022. This metabolic condition is defined by increased blood glucose concentrations. Among the different types of diabetes, type 1 diabetes, caused by a lack of insulin secretion, is particularly challenging to treat. In this regard, automatic glucose level estimation implements Continuous Glucose Monitoring (CGM) devices, showing positive therapeutic outcomes. AI-based glucose prediction has commonly followed a deterministic approach, usually with a lack of interpretability. Therefore, these AI-based methods do not provide enough information in critical decision-making scenarios, like in the medical field. This work intends to provide accurate, interpretable, and personalized glucose prediction using the Temporal Fusion Transformer (TFT), and also includes an uncertainty estimation. The TFT was trained using two databases, an in-house-collected dataset and the OhioT1DM dataset, commonly used for glucose forecasting benchmarking. For both datasets, the set of input features to train the model was varied to assess their impact on model interpretability and prediction performance. Models were evaluated using common prediction metrics, diabetes-specific metrics, uncertainty estimation, and interpretability of the model, including feature importance and attention. The obtained results showed that TFT outperforms existing methods in terms of RMSE by at least 13% for both datasets. ","This study develops a personalized glucose prediction model using the Temporal Fusion Transformer (TFT) with uncertainty estimation and interpretability, outperforming existing methods by at least 13% in RMSE for both in-house and OhioT1DM datasets."